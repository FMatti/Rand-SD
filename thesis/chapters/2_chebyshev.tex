\chapter{Interpolation and trace estimation}
\label{chp:2-chebyshev}

%Applying a non-polynomial function to a matrix is often quite difficult.
%Disadvantage of Lanczos algorithm?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chebyshev interpolation}
\label{sec:2-chebyshev-interpolation}

Matrix polynomials are comparatively easier to evaluate than most other
matrix functions. For that reason, the approach of first approximating
a function with a polynomial and subsequently evaluating said polynomial at a
matrix can be significantly more efficient.
The Chebyshev interpolation framework is suitable for this due to its stability
and recurrence relation \refequ{equ:2-chebyshev-chebyshev-recursion} which can
be exploited for efficiently evaluating Chebyshev matrix polynomials.\\

At the foundation of Chebyshev interpolation lie the \glspl{chebyshev-polynomial}.
They are defined as \cite[Chapter~3]{trefethen2019chebyshev}
\begin{equation}
    T_l: [-1, 1] \to [-1, 1],~T_l(x) = \cos(l \arccos(x)).
    \label{equ:2-chebyshev-chebyshev-definition}
\end{equation}
They satisfy the recurrence relation
\begin{equation}
    \begin{cases}
        T_0(x) = 1, T_1(x) = x \\ T_{l+1}(x) = 2x T_l(x) - T_{l-1}(x), l \geq 2,
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-recursion}
\end{equation}
which can be shown using the definition \refequ{equ:2-chebyshev-chebyshev-definition}
and a standard trigonometric identity.\\

The Chebyshev expansion of the matrix function appearing in \refequ{equ:1-introduction-spectral-density-as-trace}
with \gls{chebyshev-degree} is then \cite[Chapter~3]{trefethen2019chebyshev},
\begin{equation}
    g_{\sigma}^m(t\mtx{I} - \mtx{A}) = \sum_{l=0}^m \mu_l(t) T_l(\mtx{A})
    \label{equ:2-chebyshev-chebyshev-expansion}
\end{equation}
The corresponding coefficients $\{\mu_l(t)\}_{l=0}^m$ could be computed using the
orthogonality of the Chebyshev polynomials, and some authors suggest computing
them using a quadrature rule \cite[Algorithm~1]{lin2017randomized}. However, we
have found a significantly simpler and faster approach: Note that polynomials of
degree $m$ are uniquely defined by $m+1$ distinct evaluations \cite{gauss1799demonstratio}.
Hence, if we know the values $g_{\sigma}^m(t - \cdot)$ at $m+1$ distinct points 
$\{s_i\}_{i=0}^m$, we can uniquely determine the coefficients $\{\mu_l\}_{l=0}^m$
of this polynomial. For convenience, we may choose $s_i = \cos(2 \pi i/m), i=0,\dots,m$.
In that case,
\begin{equation}
    g_{\sigma}^m(t - s_i) = \sum_{l=0}^{m} \mu_l(t) \cos\left(\frac{2 \pi l i}{m}\right),
    \label{equ:2-chebyshev-chebyshev-nodes-evaluation}
\end{equation}
which coincides with the \glsfirst{DCT}\footnote{There exist multiple conventions for the DCT.
The one which we use is (up to scaling of the first and last coefficient)
referred to as a type 1 DCT, and is efficiently implemented in the SciPy Python package:
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}} of the coefficients $\{\mu_l\}_{l=0}^m$.
If we collect the coefficients $\{\mu_l(t)\}_{l=0}^{m}$ in a vector $\vct{\mu}(t) \in \mathbb{R}^{m+1}$ 
and the function evaluations $\{g_{\sigma}^m(t - s_i)\}_{i=0}^{m}$ in another
vector $\vct{g}(t)$, we find that
\begin{equation}
    \vct{g}(t) = \DCT \{ \vct{\mu}(t) \} \implies \vct{\mu}(t) = \DCT^{-1}\{ \vct{g}(t) \}.
    \label{equ:2-chebyshev-chebyshev-DCT}
\end{equation}
In short, computing the coefficients of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion}
of the function $g_{\sigma}$ amounts to evaluating this function at $m+1$ well
chosen points and computing the inverse \gls{DCT}.
This procedure is usually inexpensive and can be done in $\mathcal{O}(m \log(m))$
operations \cite{makhoul1980fct}.\\
%\textcolor{red}{Contrary to popular belief \cite[Algorithm~1]{lin2017randomized},
%the coefficients should not be computed using a quadrature rule for the integral
%expression of the coefficients \cite[Theorem~3.1]{trefethen2019chebyshev}, since
%in general this is not guaranteed to be exact and is furthermore more complicated
%and slower than the \gls{DCT}.}

Plugging the expansion \refequ{equ:2-chebyshev-chebyshev-expansion} into 
\refequ{equ:1-introduction-spectral-density-as-trace} gives us the expanded
spectral density
\begin{equation}
    \phi_{\sigma}^m(t) = \Tr(g_{\sigma}^m(t\mtx{I} - \mtx{A})).
    \label{equ:2-chebyshev-spectral-density-as-trace-expansion}
\end{equation}

\todo{Explain spectral transformation for $\mtx{A}$ not in $[-1, 1]$}

The convergence of the expansion to the actual spectral density is exponential, 
which is claimed by the following theorem:
\begin{theorem}{Chebyshev interpolation error for Gaussian smoothing kernel}{chebyshev-error}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix whose spectrum
    is contained in $(-1, 1)$. For any $t \in \mathbb{R}$ it holds
    \begin{equation}
        \left|  \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right| \leq \frac{C_1}{\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-error}
    \end{equation}
    with constants $C_1, C_2 > 0$ independent of $\sigma$, $m$, and $t$.
\end{theorem}
This result is a consequence of Bernstein's theorem \cite[Theorem~73]{meinardus1967approximation}.
A proof can be found in \cite[Theorem~2]{lin2017randomized}. However, since our
result slightly deviates from the aforementioned proof, we include it hereafter.
\begin{proof}
    Using basic properties of matrix functions, we obtain
    \begin{align}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        &= \left| \Tr(g_{\sigma}(t\mtx{I} - \mtx{A})) - \Tr(g_{\sigma}^m(t\mtx{I} - \mtx{A})) \right|
        && \text{(definitions \refequ{equ:1-introduction-spectral-density-as-trace} and \refequ{equ:2-chebyshev-chebyshev-expansion})} \notag \\
        &= \left| \sum_{i=1}^n \left(g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i)\right) \right|
        && \text{(property of matrix function)} \notag \\
        &\leq n \max_{i = 1, \dots, n} \left| g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i) \right|
        && \text{(pessimistic upper bound)} \notag \\
        &\leq n \max_{s \in (-1, 1)} \left| g_{\sigma}(t - s) - g_{\sigma}^m(t - s) \right|
        && \text{(extension of domain)} \notag \\
        &\leq n \frac{2}{\chi^m(\chi - 1)} \max_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)|
        && \text{(Bernstein \cite[Theorem~73]{meinardus1967approximation})}
        \label{equ:2-chebyshev-proof-bernstein-general-expression}
    \end{align}
    where in the last step we define the ellipse $\mathcal{E}_{\chi}$
    with foci $\{-1, 1\}$ and with sum of half-axes $\chi = a + b > 1$
    (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Since $g_{\sigma}(t - \cdot)$ of the form \refequ{equ:1-introduction-def-gaussian-kernel}
    is holomorphic, $\chi$ may be chosen arbitrarily.

    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}
            \draw[thick, darkblue, fill=lightblue] (0, 0) ellipse (2.5 and 1.5);
            \draw[->] (0, -2) to (0, 2) node[above] {$\Imag(z)$};
            \draw[->] (-3, 0) to (3, 0) node[right] {$\Real(z)$};
            \fill[darkblue] (-1.5, 0) circle (0.05) node[below] {$-1$};
            \fill[darkblue] (1.5, 0) circle (0.05) node[below] {$+1$};
            \draw[<->, darkblue] (0.1, 0.15) to  node[midway, right] {$b$} (0.1, 1.4);
            \draw[<->, darkblue] (0.15, 0.1) to  node[midway, above] {$a$} (2.4, 0.1);
        \end{tikzpicture}
        \caption{The Bernstein ellipse $\mathcal{E}_{\chi}$ with half axes $a$ and$b$ visualized in $\mathbb{C}$.}
        \label{fig:2-chebyshev-proof-bernstein-ellipse}
    \end{figure}

    Writing $z = x + iy$ for $x,y \in \mathbb{R}$, we estimate (using $|e^z| = e^{\Real(z)}$)
    \begin{equation}
        |g_{\sigma}(t - (x + iy))| %&= \frac{1}{n \sqrt{2 \pi \sigma^2}} \left| e^{- \frac{(t - (x + iy))^2}{2 \sigma^2}} \right| \notag \\
        = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{- \frac{(t - x)^2 - y^2}{2 \sigma^2}}
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{y^2}{2 \sigma^2}}.
    \end{equation}

    Expressing $\chi = 1 + \alpha \sigma$ for any $\alpha > 0$,
    we can estimate $\chi - \chi^{-1} \leq 2\alpha\sigma$.
    This can be established by observing
    $h(\chi) = 2\alpha\sigma - \chi + \chi^{-1} = \chi + \chi^{-1} - 2 \geq 0$
    for which $h(1) = 0$ and $h'(\chi) \geq 0$ for all $\chi > 1$.
    Furthermore, because $z$ is
    contained in $\mathcal{E}_{\chi}$ we know that the absolute value of its
    imaginary part is upper bound by the imaginary half axis $b$, which we can
    express in terms of $\chi$ to get \todo{[cite or proof]}
    \begin{equation}
        |y| \leq b = \frac{\chi - \chi^{-1}}{2} \leq \alpha\sigma.
    \end{equation}

    Consequently, for all $t \in \mathbb{R}$
    \begin{equation}
        \max_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)| 
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}.
    \end{equation}

    Plugging this estimate into \refequ{equ:2-chebyshev-proof-bernstein-general-expression}, we get
    \begin{equation}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        \leq n \frac{2}{(1 + \alpha\sigma)^m\alpha \sigma} \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}
        = \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m},
    \end{equation}
    where $C_1=\sqrt{\frac{2}{\pi}}\frac{1}{\alpha}e^{\frac{\alpha^2}{2}}$ and $C_2=\alpha$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic trace estimation}
\label{sec:2-chebyshev-stochastic-trace-estimation}

Assembling the expanded matrix function involved in \refequ{equ:2-chebyshev-spectral-density-as-trace-expansion}
explicitly in order to compute its trace can incur significant computational cost.
To avoid this cost, we may resort to stochastically estimating the trace through
matrix-vector products. The quadratic form $\vct{\psi}^{\top} \mtx{B} \vct{\psi}$
for a symmetric matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ and a standard Gaussian
random vector $\vct{\psi} \in \mathbb{R}^n$ %, $\mathbb{E}[\vct{\psi}] = \vct{0}$, $\mathbb{E}[\vct{\psi}\vct{\psi}^{\top}] = \mtx{I}$ 
is an unbiased estimate of the trace:
\begin{equation}
    \mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}]
        = \mathbb{E}\left[\sum_{i=1}^n\sum_{j=1}^n \psi_i B_{ij} \psi_j\right]
        = \sum_{i=1}^n\sum_{j=1}^n B_{ij} \mathbb{E}[\psi_i\psi_j]
        = \sum_{i=1}^n B_{ii}
        = \Tr(\mtx{B}).
    \label{equ:2-chebyshev-DGC-hutchinson}
\end{equation}

\todo{Take time dependent Hutchinson result from preprint}
%Variance of Hutchinson \cite[Proposition~1]{hutchinson1990trace} \textcolor{red}{(only for Rademacher, else 2 times Frobenius norm)}
\begin{align}
    \MSE(\vct{\psi}^{\top} \mtx{B} \vct{\psi}) &= \Var(\vct{\psi}^{\top} \mtx{B} \vct{\psi}) + (\mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}] - \Tr(\mtx{B}))^2 && \text{(mean squared error)} \notag \\
        &= \Var(\vct{\psi}^{\top} \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} \vct{\psi}) && \text{(spectral decomposition of $\mtx{B}$)} \notag \\
        &= \Var(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}) && \text{(Gaussian orthogonal invariance)} \notag \\
        &= \mathbb{E}[(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}})^2] - \mathbb{E}[\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}]^2 && \text{(definition of variance)} \notag \\
        &= \mathbb{E}[(\sum_{i=1}^{n} \widetilde{\psi}_i^2 \lambda_i)^2] - \Tr(\mtx{B})^2 && \text{($\mtx{\Lambda}$ is diagonal and \refequ{equ:2-chebyshev-DGC-hutchinson})} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j \mathbb{E}[\widetilde{\psi}_i^2 \widetilde{\psi}_j^2] - \Tr(\mtx{B})^2 && \text{(linearity of expectation value)} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j + 2 \sum_{i=1}^{n} \lambda_i^2 - \Tr(\mtx{B})^2 && \text{($\mathbb{E}[\widetilde{\psi}_i^2]=1$ and $\mathbb{E}[\widetilde{\psi}_i^4]=3$)} \notag \\
        &= \Tr(\mtx{B})^2 + 2 \lVert \mtx{B} \rVert _F^2 - \Tr(\mtx{B})^2 && \text{(definition of Frobenius norm)} \notag \\
        &= 2 \lVert \mtx{B} \rVert _F^2.
    \label{equ:2-chebyshev-DGC-hutchinson-variance}
\end{align}

If we now take a certain \glsfirst{num-random-vectors} to form the
\glsfirst{sketching-matrix} $= [\vct{\psi}_1, \dots, \vct{\psi}_{n_v}] \in \mathbb{R}^{n \times n_v}$
The Hutchinson's trace estimator can then be written as \cite{hutchinson1990trace}
\begin{equation}
    \Hutch_{n_v}(\mtx{B}) = \frac{1}{n_v} \sum_{j=1}^{n_v} \vct{\psi}_j^{\top} \mtx{B} \vct{\psi}_j = \frac{1}{n_v} \Tr(\mtx{\Psi}^{\top} \mtx{B} \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator}
\end{equation}
It is not hard to see that the mean squared error
\begin{equation}
    \MSE(\Hutch_{n_v}(\mtx{B})) = \frac{2}{\sqrt{n_v}} \lVert \mtx{B} \rVert _F^2 \leq \frac{2}{\sqrt{n_v}}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Delta-Gauss-Chebyshev method}
\label{sec:2-chebyshev-delta-gauss-chebyshev}

Finally, putting together the 
Chebyshev expansion discussed in \refsec{sec:2-chebyshev-interpolation} and 
the stochastic trace estimation from \refsec{sec:2-chebyshev-stochastic-trace-estimation},
we end up with the \glsfirst{DGC} method \cite[Algorithm~2]{lin2017randomized}
which approximates the \glsfirst{smooth-spectral-density} as
\begin{equation}
    \widetilde{\phi}_{\sigma}^m(t) = H_{n_v}(g_{\sigma}^m(t\mtx{I} - \mtx{A})).
    \label{equ:2-chebyshev-DGC-final-estimator}
\end{equation}
An efficient implementation is realizable thanks to recurrence relation for the
Chebyshev polynomials \refequ{equ:2-chebyshev-chebyshev-recursion}. The pseudocode
for this method can be found in \refalg{alg:DGC}.

\begin{algo}{Delta-Gauss-Chebyshev method}{DGC}
    Symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$, number of random vectors $n_v$,
    evaluation points $\{t_i\}_{i=1}^{n_t}$
    \begin{algorithmic}[1]
        \State Compute $\{\mu_l(t_i)\}_{l=0}^m$ for all $t_i$ using \refequ{equ:2-chebyshev-chebyshev-DCT}
        \State Generate \glsfirst{sketching-matrix} $\in \mathbb{R}^{n \times n_v}$
        \State Initialize $[\mtx{V}_1, \mtx{V}_2, \mtx{V}_3] \gets [\mtx{0}_{n \times n_v}, \mtx{\Psi}, \mtx{0}_{n \times n_v}]$
        \State Set ${\phi}_{\sigma}^m(t_i) \gets 0$ for $i=1,\dots,n_t$
        \For {$l = 0, \dots, m$}
          \State $x \gets \Tr(\mtx{\Psi}^{\top} \mtx{V}_2)$
          \For {$i = 1, \dots, n_t$}
            \State $\widetilde{\phi}_{\sigma}^m(t_i) \gets \widetilde{\phi}_{\sigma}^m(t_i) + \mu_l(t_i) x$
          \EndFor
          \State $\mtx{V}_3 \gets (2 - \delta_{l0}) \mtx{A} \mtx{V}_2 - \mtx{V}_1$ \Comment{Chebyshev recurrence \refequ{equ:2-chebyshev-chebyshev-recursion}}
          \State $\mtx{V}_1 \gets \mtx{V}_2, \mtx{V}_2 \gets \mtx{V}_3$
        \EndFor
    \end{algorithmic}
\end{algo}

One additional speed-up can be achieved by smartly computing the trace of the 
product of two matrices $\mtx{B}, \mtx{C} \in \mathbb{R}^{N \times M}$ in
$\mathcal{O}(MN)$ instead of $\mathcal{O}(M^2N)$ time, due to the relation
\begin{equation}
    \Tr(\mtx{B}^{\top}\mtx{C}) = \sum_{i=1}^{N} \sum_{j=1}^{M} B_{ij} C_{ij}.
    \label{equ:2-chebyshev-fast-trace}
\end{equation}

Denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and
$\mathcal{O}(c(n)) = n$ for sparse matrices, we determine the computational
complexity of the \gls{DGC} method to be $\mathcal{O}(m \log(m) n_t + m c(n) n_v)$,
with $\mathcal{O}(m n_t + n n_v)$ required additional storage.
