\chapter{Interpolation and trace estimation}
\label{chp:2-chebyshev}

For many \glsfirst{smoothing-kernel}, as is for example the case with Gaussian smoothing
\refequ{equ:1-introduction-def-gaussian-kernel}, we cannot compute the matrix function
\begin{equation}
    g_{\sigma}(t\mtx{I}_n - \mtx{A})
    \label{equ:2-chebyshev-matrix-function}
\end{equation}
involved in \refequ{equ:1-introduction-spectral-density-as-trace} without first
diagonalizing $\mtx{A}$, which can be prohibitively expensive for large
$\mtx{A}$. A way around this problem is to refrain from trying to assemble the
matrix function explicitly and instead use the fact that thanks to
\refequ{equ:1-introduction-spectral-density-as-trace} we are only interested
in its trace. Approximating the trace can be done by multiplying the matrix
with random vectors, to then -- for example -- evaluate the Hutchinson's trace estimator \cite{hutchinson1990trace}.
The multiplication of a matrix function with a vector can
often be determined quite efficiently using Krylov subspace methods
\cite[chapter~13.2]{higham2008functions}. Another way in which products of matrix
functions with vectors can be computed involves
expanding the function in terms of a finite set of orthogonal polynomials
and subsequently using a recurrence relation to efficiently construct the result.
This approach turns out to be particularly effective when we work with matrix
functions which smoothly depend on a parameter within a bounded interval, and if we
want to evaluate the function at a large number of values of this parameter.
In this chapter we will analyze one such expansion, the Chebyshev expansion,
which gives rise to an efficient method for approximating the spectral density,
particularly when the \gls{num-evaluation-points} is large.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chebyshev interpolation}
\label{sec:2-chebyshev-interpolation}

The Chebyshev interpolation framework is best known for its stability, beneficial
convergence properties, and simple three-term recurrence relation
\refequ{equ:2-chebyshev-chebyshev-recursion} which can
be exploited to efficiently compute products of Chebyshev polynomials with
vectors.\\

At the foundation of Chebyshev interpolation lie the \glspl{chebyshev-polynomial}.
They are defined for all $l \in \mathbb{N}$ as \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    \begin{cases}
        T_l: [-1, 1] \to [-1, 1] \\
        T_l(s) = \cos(l \arccos(s)).
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-definition}
\end{equation}
They satisfy the three-term recurrence relation
\begin{equation}
    \begin{cases}
        T_0(s) = 1 & l = 0, \\
        T_1(s) = s & l = 1, \\
        T_{l}(s) = 2s T_{l-1}(s) - T_{l-2}(s) & l \geq 2,
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-recursion}
\end{equation}
which can be shown using their definition \refequ{equ:2-chebyshev-chebyshev-definition}
and a standard trigonometric identity.\\

A function $f:[-1, 1] \to \mathbb{R}$ can be expanded in a basis of Chebyshev
polynomials up to \gls{chebyshev-degree} $\in \mathbb{N}$ \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    f^{(m)}(s) = \sum_{l=0}^{m} \mu_l T_l(s).
    \label{equ:2-chebyshev-chebyshev-expansion-general}
\end{equation}
For functions $f$ which can be analytically extended in a certain neighborhood
of $[-1, 1]$ in the complex plane, Bernstein's theorem \cite[theorem~4.3]{trefethen2008gauss}
establishes that the convergence of this expansion in the $L^{\infty}$-norm is
exponential.
\begin{theorem}{Bernstein's theorem}{2-chebyshev-bernstein}
    Let $f:[-1, 1] \to \mathbb{R}$ be analytic and bounded in the ellipse $\mathcal{E}_{\chi} \subset \mathbb{C}$
    with foci $\pm 1$ and sum of half axis lengths $\chi = a + b$ (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Then its Chebyshev expansion $f^{(m)}$ with \glsfirst{chebyshev-degree} $\geq 0$ satisfies
    \begin{equation}
        \lVert f - f^{(m)} \rVert _{\infty} \leq \frac{2}{\chi^{m}(\chi-1)} \sup_{z \in \mathcal{E}_{\chi}} |f(z)|.
        \label{equ:2-chebyshev-bernstein-convergence-result}
    \end{equation}
\end{theorem}

\begin{figure}[ht]
    \centering
    \input{figures/bernstein_ellipse.tex}
    \caption{A Bernstein ellipse $\mathcal{E}_{\chi}$ with half axis lengths $a$ and
        $b$ visualized in the complex plane $\mathbb{C}$.}
    \label{fig:2-chebyshev-proof-bernstein-ellipse}
\end{figure}

The coefficients $\{\mu_l\}_{l=0}^{m}$ in \refequ{equ:2-chebyshev-chebyshev-expansion-general}
could be computed using the orthogonality of the Chebyshev polynomials with respect
to a certain inner product, and some authors indeed suggest approximating the
involved integral using a quadrature rule \cite[equation~8, algorithm~1]{lin2017randomized}.
However, we cannot find a good theoretical guarantee that this will be accurate
and suspect that this approach only works well due to a lucky coincidence.
Instead, we
have found a significantly simpler, faster, and provably accurate way of
computing the coefficients of the Chebyshev expansion of a function $f:[-1,1] \to \mathbb{R}$:
Note that polynomials of degree $m$ are uniquely defined by $m+1$
evaluations of the polynomial at distinct points \cite{gauss1799demonstratio}.
Hence, if we know the values $f^{(m)}(s_i)$ at some $m+1$ distinct points 
$\{s_i\}_{i=0}^m$, we can uniquely determine the coefficients $\{\mu_l\}_{l=0}^{m}$
of this polynomial. For convenience, we may choose $s_i = \cos(\pi i/m), i=0,\dots,m$.
In that case,
\begin{equation}
    f^{(m)}(s_i) = \sum_{l=0}^{m} \mu_l \cos\left(\frac{\pi i l}{m}\right),
    \label{equ:2-chebyshev-chebyshev-nodes-evaluation}
\end{equation}
which coincides with a \glsfirst{DCT}\footnote{There exist multiple conventions for the DCT.
The one which we use is (up to scaling of the first and last coefficient)
referred to as a type 1 DCT, and is efficiently implemented in the SciPy Python package:
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}.} of the coefficients $\{\mu_l\}_{l=0}^{m}$.
Thus, if we collect the coefficients $\{\mu_l\}_{l=0}^{m}$ in a vector $\vct{\mu} \in \mathbb{R}^{m+1}$ 
and the function evaluations $\{f^{(m)}(s_i)\}_{i=0}^{m}$ in another
vector $\vct{f} \in \mathbb{R}^{m+1}$, we find that
\begin{equation}
    \vct{f} = \DCT \{ \vct{\mu} \} \implies \vct{\mu} = \DCT^{-1}\{ \vct{f} \}.
    \label{equ:2-chebyshev-chebyshev-DCT}
\end{equation}
In short, computing the coefficients of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion-general}
of the function $f$ amounts to evaluating this function at $m+1$ well
chosen points and computing the inverse \gls{DCT}. The corresponding algorithm
can be found in \refalg{alg:2-chebyshev-chebyshev-expansion}.
This procedure is usually inexpensive and can be done in $\mathcal{O}(m \log(m))$
operations \cite{makhoul1980fct}.

\begin{algo}{Chebyshev expansion}{2-chebyshev-chebyshev-expansion}
    \input{algorithms/chebyshev_expansion.tex}
\end{algo}

To demonstrate the higher efficiency of this \gls{DCT}-based method, we time it
against the corresponding algorithm from \cite{lin2017randomized}. The results
can be seen in \reftab{tab:2-chebyshev-timing-interpolation}.

\begin{table}[ht]
    \caption{Comparison of the runtime in milliseconds of the two approaches for computing the coefficients
        of the Chebyshev expansion of a function. We average over 7 runs of the
        algorithms and repeat these runs 1000 times to form the mean and standard
        deviation which are given in the below table. We refer to
        \cite[algorithm~1]{lin2017randomized} with \enquote{quadrature}
        and to \refalg{alg:2-chebyshev-chebyshev-expansion} with \enquote{DCT}.
        For each algorithm, we interpolate a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$,
        at \gls{num-evaluation-points} $=1000$ points, for various values of \gls{chebyshev-degree}.}
    \label{tab:2-chebyshev-timing-interpolation}
   \input{tables/timing_interpolation.tex}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic trace estimation}
\label{sec:2-chebyshev-stochastic-trace-estimation}

Matrix-free stochastic trace estimation is most useful when a matrix is not given
explicitly, but products of this matrix with vectors can be computed
efficiently. Examples of such scenarios are traces of matrix functions
\cite{ubaru2017lanczos,epperly2023xtrace} or of implicit matrices which can only
be queried through matrix-vector products \cite{ghorbani2019investigation,adepu2021hessian}.
Most algorithms for stochastic trace estimation are based on the Hutchinson's
trace estimator, which we will discuss in the following paragraphs.\\

\subsection{Constant matrices}
\label{subsec:2-chebyshev-trace-constant}

For a symmetric matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ and a standard Gaussian
random vector $\vct{\psi} \in \mathbb{R}^n$, the quadratic form $\vct{\psi}^{\top} \mtx{B} \vct{\psi}$ 
is an unbiased estimate of the trace:
\begin{equation}
    \mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}]
        = \mathbb{E}\left[\sum_{i=1}^n\sum_{j=1}^n \psi_i b_{ij} \psi_j\right]
        = \sum_{i=1}^n\sum_{j=1}^n b_{ij} \mathbb{E}[\psi_i\psi_j]
        = \sum_{i=1}^n b_{ii}
        = \Tr(\mtx{B}).
    \label{equ:2-chebyshev-DGC-hutchinson}
\end{equation}
Furthermore, the variance of this estimate is bounded by the Frobenius norm of the matrix
$\mtx{B}$:
%Variance of Hutchinson \cite[proposition~1]{hutchinson1990trace} \textcolor{red}{(only for Rademacher, else 2 times Frobenius norm)}
\begin{align*}
    \Var(\vct{\psi}^{\top} \mtx{B} \vct{\psi})
        &= \Var(\vct{\psi}^{\top} \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} \vct{\psi}) && \text{(spectral decomposition of $\mtx{B}$)} \notag \\
        &= \Var(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}) && \text{($\mtx{U}^{\top}\vct{\psi} = \widetilde{\vct{\psi}} \sim \vct{\psi}$)} \notag \\
        &= \mathbb{E}[(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}})^2] - \mathbb{E}[\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}]^2 && \text{(definition of variance)} \notag \\
        &= \mathbb{E}\bigg[\bigg(\sum_{i=1}^{n} \widetilde{\psi}_i^2 \lambda_i\bigg)^2\bigg] - \Tr(\mtx{B})^2 && \text{($\mtx{\Lambda}$ diagonal and \refequ{equ:2-chebyshev-DGC-hutchinson})} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j \mathbb{E}[\widetilde{\psi}_i^2 \widetilde{\psi}_j^2] - \Tr(\mtx{B})^2 && \text{(linearity of $\mathbb{E}$)} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j + 2 \sum_{i=1}^{n} \lambda_i^2 - \Tr(\mtx{B})^2 && \text{($\mathbb{E}[\widetilde{\psi}_i^2]=1$ and $\mathbb{E}[\widetilde{\psi}_i^4]=3$)} \notag \\
        &= \Tr(\mtx{B})^2 + 2 \lVert \mtx{B} \rVert _F^2 - \Tr(\mtx{B})^2 && \text{($\sum_{i=1}^{n} \lambda_i = \Tr(\mtx{B})$, $\sum_{i=1}^{n} \lambda_i^2 = \lVert \mtx{B} \rVert _F^2$)} \notag \\
        &= 2 \lVert \mtx{B} \rVert _F^2.
\end{align*}
The idea of the Hutchinson's trace estimator is to compute multiple such estimates
for different, independent random vectors and take the average. This will again
be an unbiased estimate of the trace, but with the reduced variance
\begin{equation}
    \Var\left( \frac{1}{n_{\Psi}} \sum_{j=1}^{n_{\Psi}}\vct{\psi}_j^{\top} \mtx{B} \vct{\psi}_j\right) = \frac{2}{n_{\Psi}} \lVert \mtx{B} \rVert _F^2
    \label{equ:2-chebyshev-hutchinson-mse}
\end{equation}
with the \gls{num-hutchinson-queries} $\in \mathbb{N}$.
Collecting the \gls{num-hutchinson-queries} independent random vectors
$\vct{\psi}_i \in \mathbb{R}^{n}$ in the standard Gaussian
\glsfirst{random-matrix} $= [\vct{\psi}_1, \dots, \vct{\psi}_{n_{\Psi}}] \in \mathbb{R}^{n \times n_{\Psi}}$,
we can then rewrite the Hutchinson's trace estimator as
\begin{equation}
    \Hutch_{n_{\Psi}}(\mtx{B}) = \frac{1}{n_{\Psi}} \Tr(\mtx{\Psi}^{\top} \mtx{B} \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator}
\end{equation}\\

\subsection{Parameter-dependent matrices}
\label{subsec:2-chebyshev-trace-parametrized}

In the case where the matrix, or -- alternatively said -- all its entries, continuously depends on a
parameter in a bounded interval, we can analogously define the Hutchinson's
estimator for parameter-dependent matrices
\begin{equation}
    \Hutch_{n_{\Psi}}(\mtx{B}(t)) = \frac{1}{n_{\Psi}} \Tr(\mtx{\Psi}^{\top} \mtx{B}(t) \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}
\end{equation}
As the counterpart of the variance in the parametrized case, we measure the
error of this estimate in the $L^1$-norm, for which we can use a result
from \cite{he2023parameter}, which we will state in the following lemma:
\begin{lemma}{$L^1$-error of parameter-dependent Hutchinson's estimator}{2-chebyshev-parameter-hutchinson}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ symmetric and continuous in
    $t \in [a, b]$, $\delta \in (0, e^{-1})$, and $n_{\Psi} \in \mathbb{N}$.
    Let $\Hutch_{n_{\Psi}}(\mtx{B}(t))$ be the $n_{\Psi}$-query
    Hutchinson's estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
    For a constant $C_{\Psi} \geq 0$, it holds with probability $\geq 1 - \delta$
    \begin{equation}
        \int_{a}^{b} \left| \Tr(\mtx{B}(t)) - \Hutch_{n_{\Psi}}(\mtx{B}(t)) \right| \mathrm{d}t \leq C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    \end{equation}
    %So, if $n_{\Psi} = \mathcal{O}\left( \frac{\log(2/\delta)}{\varepsilon^2} \right)$ then, with probability $\geq 1 - \delta$
    %\begin{equation}
    %    \int |H_l(\mtx{B}(t)) - \Tr(\mtx{B}(t))| \mathrm{d}t \leq \varepsilon \int \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    %\end{equation}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Delta-Gauss-Chebyshev method}
\label{sec:2-chebyshev-delta-gauss-chebyshev}

Now we have all the ingredients for constructing a first algorithm to approximate
the expression \refequ{equ:1-introduction-spectral-density-as-trace}:
the Chebyshev expansion of a function (\refalg{alg:2-chebyshev-chebyshev-expansion})
and the Hutchinson's trace estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
For a matrix symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$ with eigenvalues
contained in $[-1, 1]$
we expand the \glsfirst{smoothing-kernel} in terms of Chebyshev polynomials, such that
\begin{equation}
    g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) = \sum_{l=0}^{m} \mu_l(t) T_l(\mtx{A}).
    \label{equ:2-chebyshev-chebyshev-expansion}
\end{equation}
Plugging this expansion into \refequ{equ:1-introduction-spectral-density-as-trace}
gives us the expanded spectral density
\begin{equation}
    \phi_{\sigma}^{(m)}(t) = \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})).
    \label{equ:2-chebyshev-spectral-density-as-trace-expansion}
\end{equation}\\

By combining the Chebyshev expansion \refequ{equ:2-chebyshev-spectral-density-as-trace-expansion}
with stochastic trace estimation
we end up with the \glsfirst{DGC} method \cite[algorithm~2]{lin2017randomized},
which approximates the \glsfirst{smooth-spectral-density} as
\begin{equation}
    \widetilde{\phi}_{\sigma}^{(m)}(t) = H_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) = \frac{1}{n_{\Psi}} \sum_{l=0}^m \mu_l(t) \Tr(\mtx{\Psi}^{\top} T_l(\mtx{A}) \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-final-estimator}
\end{equation}
Apparently, it is rather cheap to evaluate $\widetilde{\phi}_{\sigma}^{(m)}(t)$
at multiple values of \gls{spectral-parameter}, since only the coefficients
of the linear combination of $\{\Tr(\mtx{\Psi}^{\top} T_l(\mtx{A}) \mtx{\Psi})\}_{l=0}^m$
change, which can easily be computed using \refalg{alg:2-chebyshev-chebyshev-expansion}.\\

An efficient implementation can be achieved thanks to the recurrence relation
\refequ{equ:2-chebyshev-chebyshev-recursion} which the Chebyshev polynomials satisfy.
However, it is usually prohibitively expensive to interpolate a big matrix
$\mtx{A}$ as a whole, since alone the matrix-matrix multiplication in each step
of the recurrence can cost up to $\mathcal{O}(n^3)$, and the evaluation
of the expansion at \gls{num-evaluation-points} values of \gls{spectral-parameter} could
cost a further $\mathcal{O}(n^2 m n_t)$ operations. Therefore, in case we are only interested
in the result of a linear mapping applied to the interpolant, a significant speed-up can be
achieved by directly interpolating the result of this linear mapping applied to
the interpolant (\reflin{lin:2-chebyshev-linear-mapping} in \refalg{alg:2-chebyshev-DGC}).
In \reffig{fig:2-chebyshev-sketched-interpolation} some examples
of such linear mappings -- which we will make use of later on -- are schematically
illustrated.\\
\begin{figure}[ht]
    \centering
    \input{figures/sketched_interpolation.tex}
    \caption{Schematic illustration of linear mappings which, applied to a large
        matrix $\mtx{A}$, reduce the dimensionality of the interpolation problem.}
    \label{fig:2-chebyshev-sketched-interpolation}
\end{figure}

Finally, we can give the pseudocode
of this first method in \refalg{alg:2-chebyshev-DGC}.
\begin{algo}{Delta-Gauss-Chebyshev method}{2-chebyshev-DGC}
    \input{algorithms/delta_gauss_chebyshev.tex}
\end{algo}

Denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and
$\mathcal{O}(c(n)) = n$ for sparse matrices, we determine the computational
complexity of the \gls{DGC} method to be $\mathcal{O}(m \log(m) n_t + m n_{\Psi} c(n))$,
with $\mathcal{O}(m n_t + n n_{\Psi})$ required additional storage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation-details}
\label{subsec:2-chebyshev-implementation-details}

Expression \refequ{equ:2-chebyshev-chebyshev-expansion} is only well-defined for matrices whose spectra are fully
contained in $[-1, 1]$. To also use the \gls{DGC} method on matrices $\mtx{A}$ whose
spectra we know (or can estimate) to be within a different interval $[a, b] \subset \mathbb{R}$,
we can define a \gls{spectral-transformation} as the linear mapping
\begin{equation}
    \begin{cases}
        \tau : [a, b] \to [-1, 1], \\
        \tau(t) = \frac{2t - a - b}{b - a}.
    \end{cases}
    \label{equ:2-chebyshev-spectral-transformation}
\end{equation}
The \gls{DGC} method can then be applied to $\bar{\mtx{A}} = \tau(\mtx{A})$ whose
spectrum is contained in $[-1, 1]$.\\

However, retrieving the \glsfirst{smooth-spectral-density} of the original
matrix $\mtx{A}$ after this transformation is not straight forward
and is usually swept under the rug in literature.
Let us call $\bar{\phi}$ the spectral density of $\bar{\mtx{A}}$.
Based on a derivation from \refapp{chp:A-appendix}, it turns out that in order
to approximate $\phi_{\sigma}$ of a general matrix $\mtx{A}$
for the \glsfirst{smoothing-kernel} we consider
in this thesis (Gaussian, Lorentzian), we only need to rescale their
\glsfirst{smoothing-parameter} to
\begin{equation}
    \bar{\sigma} = \frac{2\sigma}{b - a},
    \label{equ:2-chebyshev-sigma-transformation}
\end{equation}
run \refalg{alg:2-chebyshev-DGC} on $\bar{\mtx{A}}$ with $\bar{g}_{\sigma}$
on the transformed evaluation points $\{ \tau(t_i) \}_{i=1}^{n_t}$ and finally
multiply the resulting approximation with $\frac{2}{b-a}$.
In all our examples, this procedure will be used to compute spectral densities
of matrices which have eigenvalues outside of $[-1, 1]$.\\

%From \refequ{equ:1-introduction-def-spectral-density}
%it can be deduced that $\phi = \frac{2}{b - a}\bar{\phi} \circ \tau$ due to the scaling properties
%of the \glsfirst{dirac-delta}. Finding a relation between \gls{smooth-spectral-density}
%and $\bar{\phi}_{\sigma}$ is not entirely obvious, and is usually swept under
%the rug in literature. For this, we need to go back all the way to the definition of
%\gls{smooth-spectral-density} \refequ{equ:1-introduction-def-smooth-spectral-density}
%and evaluate the expression
%\begin{align*}
%    &\phi_{\sigma}(\tau^{-1}(t)) \notag \\
%    &= (\phi \ast g_{\sigma})(\tau^{-1}(t)) && \text{(definition of \gls{smooth-spectral-density})} \notag \\
%    &= \int_{-\infty}^{\infty} \phi(s) g_{\sigma}(\tau^{-1}(t)-s) \mathrm{d}s && \text{(convolution $\ast$)} \notag \\
%    &= \frac{2}{b-a}\int_{-\infty}^{\infty} \bar{\phi}(\tau(s)) g_{\sigma}(\tau^{-1}(t)-s) \mathrm{d}s && \text{(identity $\phi = \frac{2}{b-a}\bar{\phi} \circ \tau$)} \notag \\
%    &= \frac{2}{b-a}\int_{-\infty}^{\infty} \bar{\phi}(\bar{s}) g_{\sigma}\left(\tau^{-1}(t)-\tau^{-1}(\bar{s})\right) \frac{b-a}{2} \mathrm{d}\bar{s} && \text{(substitution $\bar{s} = \tau(s)$)} \notag \\
%    %&= \int_{-\infty}^{\infty} \bar{\phi}(\bar{s}) g_{\sigma}\left(\tau^{-1}(t - \bar{s})\right) \frac{2}{b-a} \mathrm{d}\bar{s} && \text{(linearity of \gls{spectral-transformation})} \notag \\
%    &= \frac{2}{b-a}\int_{-\infty}^{\infty} \bar{\phi}(\bar{s}) g_{\sigma}\left(\frac{b-a}{2}(t - \bar{s})\right) \frac{b-a}{2} \mathrm{d}\bar{s} && \text{(explicit form of $\tau^{-1}$)}
%\end{align*}
%%Therefore, we can
%%obtain \gls{smooth-spectral-density} of $\mtx{A}$ by running the method
%%with 
%By identifying
%\begin{equation}
%    \bar{g}_{\sigma}(s) = g_{\sigma}\left(\frac{b-a}{2}s\right) \frac{b-a}{2}
%\end{equation}
%we see that $\phi_{\sigma}= \frac{2}{b-a}(\bar{\phi} \ast \bar{g}_{\sigma}) \circ \tau$.
%Therefore, we can obtain \gls{smooth-spectral-density} of $\mtx{A}$ by running
%\refalg{alg:2-chebyshev-DGC} with $\bar{g}_{\sigma}$
%and $\bar{\mtx{A}}$ on the transformed evaluation points $\{ \tau(t_i) \}_{i=1}^{n_t}$,
%and rescale the result with $\frac{2}{b-a}$.
%Since all of the \glsfirst{smoothing-kernel} we consider in this thesis (Gaussian, Lorentzian) are of the form
%\gls{smoothing-kernel}$(s)=\frac{1}{\sigma}f(\frac{s}{\sigma})$ for some function $f$ independent of
%\gls{smoothing-parameter}, we only need to rescale the \glsfirst{smoothing-parameter} to

%which allow us, for most kernels (Gaussian, Lorentzian, ...) and up to multiplication
%with a constant factor, to reconstruct the spectral density of $\mtx{A}$.
%In all our examples, this procedure will be used to compute spectral densities
%of matrices which have eigenvalues outside of $[-1, 1]$.\\
%and instead expand $\bar{g}_{\bar{\sigma}}^{(m)} = g_{\sigma}^{(m)} \circ \tau^{-1}$ 
%from which we can retrieve $g_{\sigma}^{(m)} = \bar{g}_{\bar{\sigma}}^{(m)} \circ \tau$.
%\todo{Clean up this mess}
%Note that under this transformation also the \gls{smoothing-parameter} needs to be
%rescaled to
%\begin{equation}
%    \bar{\sigma} = |\tau'| \sigma = \frac{2\sigma}{b - a}
%    \label{equ:2-chebyshev-sigma-transformation}
%\end{equation}
%in order to get a consistent result.\\

A speed-up of \refalg{alg:2-chebyshev-DGC} can be achieved by smartly computing the trace of the 
product $\mtx{E}^{\top}\mtx{F}$ of two matrices $\mtx{E}, \mtx{F} \in \mathbb{R}^{N \times M}$ in
$\mathcal{O}(MN)$ instead of $\mathcal{O}(M^2N)$ time, due to the relation
\begin{equation}
    \Tr(\mtx{E}^{\top}\mtx{F}) = \sum_{i=1}^{N} \sum_{j=1}^{M} e_{ij} f_{ij}.
    \label{equ:2-chebyshev-fast-trace}
\end{equation}
This reduces the complexity of \reflin{lin:2-chebyshev-fast-trace}
in \refalg{alg:2-chebyshev-DGC} from $\mathcal{O}(nn_{\Psi}^2)$ to $\mathcal{O}(nn_{\Psi})$.
Throughout this work, as has already be done for the complexity analysis
of \refalg{alg:2-chebyshev-DGC}, we implicitly assume that all traces of this
form are computed using this technique.

\subsection{Theoretical analysis}
\label{subsec:2-chebyshev-theoretical-analysis}

In order to obtain tractable results and because it is the most common case in
literature, we choose to restrict the analysis in this section to Gaussian 
\gls{smoothing-kernel} \refequ{equ:1-introduction-def-gaussian-kernel}.\\

The convergence of the expansion of a Gaussian \gls{smoothing-kernel} is exponential
and depends on \gls{smoothing-parameter}. This can be seen quite well
in \reffig{fig:2-chebyshev-chebyshev-convergence}, and is proven in the following
lemma.

\begin{lemma}{$L^1$-error of Chebyshev expansion for Gaussian smoothing}{2-chebyshev-error}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix whose spectrum
    is contained in $[-1, 1]$. Then the expansion $g_{\sigma}^{(m)}$ of the Gaussian \glsfirst{smoothing-kernel}
    and the corresponding expansion $\phi_{\sigma}^{(m)}$ of the \glsfirst{smooth-spectral-density} satisfy
    \begin{align}
        \lVert  g_{\sigma} - g_{\sigma}^{(m)} \rVert _{\infty} &\leq \frac{C_1}{2n\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-sup-error-kernel} \\
        \lVert  \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _{\infty} &\leq \frac{C_1}{2\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-sup-error} \\
        \lVert  \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1 &\leq \frac{C_1}{\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-error}
    \end{align}
    with constants $C_1, C_2 \geq 0$ independent of \gls{smoothing-parameter} and \gls{chebyshev-degree}.
\end{lemma}

\begin{figure}[ht]
    \centering
    \input{figures/chebyshev_convergence.tex}
    \caption{The error of the Chebyshev expansion of increasing \glsfirst{chebyshev-degree}
    for a Gaussian \gls{smoothing-kernel} with different values of \gls{smoothing-parameter}.}
    \label{fig:2-chebyshev-chebyshev-convergence}
\end{figure}

This result is a consequence of \refthm{thm:2-chebyshev-bernstein}.
A proof of a similar result can be found in \cite[theorem~2]{lin2017randomized}.
However, since our result -- and more so the proof -- deviate from the aforementioned
work, we chose to reproduce it hereafter.
\begin{proof}
    From Bernstein's theorem \refthm{thm:2-chebyshev-bernstein}
    and the analyticity of \gls{smoothing-kernel} we know that for all $t \in \mathbb{R}$
    \begin{equation}
        \lVert g_{\sigma}(t - \cdot) - g_{\sigma}^{(m)}(t - \cdot) \rVert _{\infty}
        \leq \frac{2}{\chi^{m}(\chi - 1)} \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)|
        \label{equ:2-chebyshev-convergence-proof-base}
    \end{equation}
    where we can use any ellipse $\mathcal{E}_{\chi}$
    with foci $\{-1, 1\}$ and with sum of half-axes $\chi = a + b > 1$
    (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    %Since $g_{\sigma}(t - \cdot)$ is of the form \refequ{equ:1-introduction-def-gaussian-kernel}
    %is analytic in all of $\mathbb{C}$, $\chi$ may be chosen arbitrarily.

    Writing $z = x + \iota y$ for $x,y \in \mathbb{R}$, we estimate (using $|e^z| = e^{\Re(z)}$)
    \begin{equation}
        |g_{\sigma}(t - (x + \iota y))| %&= \frac{1}{n \sqrt{2 \pi \sigma^2}} \left| e^{- \frac{(t - (x + iy))^2}{2 \sigma^2}} \right| \notag \\
        = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{- \frac{(t - x)^2 - y^2}{2 \sigma^2}}
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{y^2}{2 \sigma^2}}.
    \end{equation}

    Expressing $\chi = 1 + \alpha \sigma$ for any $\alpha > 0$,
    we can estimate
    \begin{equation}
        \chi - \chi^{-1} \leq 2\alpha\sigma.
        \label{equ:2-chebyshev-bernstein-proof-estimate}
    \end{equation}
    This can be established by observing
    $h(\chi) = 2\alpha\sigma - \chi + \chi^{-1} = 2(\chi - 1) - \chi + \chi^{-1} = \chi + \chi^{-1} - 2 \geq 0$
    for which $h(1) = 0$ and $h'(\chi) \geq 0$ for all $\chi > 1$.
    Furthermore, because $z$ is
    contained in $\mathcal{E}_{\chi}$ we know that the absolute value of its
    imaginary part $y$ is upper bound by the length of the imaginary half axis $b$,
    which can be expressed in terms of $\chi$ to show with \refequ{equ:2-chebyshev-bernstein-proof-estimate}
    that
    \begin{equation}
        |y| \leq b = \frac{\chi - \chi^{-1}}{2} \leq \alpha\sigma.
    \end{equation}

    Consequently, for all $t \in \mathbb{R}$
    \begin{equation}
        \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)| 
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}.
    \end{equation}

    Plugging this estimate into \refequ{equ:2-chebyshev-convergence-proof-base}
    yields
    \begin{equation}
        \lVert g_{\sigma}(t - \cdot) - g_{\sigma}^{(m)}(t - \cdot) \rVert _{\infty}
        \leq \frac{2}{(1 + \alpha\sigma)^{m}\alpha \sigma} \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}
        = \frac{C_1}{2 n \sigma^2} (1 + C_2 \sigma)^{-m},
        \label{equ:2-chebyshev-uniform-bound}
    \end{equation}
    where $C_1=\sqrt{\frac{8}{\pi}}\frac{1}{\alpha}e^{\frac{\alpha^2}{2}}$ and $C_2=\alpha$.
    With $t=0$ this proves the first assertion.\\

    For the second assertion, we may use basic properties of matrix functions to obtain
    \begin{align*}
        &\left| \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \right| \notag \\
        &= \left| \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) - \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \right|
        && \text{(definitions \refequ{equ:1-introduction-spectral-density-as-trace} and \refequ{equ:2-chebyshev-chebyshev-expansion})} \notag \\
        &= \left| \sum_{i=1}^n \left(g_{\sigma}(t - \lambda_i) - g_{\sigma}^{(m)}(t - \lambda_i)\right) \right|
        && \text{($\lambda_1, \dots, \lambda_n$ eigenvalues of $\mtx{A}$)} \notag \\
        &\leq n \max_{i = 1, \dots, n} \left| g_{\sigma}(t - \lambda_i) - g_{\sigma}^{(m)}(t - \lambda_i) \right|
        && \text{(conservative upper bound)} \notag \\
        &\leq n \max_{s \in [-1, 1]} \left| g_{\sigma}(t - s) - g_{\sigma}^{(m)}(t - s) \right|
        && \text{(extension of domain)} \notag \\
        &= n \lVert g_{\sigma}(t - \cdot) - g_{\sigma}^{(m)}(t - \cdot) \rVert _{\infty}
        && \text{(definition of $L^{\infty}$-norm)} \notag \\
        &\leq \frac{C_1}{2 \sigma^2} (1 + C_2 \sigma)^{-m}
        && \text{(using \refequ{equ:2-chebyshev-uniform-bound})} \notag \\
    \end{align*}
    from which the result follows directly.\\

    Finally, Hölder's inequality \cite{klenke2013probability} 
    allows us to also show the last assertion with what we have found above:
    \begin{equation}
        \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1
            \leq 2 \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _{\infty}
            \leq \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m}.
    \end{equation}
\end{proof}

We now have all the tools at hand to combine the approximation error of the
Chebyshev expansion (\reflem{lem:2-chebyshev-error}) with the trace estimation
error (\reflem{lem:2-chebyshev-parameter-hutchinson}) to get a tractable theoretical
result for the accuracy of the \gls{DGC} method.

\begin{theorem}{$L^1$-error of Delta-Gauss-Chebyshev method}{2-delta-gauss-chebyshev}
    Let $\widetilde{\phi}_{\sigma}^{(m)}(t)$ be the result from running \refalg{alg:2-chebyshev-DGC}
    on a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$ with its spectrum contained in $[-1, 1]$ using
    a Gaussian \glsfirst{smoothing-kernel} with
    \glsfirst{smoothing-parameter} $>0$, \glsfirst{chebyshev-degree} $\in \mathbb{N}$, and
    \glsfirst{num-hutchinson-queries} $\in \mathbb{N}$. For $\delta \in (0, e^{-1})$ it holds with
    probability $\geq 1-\delta$, that
    %Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ symmetric and continuous in
    %$t \in [a, b]$, $\delta \in (0, e^{-1})$, and $n_{\Psi} \in \mathbb{N}$.
    %Let $\Hutch_{n_{\Psi}}(\mtx{B}(t))$ be the $n_{\Psi}$-query
    %Hutchinson estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
    %For a constant $C_{\Psi}$, it holds with probability $\geq 1 - \delta$
    %\int_{-1}^{1} \left| \phi_{\sigma}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t \notag \\
    \begin{equation}
        \lVert \phi_{\sigma} - \widetilde{\phi}_{\sigma}^{(m)}\rVert _1
        \leq \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} \left( 1 + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n n_{\Psi}}} \right) + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}}
    \end{equation}
    for some constants $C_1$ and $C_2$ as in \reflem{lem:2-chebyshev-error} and $C_{\Psi} \geq 0$.
    %So, if $n_{\Psi} = \mathcal{O}\left( \frac{\log(2/\delta)}{\varepsilon^2} \right)$ then, with probability $\geq 1 - \delta$
    %\begin{equation}
    %    \int |H_l(\mtx{B}(t)) - \Tr(\mtx{B}(t))| \mathrm{d}t \leq \varepsilon \int \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    %\end{equation}
\end{theorem}

\begin{proof}
    First, we apply the triangle inequality to get
    \begin{equation}
        \lVert \phi_{\sigma} - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1
            \leq \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1 + \lVert \phi_{\sigma}^{(m)} - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1.
    \end{equation}
    %\begin{equation}
    %    \int_{a}^{b} \left| \phi_{\sigma}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t
    %        \leq \int_{a}^{b} \left| \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \right| \mathrm{d}t + \int_{a}^{b} \left| \phi_{\sigma}^{(m)}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t
    %\end{equation}

    The first term can be dealt with using \reflem{lem:2-chebyshev-error}.
    \Reflem{lem:2-chebyshev-parameter-hutchinson} can be applied to the second term for
    \begin{align*}
        \lVert \phi_{\sigma}^{(m)} - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1
            &= \int_{-1}^{1} \left| \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) - \Hutch_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \right| \mathrm{d}t && \text{(definitions)} \notag \\
            &\leq C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \int_{-1}^{1} \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F \mathrm{d}t && \text{(\reflem{lem:2-chebyshev-parameter-hutchinson})}
    \end{align*}
    %\begin{align}
    %    \int_{a}^{b} \left| \phi_{\sigma}^{(m)}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t
    %        &= \int_{a}^{b} \left| \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) - \Hutch_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \right| \mathrm{d}t \notag \\
    %        &\leq C_{\Psi} \sqrt{\frac{\log(2/\delta)}{n_{\Psi}}} \int_{a}^{b} \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F \mathrm{d}t \notag \\
    %        %&\leq C_{\Psi} \sqrt{\frac{\log(2/\delta)}{n_{\Psi}}} \int_{a}^{b} \phi_{\sigma}^{(m)}(t) \mathrm{d}t
    %\end{align}
    We proceed with bounding the involved integrand by first applying the triangle
    inequality, then exploiting properties of the Frobenius norm of a matrix function and
    the positivity of \gls{smoothing-kernel}, and finally using the result from the proof
    of \reflem{lem:2-chebyshev-error} and the definition of \gls{smooth-spectral-density}:
    \begin{align*}
        &\lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F \notag \\
        &\leq \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) - g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _F + \lVert g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _F && \text{(triangle inequality)} \notag \\
        %&\leq \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) - g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _{(2)} + \lVert g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _{(2)} && \text{(Schatten norm)} \notag \\
        %&\leq \sqrt{n} \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) - g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _{(\infty)} + \lVert g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _{(1)} && \text{(norm inequalities)} \notag \\
        %&\leq \sqrt{n} \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) - g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _{(\in)} + \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) && \text{(norm inequalities)} \notag \\
        &\leq \sqrt{n} \lVert g_{\sigma}^{(m)} - g_{\sigma} \rVert _{\infty} + \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) && \text{(norm inequalities)} \notag \\
        &\leq \frac{C_1}{2 \sqrt{n} \sigma^2} (1 + C_2 \sigma)^{-m} + \phi_{\sigma}(t) && \text{(\reflem{lem:2-chebyshev-error})}
    \end{align*}
    Putting all things together and using the normalization $\int_{-1}^{1} \phi_{\sigma}(t) \mathrm{d}t = 1$
    we end up with the desired result:
    \begin{align*}
        \lVert \phi_{\sigma}  - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1
        &\leq \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \left( \frac{C_1}{\sqrt{n} \sigma^2} (1 + C_2 \sigma)^{-m} + 1 \right) \notag \\
        &= \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} \left( 1 + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n n_{\Psi}}} \right) + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}}.
    \end{align*}
\end{proof}
 
We see that the first term in \refthm{thm:2-delta-gauss-chebyshev} will quickly
vanish as \gls{chebyshev-degree} increases. What we are left with is the slowly
decaying $\mathcal{O}(n_{\Psi}^{-1/2})$ term. In fact, this is what bottlenecks
the \gls{DGC} method from achieving better accuracies: The Hutchinson's stochastic
trace estimator is not efficient enough for approximating spectral densities.
Therefore, we will consider alternative ways of approximating \refequ{equ:1-introduction-spectral-density-as-trace}
in the next two chapters.
