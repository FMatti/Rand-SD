\chapter{Interpolation and trace estimation}
\label{chp:2-chebyshev}

Often matrix function hard to evaluate.

Disadvantage of Lanczos algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chebyshev interpolation}
\label{sec:2-chebyshev-interpolation}

Definition of \glsfirst{chebyshev-polynomial} \cite[Chapter~3]{trefethen2019chebyshev}
\begin{equation}
    T_l: [-1, 1] \to \mathbb{R},~T_l(x) = \cos(l \arccos(x))
    \label{equ:2-chebyshev-chebyshev-definition}
\end{equation}
Recursion formula (verified standard trigonometric formulae)
\begin{equation}
    \begin{cases}
        T_0(x) = 1, T_1(x) = x \\ T_{l+1}(x) = 2x T_l(x) - T_{l-1}(x), l \geq 2
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-recursion}
\end{equation}

[Plot of Chebyshev polynomials?]

Chebyshev expansion \cite[Chapter~3]{trefethen2019chebyshev},
\gls{chebyshev-degree}
\begin{equation}
    g_{\sigma}^m(t\mtx{I} - \mtx{A}) = \frac{\mu_0(t)}{2} + \sum_{l=1}^m \mu_l(t) T_l(\mtx{A})
    \label{equ:2-chebyshev-chebyshev-expansion}
\end{equation}

Polynomials of degree $m$ are uniquely defined by $m+1$ distinct evaluations [cite Fundamental theorem of algebra?].
Hence, if we know the values $g_{\sigma}^m(t - \cdot)$ at the $m+1$ distinct points 
$\{s_i\}_{i=0}^m$, we can uniquely determine the coefficients $\{\mu_l\}_{l=0}^m$
of this polynomial. For convenience, we may choose $s_i = \cos(2 \pi i/m), i=0,\dots,m$.
In that case,
\begin{equation}
    g_{\sigma}^m(t - s_i) = \frac{\mu_0(t)}{2} + \sum_{l=1}^{m} \mu_l(t) \cos\left(\frac{2 \pi l i}{m}\right),
    \label{equ:2-chebyshev-chebyshev-nodes-evaluation}
\end{equation}
which coincides with the \glsfirst{DCT}\footnote{There exist multiple conventions for the DCT.
The formulation which we use is often referred to as a type 1 DCT,
and is efficiently implemented in the SciPy Python package:
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}} of the coefficients $\{\mu_l\}_{l=0}^m$.
If we collect the coefficients $\{\mu_l(t)\}_{l=0}^{m}$ in a vector $\vct{\mu}(t) \in \mathbb{R}^{m+1}$ 
and the function evaluations $\{g_{\sigma}^m(t - s_i)\}_{i=0}^{m}$ in another
vector $\vct{g}(t)$, we find that
\begin{equation}
    \vct{g}(t) = \DCT \{ \vct{\mu}(t) \} \implies \vct{\mu}(t) = \DCT^{-1}\{ \vct{g}(t) \}.
    \label{equ:2-chebyshev-chebyshev-DCT}
\end{equation}
In short, computing the coefficients of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion}
of the function $g_{\sigma}$ amounts to evaluating this function at $m+1$ well
chosen points and computing the inverse \gls{DCT} of these evaluations. 
This procedure is usually inexpensive and can be done in $\mathcal{O}(m \log(m))$
operations \cite{makhoul1980fct}.
\textcolor{red}{Contrary to popular belief \cite[Algorithm~1]{lin2017randomized},
the coefficients should not be computed using a quadrature rule for the integral
expression of the coefficients \cite[Theorem~3.1]{trefethen2019chebyshev}, since
in general this is not guaranteed to be exact and is furthermore more complicated
and slower than the \gls{DCT}.}

\glsfirst{DGC}
\begin{equation}
    \phi_{\sigma}^m(t) = \Tr(g_{\sigma}^m(t\mtx{I} - \mtx{A}))
    \label{equ:2-chebyshev-DGC-spectral-density-chebyshev-expansion}
\end{equation}

Bernstein theorem \cite[Theorem~73]{meinardus1967approximation} \todo{Also prove this for Lorentzian}

Convergence corollary and new proof \cite[Theorem~2]{lin2017randomized}
\begin{theorem}{Chebyshev interpolation error for Gaussian smoothing kernel}{chebyshev-error}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a Hermitian matrix with spectrum in
    $(-1, 1)$. For any $t \in \mathbb{R}$ it holds
    \begin{equation}
        \left|  \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right| \leq \frac{C_1}{\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-error}
    \end{equation}
    with constants $C_1, C_2 > 0$ independent of $\sigma$, $m$, and $t$.
\end{theorem}

New proof since different result
\begin{proof}
    Using basic properties of matrix functions, we obtain
    \begin{align}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        &= \left| \Tr(g_{\sigma}(tI - A)) - \Tr(g_{\sigma}^m(tI - A)) \right|
        && \text{(definitions \refequ{equ:1-introduction-spectral-density-as-trace} and \refequ{equ:2-chebyshev-DGC-spectral-density-chebyshev-expansion})} \notag \\
        &= \left| \sum_{i=1}^N \left(g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i)\right) \right|
        && \text{(property of matrix function)} \notag \\
        &\leq N \max_{i = 1, \dots, n} \left| g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i) \right|
        && \text{(pessimistic upper bound)} \notag \\
        &\leq N \max_{s \in (-1, 1)} \left| g_{\sigma}(t - s) - g_{\sigma}^m(t - s) \right|
        && \text{(extension of domain)} \notag \\
        &\leq N \frac{2}{\chi^m(\chi - 1)} \max_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)|
        && \text{(Bernstein \cite[Theorem~73]{meinardus1967approximation})}
        \label{equ:2-chebyshev-proof-bernstein-general-expression}
    \end{align}
    where in the last step we define the ellipse $\mathcal{E}_{\chi}$
    with foci $\{-1, 1\}$ and with sum of half-axes $\chi = a + b > 1$
    (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Since $g_{\sigma}(t - \cdot)$ of the form \refequ{equ:1-introduction-def-gaussian-kernel}
    is holomorphic, $\chi$ may be chosen arbitrarily.

    \begin{figure}[ht]
        \centering
        \begin{tikzpicture}
            \draw[thick, darkblue, fill=lightblue] (0, 0) ellipse (2.5 and 1.5);
            \draw[->] (0, -2) to (0, 2) node[above] {$\Imag(z)$};
            \draw[->] (-3, 0) to (3, 0) node[right] {$\Real(z)$};
            \fill[darkblue] (-1.5, 0) circle (0.05) node[below] {$-1$};
            \fill[darkblue] (1.5, 0) circle (0.05) node[below] {$+1$};
            \draw[<->, darkblue] (0.1, 0.15) to  node[midway, right] {$b$} (0.1, 1.4);
            \draw[<->, darkblue] (0.15, 0.1) to  node[midway, above] {$a$} (2.4, 0.1);
        \end{tikzpicture}
        \caption{The Bernstein ellipse $\mathcal{E}_{\chi}$ with half axes $a$ and$b$ visualized in $\mathbb{C}$.}
        \label{fig:2-chebyshev-proof-bernstein-ellipse}
    \end{figure}

    Writing $z = x + iy$ for $x,y \in \mathbb{R}$, we estimate (using $|e^z| = e^{\Real(z)}$)
    \begin{equation}
        |g_{\sigma}(t - (x + iy))| %&= \frac{1}{n \sqrt{2 \pi \sigma^2}} \left| e^{- \frac{(t - (x + iy))^2}{2 \sigma^2}} \right| \notag \\
        = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{- \frac{(t - x)^2 - y^2}{2 \sigma^2}}
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{y^2}{2 \sigma^2}}
    \end{equation}

    Expressing $\chi = 1 + \alpha \sigma$ for any $\alpha > 0$,
    we can estimate $\chi - \chi^{-1} \leq 2\alpha\sigma$.
    This can be established by observing
    $h(\chi) = 2\alpha\sigma - \chi + \chi^{-1} = \chi + \chi^{-1} - 2 \geq 0$
    for which $h(1) = 0$ and $h'(\chi) \geq 0$ for all $\chi > 1$.
    Furthermore, because $z$ is
    contained in $\mathcal{E}_{\chi}$ we know that the absolute value of its
    imaginary part is upper bound by the imaginary half axis $b$, which we can
    express in terms of $\chi$ to get [cite or proof]
    \begin{equation}
        |y| \leq b = \frac{\chi - \chi^{-1}}{2} \leq \alpha\sigma
    \end{equation}

    Consequently, for all $t \in \mathbb{R}$
    \begin{equation}
        \max_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)| 
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}
    \end{equation}

    Plugging this estimate into \refequ{equ:2-chebyshev-proof-bernstein-general-expression}, we get
    \begin{equation}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        \leq N \frac{2}{(1 + \alpha\sigma)^m\alpha \sigma} \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}
        = \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m}
    \end{equation}
    with $C_1=\sqrt{\frac{2}{\pi}}\frac{1}{\alpha}e^{\frac{\alpha^2}{2}}$ and $C_2=\alpha$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic trace estimation}
\label{sec:2-chebyshev-stochastic-trace-estimation}

Hutchinson \cite{hutchinson1990trace}, $\mtx{B} \in \mathbb{R}^{n \times n}$ Hermitian,
$\vct{\psi} \in \mathbb{R}^N$, $\mathbb{E}[\vct{\psi}] = \vct{0}$, $\mathbb{E}[\vct{\psi}\vct{\psi}^{\top}] = \mtx{I}$ 
\begin{equation}
    \mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}] = \Tr(\mtx{B})
    \label{equ:2-chebyshev-DGC-hutchinson}
\end{equation}

Variance of Hutchinson \cite[Proposition~1]{hutchinson1990trace} \textcolor{red}{(only for Rademacher, else 2 times Frobenius norm)}
\begin{equation}
    \operatorname{Var}(\vct{\psi}^{\top} \mtx{B} \vct{\psi}) = 2 \sum_{i \neq j} \operatorname{Re}(b_{ij})^2
    \label{equ:2-chebyshev-DGC-hutchinson-variance}
\end{equation}

Taking a certain \glsfirst{num-random-vectors} to form the
\glsfirst{sketching-matrix} $= [\vct{\psi}_1, \dots, \vct{\psi}_{n_v}] \in \mathbb{R}^{n \times n_v}$

\begin{equation}
    H_{n_v}(\mtx{B}) = \frac{1}{n_v} \Tr(\mtx{\Psi}^{\top} \mtx{B} \mtx{\Psi})
    \label{equ:2-chebyshev-DGC-hutchionson-estimator}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Delta-Gauss-Chebyshev method}
\label{sec:2-chebyshev-delta-gauss-chebyshev}

The \glsfirst{DGC} method \cite[Algorithm~2]{lin2017randomized} puts together the 
Chebyshev expansion discussed in \refsec{sec:2-chebyshev-interpolation} and 
the stochastic trace estimation in \refsec{sec:2-chebyshev-stochastic-trace-estimation}
to approximate the \glsfirst{smooth-spectral-density} as
\begin{equation}
    \widetilde{\phi}_{\sigma}^m(t) = H_{n_v}(g_{\sigma}^m(t\mtx{I} - \mtx{A}))
    \label{equ:2-chebyshev-DGC-final-estimator}
\end{equation}

Efficient implementation thanks to recursion formula \refequ{equ:2-chebyshev-chebyshev-recursion}.

Trace of product of two matrices $\mtx{B}, \mtx{C} \in \mathbb{R}^{n \times M}$
$\mathcal{O}(MN)$ instead of $\mathcal{O}(M^2N)$
\begin{equation}
    \Tr(\mtx{B}^{\top}\mtx{C}) = \sum_{i=1}^{n} \sum_{j=1}^{M} B_{ij} C_{ij}.
    \label{equ:2-chebyshev-fast-trace}
\end{equation}

\begin{algo}{Delta-Gauss-Chebyshev method}{DGC}
    Hermitian matrix $\mtx{A} \in \mathbb{R}^{n \times n}$, number of random vectors $n_v$,
    evaluation points $\{t_i\}_{i=1}^{n_t}$
    \begin{algorithmic}[1]
        \State Compute $\{\mu_l(t_i)\}_{l=0}^m$ for all $t_i$ using \refequ{equ:2-chebyshev-chebyshev-DCT}
        \State Generate \glsfirst{sketching-matrix} $\in \mathbb{R}^{n \times n_v}$
        \State Initialize $[\mtx{V}_1, \mtx{V}_2, \mtx{V}_3] \gets [\mtx{0}_{n \times n_v}, \mtx{\Psi}, \mtx{0}_{n \times n_v}]$
        \State Set ${\phi}_{\sigma}^m(t_i) \gets 0$ for $i=1,\dots,n_t$
        \For {$l = 0, \dots, m$}
          \State $x \gets \Tr(\mtx{\Psi}^{\top} \mtx{V}_2)$
          \For {$i = 1, \dots, n_t$}
            \State $\widetilde{\phi}_{\sigma}^m(t_i) \gets \widetilde{\phi}_{\sigma}^m(t_i) + \mu_l(t_i) x$
          \EndFor
          \State $\mtx{V}_3 \gets (2 - \delta_{l0}) \mtx{A} \mtx{V}_2 - \mtx{V}_1$ \Comment{Chebyshev recurrence \refequ{equ:2-chebyshev-chebyshev-recursion}}
          \State $\mtx{V}_1 \gets \mtx{V}_2, \mtx{V}_2 \gets \mtx{V}_3$
        \EndFor
    \end{algorithmic}
\end{algo}

Cost of matvec $c$, usually $\mathcal{O}(c(N)) = N^2$ for dense and
$\mathcal{O}(c(N)) = N$ for sparse matrices

Computational complexity: $\mathcal{O}(m \log(m) n_t + m c(N) n_v)$

Additional storage complexity: $\mathcal{O}(m n_t + N n_v)$

[Summary box, formula, complexity, \dots]