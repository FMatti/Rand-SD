\chapter{Interpolation and trace estimation}
\label{chp:2-chebyshev}

For many \glsfirst{smoothing-kernel}, as is for example the case with Gaussian smoothing
\refequ{equ:1-introduction-def-gaussian-kernel}, we cannot compute the matrix function
\begin{equation}
    g_{\sigma}(t\mtx{I}_n - \mtx{A})
    \label{equ:2-chebyshev-matrix-function}
\end{equation}
involved in \refequ{equ:1-introduction-spectral-density-as-trace} without first
diagonalizing $\mtx{A}$, which can be prohibitively expensive for large
$\mtx{A}$. A way around this problem is to refrain from trying to assemble the
matrix function explicitly and instead use the fact that thanks to
\refequ{equ:1-introduction-spectral-density-as-trace} we are only interested
in its trace. Approximating the trace can be done by multiplying the matrix
with random vectors, to then -- for example -- evaluate the Girard-Hutchinson trace estimator \cite{hutchinson1990trace}.
The multiplication of a matrix function with a vector can
often be determined quite efficiently using Krylov subspace methods
\cite[chapter~13.2]{higham2008functions}. Another way in which products of matrix
functions with vectors can be computed involves
expanding the function in terms of a finite set of orthogonal polynomials
and subsequently using a recurrence relation to efficiently construct the result.
This approach turns out to be particularly effective when we work with matrix
functions which smoothly depend on a parameter within a bounded interval, and if we
want to evaluate the function at a large number of values of this parameter.
In this chapter we will analyze one such expansion, the Chebyshev expansion,
which gives rise to an efficient method for approximating the spectral density,
particularly when the \gls{num-evaluation-points} is large.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chebyshev interpolation}
\label{sec:2-chebyshev-interpolation}

The Chebyshev interpolation framework is best known for its stability, beneficial
convergence properties, and simple three-term recurrence relation
\refequ{equ:2-chebyshev-chebyshev-recursion} which can
be exploited to efficiently compute products of Chebyshev polynomials with
vectors.\\

At the foundation of Chebyshev interpolation lie the \glspl{chebyshev-polynomial}.
They are defined for all $l \in \mathbb{N}$ as \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    \begin{cases}
        T_l: [-1, 1] \to [-1, 1] \\
        T_l(s) = \cos(l \arccos(s)).
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-definition}
\end{equation}
They satisfy the three-term recurrence relation
\begin{equation}
    \begin{cases}
        T_0(s) = 1 & l = 0, \\
        T_1(s) = s & l = 1, \\
        T_{l}(s) = 2s T_{l-1}(s) - T_{l-2}(s) & l \geq 2,
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-recursion}
\end{equation}
which can be shown using their definition \refequ{equ:2-chebyshev-chebyshev-definition}
and a standard trigonometric identity.\\

A function $f:[-1, 1] \to \mathbb{R}$ can be expanded in a basis of Chebyshev
polynomials up to \gls{chebyshev-degree} $\in \mathbb{N}$ \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    f^{(m)}(s) = \sum_{l=0}^{m} \mu_l T_l(s).
    \label{equ:2-chebyshev-chebyshev-expansion-general}
\end{equation}
For functions $f$ which can be analytically extended in a certain neighborhood
of $[-1, 1]$ in the complex plane, Bernstein's theorem \cite[theorem~4.3]{trefethen2008gauss}
establishes that the convergence of this expansion in the $L^{\infty}$-norm is
exponential.\\

The coefficients $\{\mu_l\}_{l=0}^{m}$ in \refequ{equ:2-chebyshev-chebyshev-expansion-general}
could be computed using the orthogonality of the Chebyshev polynomials with respect
to a certain inner product, and some authors indeed suggest approximating the
involved integral using a quadrature rule with evenly spaced nodes
\cite[equation~8]{lin2017randomized}.
However, we cannot find a good theoretical guarantee that this will be accurate.
Instead, we use a significantly simpler, faster, and provably accurate way of
computing the coefficients of the Chebyshev expansion of a function
$f:[-1,1] \to \mathbb{R}$ \cite{trefethen2019chebyshev, ahmed1970chebyshev}:
If the values $f^{(m)}(s_i)$ at some $m+1$ distinct points $\{s_i\}_{i=0}^m$ are
known, the coefficients $\{\mu_l\}_{l=0}^{m}$ of this polynomial are
uniquely determined \cite{gauss1799demonstratio}. For the choice $s_i = \cos(\pi i/m), i=0,\dots,m$,
\refequ{equ:2-chebyshev-chebyshev-expansion-general} reads
\begin{equation}
    f^{(m)}(s_i) = \sum_{l=0}^{m} \mu_l \cos\left(\frac{\pi i l}{m}\right),
    \label{equ:2-chebyshev-chebyshev-nodes-evaluation}
\end{equation}
which coincides with a \glsfirst{DCT}\footnote{There exist multiple conventions for the \gls{DCT}.
The one which we use is (up to scaling of the first and last coefficient)
referred to as a type I \gls{DCT}, and is efficiently implemented in the SciPy Python package:
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}.} of the coefficients $\{\mu_l\}_{l=0}^{m}$.
Thus, if we collect the coefficients $\{\mu_l\}_{l=0}^{m}$ in a vector $\vct{\mu} \in \mathbb{R}^{m+1}$ 
and the function evaluations $\{f^{(m)}(s_i)\}_{i=0}^{m}$ in another
vector $\vct{f} \in \mathbb{R}^{m+1}$, we can pass back and forth between the
two with
\begin{equation}
    \vct{f} = \DCT \{ \vct{\mu} \} \implies \vct{\mu} = \DCT^{-1}\{ \vct{f} \}.
    \label{equ:2-chebyshev-chebyshev-DCT}
\end{equation}
In short, computing the coefficients of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion-general}
of the function $f$ amounts to evaluating this function at $m+1$ well
chosen points and computing the inverse \gls{DCT}. The corresponding algorithm
can be found in \refalg{alg:2-chebyshev-chebyshev-expansion}.
This procedure is usually inexpensive and can be done in $\mathcal{O}(m \log(m))$
operations \cite{makhoul1980fct}.

\begin{algo}{Chebyshev expansion}{2-chebyshev-chebyshev-expansion}
    \input{algorithms/chebyshev_expansion.tex}
\end{algo}

To demonstrate the higher efficiency of this \gls{DCT}-based method, we time it
against the corresponding algorithm from \cite{lin2017randomized}. The results
can be seen in \reftab{tab:2-chebyshev-timing-interpolation}.

\begin{table}[ht]
    \caption{Comparison of the runtime in milliseconds of the two approaches for computing the coefficients
        of the Chebyshev expansion of a function. We average over 7 runs of the
        algorithms and repeat these runs 1000 times to form the mean and standard
        deviation which are given in the below table. We refer to
        \cite[algorithm~1]{lin2017randomized} with \enquote{quadrature}
        and to \refalg{alg:2-chebyshev-chebyshev-expansion} with \enquote{DCT}.
        For each algorithm, we interpolate a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$,
        at \gls{num-evaluation-points} $=1000$ points, for various values of \gls{chebyshev-degree}.}
    \label{tab:2-chebyshev-timing-interpolation}
   \input{tables/timing_interpolation.tex}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic trace estimation}
\label{sec:2-chebyshev-stochastic-trace-estimation}

Matrix-free stochastic trace estimation is most useful when a matrix is not given
explicitly, but products of this matrix with vectors can be computed
efficiently. Examples of such scenarios are traces of matrix functions
\cite{ubaru2017lanczos,epperly2023xtrace} or of implicit matrices which can only
be queried through matrix-vector products \cite{ghorbani2019investigation,adepu2021hessian}.
Most algorithms for stochastic trace estimation are based on the Girard-Hutchinson
trace estimator, which we will discuss in the following paragraphs.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Constant matrices}
\label{subsec:2-chebyshev-trace-constant}

For a symmetric matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ and a standard Gaussian
random vector $\vct{\psi} \in \mathbb{R}^n$, the quadratic form $\vct{\psi}^{\top} \mtx{B} \vct{\psi}$ 
is an unbiased estimate of the trace:
\begin{equation}
    \mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}]
        = \mathbb{E}\left[\sum_{i=1}^n\sum_{j=1}^n \psi_i b_{ij} \psi_j\right]
        = \sum_{i=1}^n\sum_{j=1}^n b_{ij} \mathbb{E}[\psi_i\psi_j]
        = \sum_{i=1}^n b_{ii}
        = \Tr(\mtx{B}).
    \label{equ:2-chebyshev-DGC-hutchinson}
\end{equation}
Furthermore, the variance of this estimate is bounded by the Frobenius norm of the matrix
$\mtx{B}$:
\begin{align*}
    \Var(\vct{\psi}^{\top} \mtx{B} \vct{\psi})
        &= \Var(\vct{\psi}^{\top} \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} \vct{\psi}) && \text{(spectral decomposition of $\mtx{B}$)} \notag \\
        &= \Var(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}) && \text{($\mtx{U}^{\top}\vct{\psi} = \widetilde{\vct{\psi}} \sim \vct{\psi}$)} \notag \\
        &= \mathbb{E}[(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}})^2] - \mathbb{E}[\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}]^2 && \text{(definition of variance)} \notag \\
        &= \mathbb{E}\bigg[\bigg(\sum_{i=1}^{n} \widetilde{\psi}_i^2 \lambda_i\bigg)^2\bigg] - \Tr(\mtx{B})^2 && \text{($\mtx{\Lambda}$ diagonal and \refequ{equ:2-chebyshev-DGC-hutchinson})} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j \mathbb{E}[\widetilde{\psi}_i^2 \widetilde{\psi}_j^2] - \Tr(\mtx{B})^2 && \text{(linearity of $\mathbb{E}$)} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j + 2 \sum_{i=1}^{n} \lambda_i^2 - \Tr(\mtx{B})^2 && \text{($\mathbb{E}[\widetilde{\psi}_i^2]=1$ and $\mathbb{E}[\widetilde{\psi}_i^4]=3$)} \notag \\
        &= \Tr(\mtx{B})^2 + 2 \lVert \mtx{B} \rVert _F^2 - \Tr(\mtx{B})^2 && \text{($\sum_{i=1}^{n} \lambda_i = \Tr(\mtx{B})$, $\sum_{i=1}^{n} \lambda_i^2 = \lVert \mtx{B} \rVert _F^2$)} \notag \\
        &= 2 \lVert \mtx{B} \rVert _F^2.
\end{align*}
The idea of the Girard-Hutchinson trace estimator is to compute multiple such estimates
for different, independent random vectors and take the average. This will again
be an unbiased estimate of the trace, but with the reduced variance
\begin{equation}
    \Var\left( \frac{1}{n_{\Psi}} \sum_{j=1}^{n_{\Psi}}\vct{\psi}_j^{\top} \mtx{B} \vct{\psi}_j\right) = \frac{2}{n_{\Psi}} \lVert \mtx{B} \rVert _F^2
    \label{equ:2-chebyshev-hutchinson-mse}
\end{equation}
with the \gls{num-hutchinson-queries} $\in \mathbb{N}$.
Collecting the \gls{num-hutchinson-queries} independent random vectors
$\vct{\psi}_i \in \mathbb{R}^{n}$ in the standard Gaussian
\glsfirst{random-matrix} $= [\vct{\psi}_1, \dots, \vct{\psi}_{n_{\Psi}}] \in \mathbb{R}^{n \times n_{\Psi}}$,
we can then rewrite the Girard-Hutchinson trace estimator as
\begin{equation}
    \Hutch_{n_{\Psi}}(\mtx{B}) = \frac{1}{n_{\Psi}} \Tr(\mtx{\Psi}^{\top} \mtx{B} \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Parameter-dependent matrices}
\label{subsec:2-chebyshev-trace-parametrized}

In the case where the matrix, or -- alternatively said -- all its entries, continuously depends on a
parameter in a bounded interval, we can analogously define the Girard-Hutchinson
estimator for parameter-dependent matrices
\begin{equation}
    \Hutch_{n_{\Psi}}(\mtx{B}(t)) = \frac{1}{n_{\Psi}} \Tr(\mtx{\Psi}^{\top} \mtx{B}(t) \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}
\end{equation}
As the counterpart of the variance in the parametrized case, we measure the
error of this estimate in the $L^1$-norm, for which we can use a result
from \cite{he2023parameter}, which we will state in the following lemma.
\begin{lemma}{$L^1$-error of parameter-dependent Girard-Hutchinson estimator}{2-chebyshev-parameter-hutchinson}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ symmetric and continuous in
    $t \in [a, b]$, $\delta \in (0, e^{-1})$, and $n_{\Psi} \in \mathbb{N}$.
    Let $\Hutch_{n_{\Psi}}(\mtx{B}(t))$ be the $n_{\Psi}$-query
    Girard-Hutchinson estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
    With the constant $c_{\Psi} = 24e$, it holds with probability $\geq 1 - \delta$
    \begin{equation}
        \int_{a}^{b} \left| \Tr(\mtx{B}(t)) - \Hutch_{n_{\Psi}}(\mtx{B}(t)) \right| \mathrm{d}t \leq c_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    \end{equation}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Delta-Gauss-Chebyshev method}
\label{sec:2-chebyshev-delta-gauss-chebyshev}

Now we have all the ingredients for constructing a first algorithm to approximate
the expression \refequ{equ:1-introduction-spectral-density-as-trace}:
the Chebyshev expansion of a function (\refalg{alg:2-chebyshev-chebyshev-expansion})
and the Girard-Hutchinson trace estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
For a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$ with eigenvalues
contained in $[-1, 1]$
we expand the \glsfirst{smoothing-kernel} in terms of Chebyshev polynomials, such that
\begin{equation}
    g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) = \sum_{l=0}^{m} \mu_l(t) T_l(\mtx{A}).
    \label{equ:2-chebyshev-chebyshev-expansion}
\end{equation}
Plugging this expansion into \refequ{equ:1-introduction-spectral-density-as-trace}
gives us the expanded spectral density
\begin{equation}
    \phi_{\sigma}^{(m)}(t) = \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})).
    \label{equ:2-chebyshev-spectral-density-as-trace-expansion}
\end{equation}\\

By combining the Chebyshev expansion \refequ{equ:2-chebyshev-spectral-density-as-trace-expansion}
with stochastic trace estimation
we end up with the \glsfirst{DGC} method \cite[algorithm~2]{lin2017randomized},
which approximates the \glsfirst{smooth-spectral-density} as
\begin{equation}
    \widetilde{\phi}_{\sigma}^{(m)}(t) = H_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) = \frac{1}{n_{\Psi}} \sum_{l=0}^m \mu_l(t) \Tr(\mtx{\Psi}^{\top} T_l(\mtx{A}) \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-final-estimator}
\end{equation}
Apparently, it is rather cheap to evaluate $\widetilde{\phi}_{\sigma}^{(m)}(t)$
at multiple values of \gls{spectral-parameter}, since only the coefficients
of the linear combination of $\{\Tr(\mtx{\Psi}^{\top} T_l(\mtx{A}) \mtx{\Psi})\}_{l=0}^m$
change, which can easily be computed using \refalg{alg:2-chebyshev-chebyshev-expansion}.\\

An efficient implementation can be achieved thanks to the recurrence relation
\refequ{equ:2-chebyshev-chebyshev-recursion} which the Chebyshev polynomials satisfy.
However, it is usually prohibitively expensive to interpolate a big matrix
$\mtx{A}$ as a whole, since alone the matrix-matrix multiplication in each step
of the recurrence can cost up to $\mathcal{O}(n^3)$, and the evaluation
of the expansion at \gls{num-evaluation-points} values of \gls{spectral-parameter} could
cost a further $\mathcal{O}(n^2 m n_t)$ operations. Therefore, in case we are only interested
in the result of a linear mapping applied to the interpolant, a significant speed-up can be
achieved by directly interpolating the result of this linear mapping applied to
the interpolant (\reflin{lin:2-chebyshev-linear-mapping} in \refalg{alg:2-chebyshev-DGC}).
In \reffig{fig:2-chebyshev-sketched-interpolation} some examples
of such linear mappings -- which we will make use of later on -- are schematically
illustrated.\\
\begin{figure}[ht]
    \centering
    \input{figures/sketched_interpolation.tex}
    \caption{Schematic illustration of linear mappings which, applied to a large
        matrix $\mtx{A}$, reduce the dimensionality of the interpolation problem.}
    \label{fig:2-chebyshev-sketched-interpolation}
\end{figure}

Finally, we can give the pseudocode
of this first method in \refalg{alg:2-chebyshev-DGC}.
\begin{algo}{Delta-Gauss-Chebyshev method}{2-chebyshev-DGC}
    \input{algorithms/delta_gauss_chebyshev.tex}
\end{algo}

Denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and
$\mathcal{O}(c(n)) = n$ for sparse matrices, we determine the computational
complexity of the \gls{DGC} method to be $\mathcal{O}(m \log(m) n_t + m n_{\Psi} c(n))$,
with $\mathcal{O}(m n_t + n n_{\Psi})$ required additional storage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation details}
\label{subsec:2-chebyshev-implementation-details}

Expression \refequ{equ:2-chebyshev-chebyshev-expansion} is only well-defined for matrices whose spectra are fully
contained in $[-1, 1]$. To also use the \gls{DGC} method on matrices $\mtx{A}$ whose
spectra we know, or can estimate \cite{lin2016review, zhou2011spectrum}, to be
within a different interval $[a, b] \subset \mathbb{R}$,
we can define a \gls{spectral-transformation} as the linear mapping
\begin{equation}
    \begin{cases}
        \tau : [a, b] \to [-1, 1], \\
        \tau(t) = \frac{2t - a - b}{b - a}.
    \end{cases}
    \label{equ:2-chebyshev-spectral-transformation}
\end{equation}
The \gls{DGC} method can then be applied to $\bar{\mtx{A}} = \tau(\mtx{A})$ whose
spectrum is contained in $[-1, 1]$.\\

However, retrieving the \glsfirst{smooth-spectral-density} of the original
matrix $\mtx{A}$ after this transformation is not straight-forward
and is usually swept under the rug in literature.
Let us call $\bar{\phi}$ the spectral density of $\bar{\mtx{A}}$.
Based on a derivation from \refapp{chp:A-appendix}, it turns out that in order
to approximate $\phi_{\sigma}$ of a general matrix $\mtx{A}$
for the \glsfirst{smoothing-kernel} we consider
in this thesis (Gaussian, Lorentzian), we only need to rescale their
\glsfirst{smoothing-parameter} to
\begin{equation}
    \bar{\sigma} = \frac{2\sigma}{b - a},
    \label{equ:2-chebyshev-sigma-transformation}
\end{equation}
run \refalg{alg:2-chebyshev-DGC} on $\bar{\mtx{A}}$ with $\bar{g}_{\sigma}$
on the transformed evaluation points $\{ \tau(t_i) \}_{i=1}^{n_t}$ and finally
multiply the resulting approximation with $\frac{2}{b-a}$.
In all our examples, this procedure will be used to compute spectral densities
of matrices which have eigenvalues outside of $[-1, 1]$.\\

A speed-up of \refalg{alg:2-chebyshev-DGC} can be achieved by smartly computing the trace of the 
product $\mtx{E}^{\top}\mtx{F}$ of two matrices $\mtx{E}, \mtx{F} \in \mathbb{R}^{N \times M}$ in
$\mathcal{O}(MN)$ instead of $\mathcal{O}(M^2N)$ time, due to the relation
\begin{equation}
    \Tr(\mtx{E}^{\top}\mtx{F}) = \sum_{i=1}^{N} \sum_{j=1}^{M} e_{ij} f_{ij}.
    \label{equ:2-chebyshev-fast-trace}
\end{equation}
This reduces the complexity of \reflin{lin:2-chebyshev-fast-trace}
in \refalg{alg:2-chebyshev-DGC} from $\mathcal{O}(nn_{\Psi}^2)$ to $\mathcal{O}(nn_{\Psi})$.
Throughout this work, as has already be done for the complexity analysis
of \refalg{alg:2-chebyshev-DGC}, we implicitly assume that all traces of this
form are computed using this technique.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Theoretical analysis}
\label{subsec:2-chebyshev-theoretical-analysis}

In order to obtain tractable results and because it is the most common case in
literature, we choose to restrict the analysis in this section to Gaussian 
\gls{smoothing-kernel} \refequ{equ:1-introduction-def-gaussian-kernel}.\\

The convergence of the expansion of a Gaussian \gls{smoothing-kernel} is exponential
and depends on \gls{smoothing-parameter}. This can be seen quite well
in \reffig{fig:2-chebyshev-chebyshev-convergence}, and is proven in the following
lemma.

\begin{lemma}{$L^1$-error of Chebyshev expansion for Gaussian smoothing}{2-chebyshev-error}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix whose spectrum
    is contained in $[-1, 1]$. Then the expansion $g_{\sigma}^{(m)}$ of the Gaussian \glsfirst{smoothing-kernel}
    and the corresponding expansion $\phi_{\sigma}^{(m)}$ of the \glsfirst{smooth-spectral-density} satisfy
    \begin{align}
        \lVert  g_{\sigma} - g_{\sigma}^{(m)} \rVert _{\infty} &\leq \frac{\sqrt{2}}{n \sigma^2} (1 + \sigma)^{-m},
        \label{equ:2-chebyshev-interpolation-sup-error-kernel} \\
        \lVert  \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _{\infty} &\leq \frac{\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m},
        \label{equ:2-chebyshev-interpolation-sup-error} \\
        \lVert  \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1 &\leq \frac{2\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m}.
        \label{equ:2-chebyshev-interpolation-error}
    \end{align}
    for all \gls{smoothing-parameter} $>0$.
\end{lemma}

\begin{figure}[ht]
    \centering
    \input{figures/chebyshev_convergence.tex}
    \caption{The error of the Chebyshev expansion of increasing \glsfirst{chebyshev-degree}
    for a Gaussian \gls{smoothing-kernel} with different values of \gls{smoothing-parameter}.}
    \label{fig:2-chebyshev-chebyshev-convergence}
\end{figure}

This result is a consequence of Bernstein's theorem \cite[theorem~4.3]{trefethen2008gauss}.
A proof of a similar result can be found in \cite[theorem~2]{lin2017randomized}.
However, since our result -- and more so the proof -- deviate from the aforementioned
work, we chose to reproduce it hereafter.
\begin{proof}
    From Bernstein's theorem \cite[theorem~4.3]{trefethen2008gauss}
    and the analyticity of \gls{smoothing-kernel} we know that for all $t \in \mathbb{R}$
    \begin{equation}
        \lVert g_{\sigma}(t - \cdot) - g_{\sigma}^{(m)}(t - \cdot) \rVert _{\infty}
        \leq \frac{2}{\chi^{m}(\chi - 1)} \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)|
        \label{equ:2-chebyshev-convergence-proof-base}
    \end{equation}
    where we can use any ellipse $\mathcal{E}_{\chi}$
    with foci $\{-1, 1\}$ and with sum of half-axes $\chi = a + b > 1$
    (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).

    \begin{figure}[ht]
        \centering
        \input{figures/bernstein_ellipse.tex}
        \caption{A Bernstein ellipse $\mathcal{E}_{\chi}$ with half axis lengths $a$ and
            $b$ visualized in the complex plane $\mathbb{C}$.}
        \label{fig:2-chebyshev-proof-bernstein-ellipse}
    \end{figure}

    Writing $z = x + \iota y$ for $x,y \in \mathbb{R}$, we estimate (using $|e^z| = e^{\Re(z)}$)
    \begin{equation}
        |g_{\sigma}(t - (x + \iota y))|
        = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{- \frac{(t - x)^2 - y^2}{2 \sigma^2}}
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{y^2}{2 \sigma^2}}.
    \end{equation}

    Expressing $\chi = 1 + c \sigma$ for any $c > 0$,
    we can estimate
    \begin{equation}
        \chi - \chi^{-1} \leq 2c\sigma.
        \label{equ:2-chebyshev-bernstein-proof-estimate}
    \end{equation}
    This can be established by observing
    $h(\chi) = 2c\sigma - \chi + \chi^{-1} = 2(\chi - 1) - \chi + \chi^{-1} = \chi + \chi^{-1} - 2 \geq 0$
    because $h(1) = 0$ and $h'(\chi) \geq 0$ for all $\chi > 1$.
    Furthermore, because $z$ is
    contained in $\mathcal{E}_{\chi}$ we know that the absolute value of its
    imaginary part $y$ is upper bound by the length of the imaginary half axis $b$,
    which can be expressed in terms of $\chi$ to show with \refequ{equ:2-chebyshev-bernstein-proof-estimate}
    that
    \begin{equation}
        |y| \leq b = \frac{\chi - \chi^{-1}}{2} \leq c\sigma.
    \end{equation}

    Consequently, for all $t \in \mathbb{R}$
    \begin{equation}
        \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)| 
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{c^2}{2}}.
    \end{equation}

    Plugging this estimate into \refequ{equ:2-chebyshev-convergence-proof-base}
    yields
    \begin{equation}
        \lVert g_{\sigma}(t - \cdot) - g_{\sigma}^{(m)}(t - \cdot) \rVert _{\infty}
        \leq \frac{2}{(1 + c\sigma)^{m}c \sigma} \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{c^2}{2}}
        \label{equ:2-chebyshev-uniform-bound-prelim}
    \end{equation}
    for every $c>0$. In particular, for $c=1$ and with $\sqrt{e/\pi} \leq 1$ we have
    \begin{equation}
        \lVert g_{\sigma}(t - \cdot) - g_{\sigma}^{(m)}(t - \cdot) \rVert _{\infty}
        \leq \frac{\sqrt{2}}{n \sigma^2}  (1 + \sigma)^{-m},
        \label{equ:2-chebyshev-uniform-bound}
    \end{equation}
    which shows the first assertion with $t=0$.\\

    For the second assertion, we may use basic properties of matrix functions to obtain
    \begin{align*}
        &\left| \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \right| \notag \\
        &= \left| \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) - \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \right|
        && \text{(definitions \refequ{equ:1-introduction-spectral-density-as-trace} and \refequ{equ:2-chebyshev-chebyshev-expansion})} \notag \\
        &= \left| \sum_{i=1}^n \left(g_{\sigma}(t - \lambda_i) - g_{\sigma}^{(m)}(t - \lambda_i)\right) \right|
        && \text{($\lambda_1, \dots, \lambda_n$ eigenvalues of $\mtx{A}$)} \notag \\
        &\leq n \max_{i = 1, \dots, n} \left| g_{\sigma}(t - \lambda_i) - g_{\sigma}^{(m)}(t - \lambda_i) \right|
        && \text{(conservative upper bound)} \notag \\
        &\leq n \max_{s \in [-1, 1]} \left| g_{\sigma}(t - s) - g_{\sigma}^{(m)}(t - s) \right|
        && \text{(extension of domain)} \notag \\
        &= n \lVert g_{\sigma}(t - \cdot) - g_{\sigma}^{(m)}(t - \cdot) \rVert _{\infty}
        && \text{(definition of $L^{\infty}$-norm)} \notag \\
        &\leq \frac{\sqrt{2}}{\sigma^2}  (1 + \sigma)^{-m}
        && \text{(using \refequ{equ:2-chebyshev-uniform-bound})} \notag \\
    \end{align*}
    from which the result follows directly.\\

    Finally, H\"older's inequality \cite{klenke2013probability} 
    allows us to also show the last assertion with what we have found above:
    \begin{equation}
        \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1
            \leq 2 \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _{\infty}
            \leq \frac{2\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m}.
    \end{equation}
\end{proof}

We now have all the tools at hand to combine the approximation error of the
Chebyshev expansion (\reflem{lem:2-chebyshev-error}) with the trace estimation
error (\reflem{lem:2-chebyshev-parameter-hutchinson}) to get a tractable theoretical
result for the accuracy of the \gls{DGC} method.

\begin{theorem}{$L^1$-error of Delta-Gauss-Chebyshev method}{2-delta-gauss-chebyshev}
    Let $\widetilde{\phi}_{\sigma}^{(m)}(t)$ be the result from running \refalg{alg:2-chebyshev-DGC}
    on a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$ with its spectrum contained in $[-1, 1]$ using
    a Gaussian \glsfirst{smoothing-kernel} with
    \glsfirst{smoothing-parameter} $>0$, \glsfirst{chebyshev-degree} $\in \mathbb{N}$, and
    \glsfirst{num-hutchinson-queries} $\in \mathbb{N}$. For $\delta \in (0, e^{-1})$ it holds with
    probability $\geq 1-\delta$, that
    \begin{equation}
        \lVert \phi_{\sigma} - \widetilde{\phi}_{\sigma}^{(m)}\rVert _1
        \leq \frac{\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m} \left( 2 + c_{\Psi} \frac{\log(1/\delta)}{\sqrt{n n_{\Psi}}} \right) + c_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}}
    \end{equation}
    for $c_{\Psi} \geq 24e$.
\end{theorem}

\begin{proof}
    First, we apply the triangle inequality to get
    \begin{equation}
        \lVert \phi_{\sigma} - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1
            \leq \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1 + \lVert \phi_{\sigma}^{(m)} - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1.
    \end{equation}
    The first term can be dealt with using \reflem{lem:2-chebyshev-error}.
    \Reflem{lem:2-chebyshev-parameter-hutchinson} can be applied to the second term for
    \begin{align*}
        \lVert \phi_{\sigma}^{(m)} - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1
            &= \int_{-1}^{1} \left| \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) - \Hutch_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \right| \mathrm{d}t && \text{(definitions)} \notag \\
            &\leq c_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \int_{-1}^{1} \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F \mathrm{d}t && \text{(\reflem{lem:2-chebyshev-parameter-hutchinson})}
    \end{align*}
    We proceed with bounding the involved integrand by first applying the triangle
    inequality, then exploiting properties of the Frobenius norm of a matrix function and
    the positivity of \gls{smoothing-kernel}, and finally using the result from the proof
    of \reflem{lem:2-chebyshev-error} and the definition of \gls{smooth-spectral-density}:
    \begin{align*}
        &\lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F \notag \\
        &\leq \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) - g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _F + \lVert g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _F && \text{(triangle inequality)} \notag \\
        &\leq \sqrt{n} \lVert g_{\sigma}^{(m)} - g_{\sigma} \rVert _{\infty} + \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) && \text{(norm inequalities)} \notag \\
        &\leq  \frac{\sqrt{2}}{\sqrt{n}\sigma^2} (1 + \sigma)^{-m} + \phi_{\sigma}(t) && \text{(\reflem{lem:2-chebyshev-error})}
    \end{align*}
    Putting all things together and using the normalization $\int_{-1}^{1} \phi_{\sigma}(t) \mathrm{d}t = 1$
    we end up with the desired result:
    \begin{align*}
        \lVert \phi_{\sigma}  - \widetilde{\phi}_{\sigma}^{(m)} \rVert _1
        &\leq  \frac{2\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m} + c_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \left( \frac{\sqrt{2}}{\sqrt{n}\sigma^2} (1 + \sigma)^{-m} + 1 \right) \notag \\
        &=  \frac{\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m} \left( 2 + c_{\Psi} \frac{\log(1/\delta)}{\sqrt{n n_{\Psi}}} \right) + c_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}}.
    \end{align*}
\end{proof}
 
We see that the first term in \refthm{thm:2-delta-gauss-chebyshev} will quickly
vanish as \gls{chebyshev-degree} increases. What we are left with is the slowly
decaying $\mathcal{O}(n_{\Psi}^{-1/2})$ term. In fact, this is what bottlenecks
the \gls{DGC} method from achieving better accuracies: The Girard-Hutchinson stochastic
trace estimator is not efficient enough for approximating spectral densities.
Therefore, we will consider alternative ways of approximating \refequ{equ:1-introduction-spectral-density-as-trace}
in the next two chapters.
