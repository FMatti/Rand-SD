\chapter{Interpolation and trace estimation}
\label{chp:2-chebyshev}

For many \glsfirst{smoothing-kernel}, as is for example the case with Gaussian smoothing
\refequ{equ:1-introduction-def-gaussian-kernel}, we cannot compute the matrix function
\begin{equation}
    g_{\sigma}(t\mtx{I}_n - \mtx{A})
    \label{equ:2-chebyshev-matrix-function}
\end{equation}
involved in \refequ{equ:1-introduction-spectral-density-as-trace} without
diagonalizing $\mtx{A}$, which can be prohibitively expensive for large
$\mtx{A}$. A way around this problem is to refrain from trying to assemble the
matrix function explicitly and instead use the fact that we are only interested
in its trace. Approximating the trace can be done by multiplying the matrix
with random vectors, to then -- for example -- evaluate the Hutchinson's trace estimator \cite{hutchinson1990trace}.
The multiplication of a matrix function with a vector can
often be determined quite efficiently using Krylov subspace methods
\cite[chapter~13.2]{higham2008functions}. Another way in which products of matrix
functions with vectors can be computed involves
expanding the function in terms of a finite set of orthogonal polynomials
and subsequently using a recurrence relation to efficiently construct the result.
This approach turns out to be particularly effective when we work with matrix
functions which smoothly depend on a parameter within a bounded interval, and if we
want to evaluate the function at a large number of values of this parameter.
In this chapter we will analyze one such expansion, the Chebyshev expansion,
which gives rise to an efficient method for approximating the spectral density,
particularly when the \gls{num-evaluation-points} is large.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chebyshev interpolation}
\label{sec:2-chebyshev-interpolation}

The Chebyshev interpolation framework is best known for its stability, beneficial
convergence properties, and simple three-term recurrence relation
\refequ{equ:2-chebyshev-chebyshev-recursion} which can
be exploited to efficiently compute products of Chebyshev polynomials with
vectors.\\

At the foundation of Chebyshev interpolation lie the \glspl{chebyshev-polynomial}.
They are defined as \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    T_l: [-1, 1] \to [-1, 1],~T_l(s) = \cos(l \arccos(s)).
    \label{equ:2-chebyshev-chebyshev-definition}
\end{equation}
They satisfy the three-term recurrence relation
\begin{equation}
    \begin{cases}
        T_0(s) = 1, & l = 0 \\
        T_1(s) = s, & l = 1 \\
        T_{l}(s) = 2s T_{l-1}(s) - T_{l-2}(s), & l \geq 2,
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-recursion}
\end{equation}
which can be shown using their definition \refequ{equ:2-chebyshev-chebyshev-definition}
and a standard trigonometric identity.\\

A function $f:[-1, 1] \to \mathbb{R}$ can be expanded in a basis of Chebyshev
polynomials up to \gls{chebyshev-degree} \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    f^{(m)}(s) = \sum_{l=0}^{m} \mu_l T_l(s).
    \label{equ:2-chebyshev-chebyshev-expansion-general}
\end{equation}
For functions $f$ which can be analytically extended in a certain neighborhood
of $[-1, 1]$ in the complex plane, Bernstein's theorem \cite[theorem~4.3]{trefethen2008gauss}
establishes that the convergence of this expansion in the supremum-norm is
exponential.
\begin{theorem}{Bernstein's theorem}{2-chebyshev-bernstein}
    Let $f:[-1, 1] \to \mathbb{R}$ be analytic and bounded in the ellipse $\mathcal{E}_{\chi} \subset \mathbb{C}$
    with foci $\pm 1$ and sum of half axis lengths $\chi = a + b$ (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Then its Chebyshev expansion $f^{(m)}$ with \glsfirst{chebyshev-degree} $\geq 0$ satisfies
    \begin{equation}
        \sup_{s \in [-1, 1]} |f(s) - f^{(m)}(s)| \leq \frac{2}{\chi^{(m)}(\chi-1)} \sup_{z \in \mathcal{E}_{\chi}} |f(z)|.
        \label{equ:2-chebyshev-bernstein-convergence-result}
    \end{equation}
\end{theorem}

\begin{figure}[ht]
    \centering
    \input{figures/bernstein_ellipse.tex}
    \caption{A Bernstein ellipse $\mathcal{E}_{\chi}$ with half axis lengths $a$ and
        $b$ visualized in the complex plane $\mathbb{C}$.}
    \label{fig:2-chebyshev-proof-bernstein-ellipse}
\end{figure}

The coefficients $\{\mu_l\}_{l=0}^{m}$ in \refequ{equ:2-chebyshev-chebyshev-expansion-general}
could be computed using the orthogonality of the Chebyshev polynomials with respect
to a certain inner product, and some authors indeed suggest approximating the
involved integral using a quadrature rule \cite[algorithm~1]{lin2017randomized}.
However, there is no good theoretical guarantee that this will be accurate. Instead, we
have found a significantly simpler, faster, and provably accurate method:
Note that polynomials of degree $m$ are uniquely defined by $m+1$
evaluations of the polynomial at distinct points \cite{gauss1799demonstratio}.
Hence, if we know the values $f^{(m)}(s_i)$ at some $m+1$ distinct points 
$\{s_i\}_{i=0}^(m)$, we can uniquely determine the coefficients $\{\mu_l\}_{l=0}^{m}$
of this polynomial. For convenience, we may choose $s_i = \cos(\pi i/m), i=0,\dots,m$.
In that case,
\begin{equation}
    f^{(m)}(s_i) = \sum_{l=0}^{m} \mu_l \cos\left(\frac{\pi l i}{m}\right),
    \label{equ:2-chebyshev-chebyshev-nodes-evaluation}
\end{equation}
which coincides with a \glsfirst{DCT}\footnote{There exist multiple conventions for the DCT.
The one which we use is (up to scaling of the first and last coefficient)
referred to as a type 1 DCT, and is efficiently implemented in the SciPy Python package:
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}} of the coefficients $\{\mu_l\}_{l=0}^{m}$.
If we collect the coefficients $\{\mu_l\}_{l=0}^{m}$ in a vector $\vct{\mu} \in \mathbb{R}^{m+1}$ 
and the function evaluations $\{f^{(m)}(s_i)\}_{i=0}^{m}$ in another
vector $\vct{f} \in \mathbb{R}^{m+1}$, we find that
\begin{equation}
    \vct{f} = \DCT \{ \vct{\mu} \} \implies \vct{\mu} = \DCT^{-1}\{ \vct{f} \}.
    \label{equ:2-chebyshev-chebyshev-DCT}
\end{equation}
In short, computing the coefficients of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion-general}
of the function $f$ amounts to evaluating this function at $m+1$ well
chosen points and computing the inverse \gls{DCT}. The corresponding algorithm
can be found in \refalg{alg:2-chebyshev-chebyshev-expansion}
This procedure is usually inexpensive and can be done in $\mathcal{O}(m \log(m))$
operations \cite{makhoul1980fct}.

\begin{algo}{Chebyshev expansion}{2-chebyshev-chebyshev-expansion}
    \input{algorithms/chebyshev_expansion.tex}
\end{algo}

To demonstrate the higher efficiency of this \gls{DCT}-based method, we time it
against the corresponding algorithm from \cite{lin2017randomized}. The results
can be seen in \reftab{tab:2-chebyshev-timing-interpolation}.

\begin{table}[ht]
    \caption{Runtime comparison of the two approaches with which the coefficients
        of the Chebyshev expansion of a function. We average over 7 runs of the
        algorithms and repeat these runs 1000 times to form the mean and standard
        deviation which are given in the below table. We refer to
        \cite[algorithm~1]{lin2017randomized} with \enquote{quadrature}
        and to \refalg{alg:2-chebyshev-chebyshev-expansion} with \enquote{DCT}.
        For each algorithm, we interpolate \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$,
        at \gls{num-evaluation-points} $=1000$ points, for various values of \gls{chebyshev-degree}.}
    \label{tab:2-chebyshev-timing-interpolation}
   \input{tables/timing_interpolation.tex}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic trace estimation}
\label{sec:2-chebyshev-stochastic-trace-estimation}

Matrix-free stochastic trace estimation is most useful when a matrix is not given
explicitly, but products of this matrix with vectors can be computed
efficiently. Examples of such scenarios are traces of matrix functions
\cite{ubaru2017lanczos,epperly2023xtrace} or of implicit matrices which can only
be queried through matrix-vector products \cite{ghorbani2019investigation,adepu2021hessian}.
Most algorithms for stochastic trace estimation are based on the Hutchinson's
trace estimator, which we will discuss in the following paragraphs.\\

\subsection{Constant matrices}
\label{subsec:2-chebyshev-trace-constant}

For a symmetric matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ and a standard Gaussian
random vector $\vct{\psi} \in \mathbb{R}^n$, the quadratic form $\vct{\psi}^{\top} \mtx{B} \vct{\psi}$ 
is an unbiased estimate of the trace:
\begin{equation}
    \mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}]
        = \mathbb{E}\left[\sum_{i=1}^n\sum_{j=1}^n \psi_i b_{ij} \psi_j\right]
        = \sum_{i=1}^n\sum_{j=1}^n b_{ij} \mathbb{E}[\psi_i\psi_j]
        = \sum_{i=1}^n b_{ii}
        = \Tr(\mtx{B}).
    \label{equ:2-chebyshev-DGC-hutchinson}
\end{equation}
Furthermore, the variance of this estimate is bounded by the Frobenius norm of the matrix
$\mtx{B}$:
%Variance of Hutchinson \cite[proposition~1]{hutchinson1990trace} \textcolor{red}{(only for Rademacher, else 2 times Frobenius norm)}
\begin{align*}
    \Var(\vct{\psi}^{\top} \mtx{B} \vct{\psi})
        &= \Var(\vct{\psi}^{\top} \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} \vct{\psi}) && \text{(spectral decomposition of $\mtx{B}$)} \notag \\
        &= \Var(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}) && \text{(Gaussian orthogonal invariance)} \notag \\
        &= \mathbb{E}[(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}})^2] - \mathbb{E}[\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}]^2 && \text{(definition of variance)} \notag \\
        &= \mathbb{E}[(\sum_{i=1}^{n} \widetilde{\psi}_i^2 \lambda_i)^2] - \Tr(\mtx{B})^2 && \text{($\mtx{\Lambda}$ is diagonal and \refequ{equ:2-chebyshev-DGC-hutchinson})} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j \mathbb{E}[\widetilde{\psi}_i^2 \widetilde{\psi}_j^2] - \Tr(\mtx{B})^2 && \text{(linearity of expectation value)} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j + 2 \sum_{i=1}^{n} \lambda_i^2 - \Tr(\mtx{B})^2 && \text{($\mathbb{E}[\widetilde{\psi}_i^2]=1$ and $\mathbb{E}[\widetilde{\psi}_i^4]=3$)} \notag \\
        &= \Tr(\mtx{B})^2 + 2 \lVert \mtx{B} \rVert _F^2 - \Tr(\mtx{B})^2 && \text{(definition of Frobenius norm)} \notag \\
        &= 2 \lVert \mtx{B} \rVert _F^2.
\end{align*}
The idea of the Hutchinson's trace estimator is to compute multiple such estimates
for different, independent random vectors and take the average. This will again
be an unbiased estimate of the trace, but with the reduced variance
\begin{equation}
    \Var\left( \frac{1}{n_{\Psi}} \sum_{j=1}^{n_{\Psi}}\vct{\psi}_j^{\top} \mtx{B} \vct{\psi}_j\right) = \frac{2}{n_{\Psi}} \lVert \mtx{B} \rVert _F^2
    \label{equ:2-chebyshev-hutchinson-mse}
\end{equation}
with the \gls{num-hutchinson-queries} $\in \mathbb{N}$.
Collecting the \gls{num-hutchinson-queries} independent random vectors
$\vct{\psi}_i \in \mathbb{R}^{n}$ in the standard Gaussian
\glsfirst{random-matrix} $= [\vct{\psi}_1, \dots, \vct{\psi}_{n_{\Psi}}] \in \mathbb{R}^{n \times n_{\Psi}}$,
the Hutchinson's trace estimator can then be written as
\begin{equation}
    \Hutch_{n_{\Psi}}(\mtx{B}) = \frac{1}{n_{\Psi}} \Tr(\mtx{\Psi}^{\top} \mtx{B} \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator}
\end{equation}\\

\subsection{Parameter-dependent matrices}
\label{subsec:2-chebyshev-trace-parametrized}

In the case where the matrix, or alternatively said all its entries, continuously depends on a
parameter in a bounded interval, we can analogously define the Hutchinson's
estimator for parameter-dependent matrices
\begin{equation}
    \Hutch_{n_{\Psi}}(\mtx{B}(t)) = \frac{1}{n_{\Psi}} \Tr(\mtx{\Psi}^{\top} \mtx{B}(t) \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}
\end{equation}
As the counterpart of the variance in the parametrized case, we measure the
error of this estimate in the $1$-norm, for which we can use a result
from \cite{he2023parameter}, which we will state in the following lemma:
\begin{lemma}{$L^1$-error of parameter-dependent Hutchinson's estimator}{2-chebyshev-parameter-hutchinson}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ symmetric and continuous in
    $t \in [a, b]$, $\delta \in (0, e^{-1})$, and $n_{\Psi} \in \mathbb{N}$.
    Let $\Hutch_{n_{\Psi}}(\mtx{B}(t))$ be the $n_{\Psi}$-query
    Hutchinson's estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
    For a constant $C_{\Psi}$, it holds with probability $\geq 1 - \delta$
    \begin{equation}
        \int_{a}^{b} \left| \Hutch_{n_{\Psi}}(\mtx{B}(t))  - \Tr(\mtx{B}(t)) \right| \mathrm{d}t \leq C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    \end{equation}
    %So, if $n_{\Psi} = \mathcal{O}\left( \frac{\log(2/\delta)}{\varepsilon^2} \right)$ then, with probability $\geq 1 - \delta$
    %\begin{equation}
    %    \int |H_l(\mtx{B}(t)) - \Tr(\mtx{B}(t))| \mathrm{d}t \leq \varepsilon \int \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    %\end{equation}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Delta-Gauss-Chebyshev method}
\label{sec:2-chebyshev-delta-gauss-chebyshev}

Now we have all the ingredients for constructing a first algorithm to approximate
the expression \refequ{equ:1-introduction-spectral-density-as-trace}:
the Chebyshev expansion of a function (\refalg{alg:2-chebyshev-chebyshev-expansion})
and the Hutchinson's trace estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
We expand the \glsfirst{smoothing-kernel} in terms of Chebyshev polynomials, such that
\begin{equation}
    g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) = \sum_{l=0}^{m} \mu_l(t) T_l(\mtx{A}).
    \label{equ:2-chebyshev-chebyshev-expansion}
\end{equation}
Plugging this expansion into \refequ{equ:1-introduction-spectral-density-as-trace}
gives us the expanded spectral density
\begin{equation}
    \phi_{\sigma}^{(m)}(t) = \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})).
    \label{equ:2-chebyshev-spectral-density-as-trace-expansion}
\end{equation}\\

By combining the Chebyshev expansion \refequ{equ:2-chebyshev-spectral-density-as-trace-expansion}
with stochastic trace estimation
we end up with the \glsfirst{DGC} method \cite[algorithm~2]{lin2017randomized},
which approximates the \glsfirst{smooth-spectral-density} as
\begin{equation}
    \widetilde{\phi}_{\sigma}^{(m)}(t) = H_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) = \frac{1}{n_{\Psi}} \sum_{l=0}^m \mu_l(t) \Tr(\mtx{\Psi}^{\top} T_l(\mtx{A}) \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-final-estimator}
\end{equation}
Apparently, it is rather cheap to evaluate $\widetilde{\phi}_{\sigma}^{(m)}(t)$
at multiple values of \gls{spectral-parameter}, since only the coefficients
of the linear combination of $\{\Tr(\mtx{\Psi}^{\top} T_l(\mtx{A}) \mtx{\Psi})\}_{l=0}^m$
change, which can be efficiently computed using \refalg{alg:2-chebyshev-chebyshev-expansion}.\\

An efficient implementation can be achieved thanks to the recurrence relation
which the Chebyshev polynomials satisfy \refequ{equ:2-chebyshev-chebyshev-recursion}.
However, it is usually prohibitively expensive to interpolate a big matrix
$\mtx{A}$ as a whole, since alone the matrix-matrix multiplication in each step
of the recurrence can cost up to $\mathcal{O}(n^3)$, and the evaluation
of the expansion at \gls{num-evaluation-points} values of \gls{spectral-parameter} could
cost a further $\mathcal{O}(n^2 m n_t)$ operations. Therefore, in case we are only interested
in evaluating a linear mapping applied to the interpolant, a significant speed-up can be
achieved by directly interpolating the result of this linear mapping applied to
the interpolant (\reflin{lin:2-chebyshev-linear-mapping} in \refalg{alg:2-chebyshev-DGC}).
In \reffig{fig:2-chebyshev-sketched-interpolation} some examples
of such linear mappings -- which we will make use of later on -- are schematically
illustrated.\\
\begin{figure}[ht]
    \centering
    \input{figures/sketched_interpolation.tex}
    \caption{Schematic illustration of linear mappings which, applied to a large
        matrix $\mtx{A}$, reduce the dimensionality of the interpolation problem.}
    \label{fig:2-chebyshev-sketched-interpolation}
\end{figure}

Finally, we can give the pseudocode
of this first method in \refalg{alg:2-chebyshev-DGC}.
\begin{algo}{Delta-Gauss-Chebyshev method}{2-chebyshev-DGC}
    \input{algorithms/delta_gauss_chebyshev.tex}
\end{algo}

Denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and
$\mathcal{O}(c(n)) = n$ for sparse matrices, we determine the computational
complexity of the \gls{DGC} method to be $\mathcal{O}(m \log(m) n_t + m n_{\Psi} c(n))$,
with $\mathcal{O}(m n_t + n n_{\Psi})$ required additional storage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation-details}
\label{subsec:2-chebyshev-implementation-details}

Expression \refequ{equ:2-chebyshev-chebyshev-expansion} is only well-defined for matrices whose spectra are fully
contained in $[-1, 1]$. To also apply the \gls{DGC} method for matrices whose
spectra we know (or can estimate) to be within $[a, b]$, we can define a
\gls{spectral-transformation} as follows
\begin{equation}
    \tau(t) = \frac{2t - a - b}{b - a},
    \label{equ:2-chebyshev-spectral-transformation}
\end{equation}
and instead expand $\bar{g}_{\bar{\sigma}}^{(m)} = g_{\sigma}^{(m)} \circ \tau^{-1}$ 
from which we can retrieve $g_{\sigma}^{(m)} = \bar{g}_{\bar{\sigma}}^{(m)} \circ \tau$.
Note that under this transformation also the \gls{smoothing-parameter} needs to be
rescaled to
\begin{equation}
    \bar{\sigma} = |\tau'| \sigma = \frac{2\sigma}{b - a}
    \label{equ:2-chebyshev-sigma-transformation}
\end{equation}
in order to get a consistent result.\\

A speed-up can be achieved by smartly computing the trace of the 
product of two matrices $\mtx{B}, \mtx{C} \in \mathbb{R}^{N \times M}$ in
$\mathcal{O}(MN)$ instead of $\mathcal{O}(M^2N)$ time, due to the relation
\begin{equation}
    \Tr(\mtx{B}^{\top}\mtx{C}) = \sum_{i=1}^{N} \sum_{j=1}^{M} b_{ij} c_{ij}.
    \label{equ:2-chebyshev-fast-trace}
\end{equation}
This reduces the complexity of \reflin{lin:2-chebyshev-fast-trace}
in \refalg{alg:2-chebyshev-DGC} from $\mathcal{O}(n^2n_{\Psi})$ to $\mathcal{O}(nn_{\Psi})$.
Throughout the rest of this work we implicitly assume that all traces of this
form are computed using this technique.

\subsection{Theoretical analysis}
\label{subsec:2-chebyshev-theoretical-analysis}

The convergence of the expansion of \gls{smoothing-kernel} is exponential
and depends on \gls{smoothing-parameter}. This can be seen quite well
in \reffig{fig:2-chebyshev-chebyshev-convergence}, and is proven in the following
lemma.

\begin{lemma}{$L^1$-error of Chebyshev expansion for Gaussian smoothing}{2-chebyshev-error}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix whose spectrum
    is contained in $[-1, 1]$. Then
    \begin{equation}
        \lVert  \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1 \leq \frac{C_1}{\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-error}
    \end{equation}
    with constants $C_1, C_2 > 0$ independent of \gls{smoothing-parameter} and \gls{chebyshev-degree}.
\end{lemma}

\begin{figure}[ht]
    \centering
    \input{figures/chebyshev_convergence.tex}
    \caption{The error of Chebyshev expansion of increasing \gls{chebyshev-degree}
    for a Gaussian \gls{smoothing-kernel} with different values of \gls{smoothing-parameter}.}
    \label{fig:2-chebyshev-chebyshev-convergence}
\end{figure}

This result is a consequence of \refthm{thm:2-chebyshev-bernstein}.
A proof of a similar result can be found in \cite[theorem~2]{lin2017randomized}.
However, since our result and more so the proof deviate from the aforementioned
work, we chose to reproduce it hereafter.
\begin{proof}
    Using basic properties of matrix functions, we obtain
    \begin{align}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \right|
        &= \left| \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) - \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \right|
        && \text{(definitions \refequ{equ:1-introduction-spectral-density-as-trace} and \refequ{equ:2-chebyshev-chebyshev-expansion})} \notag \\
        &= \left| \sum_{i=1}^n \left(g_{\sigma}(t - \lambda_i) - g_{\sigma}^{(m)}(t - \lambda_i)\right) \right|
        && \text{(property of matrix function)} \notag \\
        &\leq n \sup_{i = 1, \dots, n} \left| g_{\sigma}(t - \lambda_i) - g_{\sigma}^{(m)}(t - \lambda_i) \right|
        && \text{(pessimistic upper bound)} \notag \\
        &\leq n \sup_{s \in [-1, 1]} \left| g_{\sigma}(t - s) - g_{\sigma}^{(m)}(t - s) \right|
        && \text{(extension of domain)} \notag \\
        &\leq n \frac{2}{\chi^{m}(\chi - 1)} \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)|
        && \text{(Bernstein \cite[theorem~73]{meinardus1967approximation})}
        \label{equ:2-chebyshev-proof-bernstein-general-expression}
    \end{align}
    where in the last step we define the ellipse $\mathcal{E}_{\chi}$
    with foci $\{-1, 1\}$ and with sum of half-axes $\chi = a + b > 1$
    (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Since $g_{\sigma}(t - \cdot)$ of the form \refequ{equ:1-introduction-def-gaussian-kernel}
    is holomorphic, $\chi$ may be chosen arbitrarily.

    Writing $z = x + \iota y$ for $x,y \in \mathbb{R}$, we estimate (using $|e^z| = e^{\Real(z)}$)
    \begin{equation}
        |g_{\sigma}(t - (x + \iota y))| %&= \frac{1}{n \sqrt{2 \pi \sigma^2}} \left| e^{- \frac{(t - (x + iy))^2}{2 \sigma^2}} \right| \notag \\
        = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{- \frac{(t - x)^2 - y^2}{2 \sigma^2}}
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{y^2}{2 \sigma^2}}.
    \end{equation}

    Expressing $\chi = 1 + \alpha \sigma$ for any $\alpha > 0$,
    we can estimate $\chi - \chi^{-1} \leq 2\alpha\sigma$.
    This can be established by observing
    $h(\chi) = 2\alpha\sigma - \chi + \chi^{-1} = \chi + \chi^{-1} - 2 \geq 0$
    for which $h(1) = 0$ and $h'(\chi) \geq 0$ for all $\chi > 1$.
    Furthermore, because $z$ is
    contained in $\mathcal{E}_{\chi}$ we know that the absolute value of its
    imaginary part is upper bound by the imaginary half axis $b$, which can be
    expressed in terms of $\chi$ to get
    \begin{equation}
        |y| \leq b = \frac{\chi - \chi^{-1}}{2} \leq \alpha\sigma.
    \end{equation}

    Consequently, for all $t \in \mathbb{R}$
    \begin{equation}
        \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)| 
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}.
    \end{equation}

    Plugging this estimate into \refequ{equ:2-chebyshev-proof-bernstein-general-expression}, we get
    \begin{equation}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \right|
        \leq n \frac{2}{(1 + \alpha\sigma)^{m}\alpha \sigma} \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}
        = \frac{C_1}{2 \sigma^2} (1 + C_2 \sigma)^{-m},
        \label{equ:2-chebyshev-uniform-bound}
    \end{equation}
    where $C_1=\sqrt{\frac{8}{\pi}}\frac{1}{\alpha}e^{\frac{\alpha^2}{2}}$ and $C_2=\alpha$.

    Finally, Hölder's inequality \cite{klenke2013probability} and 
    \refequ{equ:2-chebyshev-uniform-bound} allow us to show
    \begin{equation}
        \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1
            \leq 2 \sup_{t \in [-1, 1]} \left| \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \right|
            \leq \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m}.
    \end{equation}
\end{proof}

We now have all the tools at hand to combine the approximation error of the
Chebyshev expansion (\reflem{lem:2-chebyshev-error}) with the trace estimation
error (\reflem{lem:2-chebyshev-parameter-hutchinson}) to get a theoretical
result for the accuracy of the \gls{DGC} method.

\begin{theorem}{$L^1$-error of Delta-Gauss-Chebyshev method}{2-delta-gauss-chebyshev}
    Let $\widetilde{\phi}_{\sigma}^{(m)}(t)$ be the result from running \refalg{alg:2-chebyshev-DGC}
    on a matrix $\mtx{A} \in \mathbb{R}^{n \times n}$ with its spectrum contained in $[-1, 1]$ using
    \glsfirst{smoothing-parameter} $>0$, \glsfirst{chebyshev-degree} $\in \mathbb{N}$, and
    \glsfirst{num-hutchinson-queries} $\in \mathbb{N}$. For $\delta \in (0, e^{-1})$ it holds with
    probability $1-\delta$, that
    %Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ symmetric and continuous in
    %$t \in [a, b]$, $\delta \in (0, e^{-1})$, and $n_{\Psi} \in \mathbb{N}$.
    %Let $\Hutch_{n_{\Psi}}(\mtx{B}(t))$ be the $n_{\Psi}$-query
    %Hutchinson estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
    %For a constant $C_{\Psi}$, it holds with probability $\geq 1 - \delta$
    %\int_{-1}^{1} \left| \phi_{\sigma}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t \notag \\
    \begin{equation}
        \lVert \phi_{\sigma} - \widetilde{\phi}_{\sigma}^{(m)}\rVert _1
        \leq \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} \left( 1 + 2 C_{\Psi} \frac{n \log(1/\delta)}{\sqrt{n_{\Psi}}} \right) + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}}
    \end{equation}
    for some constants $C_1$, $C_2$, and $C_{\Psi}$.
    %So, if $n_{\Psi} = \mathcal{O}\left( \frac{\log(2/\delta)}{\varepsilon^2} \right)$ then, with probability $\geq 1 - \delta$
    %\begin{equation}
    %    \int |H_l(\mtx{B}(t)) - \Tr(\mtx{B}(t))| \mathrm{d}t \leq \varepsilon \int \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    %\end{equation}
\end{theorem}

\begin{proof}
    First, we apply the triangle inequality to get
    \begin{equation}
        \lVert \phi_{\sigma}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \rVert _1
            \leq \lVert \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \rVert _1 + \lVert \phi_{\sigma}^{(m)}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \rVert _1.
    \end{equation}
    %\begin{equation}
    %    \int_{a}^{b} \left| \phi_{\sigma}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t
    %        \leq \int_{a}^{b} \left| \phi_{\sigma}(t) - \phi_{\sigma}^{(m)}(t) \right| \mathrm{d}t + \int_{a}^{b} \left| \phi_{\sigma}^{(m)}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t
    %\end{equation}

    The first term can be dealt with using \reflem{lem:2-chebyshev-error}.
    \Reflem{lem:2-chebyshev-parameter-hutchinson} can be applied to the second term for
    \begin{align*}
        \lVert \phi_{\sigma}^{(m)}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \rVert _1
            &= \lVert \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) - \Hutch_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \rVert _1 && \text{(definitions)} \notag \\
            &\leq C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \int_{-1}^{1} \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F \mathrm{d}t && \text{(\reflem{lem:2-chebyshev-parameter-hutchinson})}
    \end{align*}
    %\begin{align}
    %    \int_{a}^{b} \left| \phi_{\sigma}^{(m)}(t) - \widetilde{\phi}_{\sigma}^{(m)}(t) \right| \mathrm{d}t
    %        &= \int_{a}^{b} \left| \Tr(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) - \Hutch_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})) \right| \mathrm{d}t \notag \\
    %        &\leq C_{\Psi} \sqrt{\frac{\log(2/\delta)}{n_{\Psi}}} \int_{a}^{b} \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F \mathrm{d}t \notag \\
    %        %&\leq C_{\Psi} \sqrt{\frac{\log(2/\delta)}{n_{\Psi}}} \int_{a}^{b} \phi_{\sigma}^{(m)}(t) \mathrm{d}t
    %\end{align}
    We proceed with bounding the involved integrand by first applying the triangle
    inequality, then exploiting properties of the Frobenius norm of a matrix function and
    the positivity of \gls{smoothing-kernel}, and finally using the result from the proof
    of \reflem{lem:2-chebyshev-error} and the definition of \gls{smooth-spectral-density}:
    \begin{align*}
        \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \rVert _F
        &\leq \lVert g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) - g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _F + \lVert g_{\sigma}(t\mtx{I}_n - \mtx{A}) \rVert _F && \text{(triangle inequality)} \notag \\
        &\leq \sqrt{n} \sup_{s \in [-1, 1]} | g_{\sigma}^{(m)}(s) - g_{\sigma}(s) | + \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) && \text{(upper bounds)} \notag \\
        &\leq \sqrt{n} \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} + \phi_{\sigma}(t) && \text{(\reflem{lem:2-chebyshev-error})}
    \end{align*}
    Putting all things together and using the normalization $\int_{-1}^{1} \phi_{\sigma}(t) \mathrm{d}t = 1$
    we end up with the desired result:
    \begin{align*}
        \lVert \phi_{\sigma}(t)  - \widetilde{\phi}_{\sigma}^{(m)}(t) \rVert _1
        &\leq \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}} \left( 1 + 2 \sqrt{n} \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} \right) \notag \\
        &= \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m} \left( 1 + 2 C_{\Psi} \frac{n \log(1/\delta)}{\sqrt{n_{\Psi}}} \right) + C_{\Psi} \frac{\log(1/\delta)}{\sqrt{n_{\Psi}}}.
    \end{align*}
\end{proof}

We see that the first term in \refthm{thm:2-delta-gauss-chebyshev} will quickly
vanish as \gls{chebyshev-degree} increases. What we are left with is the slowly
decaying $\mathcal{O}(n_{\Psi}^{-1/2})$ term. In fact, this is what bottlenecks
the \gls{DGC} method from achieving better accuracies: The Hutchinson's stochastic
trace estimator is not efficient enough for approximating spectral densities.
Therefore, we will consider alternative ways of approximating \refequ{equ:1-introduction-spectral-density-as-trace}
in the next two chapters.
