\chapter{Interpolation and trace estimation}
\label{chp:2-chebyshev}

For many \glsfirst{smoothing-kernel}, as is the case for Gaussian smoothing
\refequ{equ:1-introduction-def-gaussian-kernel}, we cannot compute the matrix
\begin{equation}
    g_{\sigma}(t\mtx{I} - \mtx{A})
    \label{equ:2-chebyshev-matrix-function}
\end{equation}
without diagonalizing $\mtx{A}$, which can be prohibitively expensive for large
$\mtx{A}$. Hence, computing the \glsfirst{smooth-spectral-density} 
directly using \refequ{equ:1-introduction-spectral-density-as-trace} is out of
question.\\

A way around this problem is to approximate the trace by multiplying the matrix
with random vectors, e.g. by using the Hutchinson's trace estimator \cite{hutchinson1990trace}.
The approximate result of a multiplication of a matrix function with a vector can
often be determined quite efficiently \todo{[cite matrix function approximation Krylov]}.
One such way, which we will discuss in more detail hereafter, involves
expanding the function in terms of a finite set of orthogonal polynomials which
satisfy a recurrence relation.

\todo{for large \gls{num-evaluation-points} other approaches disqualified}
%Applying a non-polynomial function to a matrix is often quite difficult.
%Disadvantage of Lanczos algorithm?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chebyshev interpolation}
\label{sec:2-chebyshev-interpolation}

Matrix polynomials are comparatively easier to evaluate than most other
matrix functions. For that reason, the approach of first approximating
a function with a polynomial and subsequently evaluating said polynomial at a
matrix can be significantly more efficient.
The Chebyshev interpolation framework is suitable for this due to its stability
and recurrence relation \refequ{equ:2-chebyshev-chebyshev-recursion} which can
be exploited for efficiently evaluating Chebyshev matrix polynomials.\\

At the foundation of Chebyshev interpolation lie the \glspl{chebyshev-polynomial}.
They are defined as \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    T_l: [-1, 1] \to [-1, 1],~T_l(s) = \cos(l \arccos(s)).
    \label{equ:2-chebyshev-chebyshev-definition}
\end{equation}
They satisfy the recurrence relation
\begin{equation}
    \begin{cases}
        T_0(s) = 1, T_1(s) = s \\ T_{l+1}(s) = 2s T_l(s) - T_{l-1}(s), l \geq 2,
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-recursion}
\end{equation}
which can be shown using the definition \refequ{equ:2-chebyshev-chebyshev-definition}
and a standard trigonometric identity.\\

A function $f:[-1, 1] \to \mathbb{R}$ can be expanded in a basis of Chebyshev
polynomials up to \gls{chebyshev-degree} \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    f^m(s) = \sum_{l=0}^m \mu_l T_l(s).
    \label{equ:2-chebyshev-chebyshev-expansion-general}
\end{equation}
For functions $f$ which can be analytically extended in a certain neighborhood
of $[-1, 1]$ in the complex plan, Bernstein's theorem \cite[theorem~73]{meinardus1967approximation}
establishes that the convergence of this expansion in the supremum-norm is
exponential: 
\begin{theorem}{Bernstein's theorem}{2-chebyshev-bernstein}
    Let $f:[-1, 1] \to \mathbb{R}$ be analytic and bounded in the ellipse $\mathcal{E}_{\chi} \subset \mathbb{C}$
    with foci $\pm 1$ and sum of half axes $\chi = a + b$ (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Then
    \begin{equation}
        \sup_{s \in [-1, 1]} |f(s) - f^m(s)| \leq \frac{2}{\chi^m(\chi-1)} \sup_{z \in \mathcal{E}_{\chi} |f(z)|}.
        \label{equ:2-chebyshev-bernstein-convergence-result}
    \end{equation}
\end{theorem}

\begin{figure}[ht]
    \centering
    \input{figures/bernstein_ellipse.tex}
    \caption{The Bernstein ellipse $\mathcal{E}_{\chi}$ with half axes $a$ and$b$ visualized in $\mathbb{C}$.}
    \label{fig:2-chebyshev-proof-bernstein-ellipse}
\end{figure}

The coefficients $\{\mu_l\}_{l=0}^m$ in \refequ{equ:2-chebyshev-chebyshev-expansion-general}
could be computed using the orthogonality of the Chebyshev polynomials, and some
authors suggest computing the in this way emerging integral using a quadrature
rule \cite[algorithm~1]{lin2017randomized}. However, we
have found a significantly simpler and faster approach: Note that polynomials of
degree $m$ are uniquely defined by $m+1$ distinct evaluations \cite{gauss1799demonstratio}.
Hence, if we know the values $f^m(s_i)$ at $m+1$ distinct points 
$\{s_i\}_{i=0}^m$, we can uniquely determine the coefficients $\{\mu_l\}_{l=0}^m$
of this polynomial. For convenience, we may choose $s_i = \cos(2 \pi i/m), i=0,\dots,m$.
In that case,
\begin{equation}
    f^m(s_i) = \sum_{l=0}^{m} \mu_l \cos\left(\frac{2 \pi l i}{m}\right),
    \label{equ:2-chebyshev-chebyshev-nodes-evaluation}
\end{equation}
which coincides with the \glsfirst{DCT}\footnote{There exist multiple conventions for the DCT.
The one which we use is (up to scaling of the first and last coefficient)
referred to as a type 1 DCT, and is efficiently implemented in the SciPy Python package:
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}} of the coefficients $\{\mu_l\}_{l=0}^m$.
If we collect the coefficients $\{\mu_l\}_{l=0}^{m}$ in a vector $\vct{\mu} \in \mathbb{R}^{m+1}$ 
and the function evaluations $\{f^m(s_i)\}_{i=0}^{m}$ in another
vector $\vct{f} \in \mathbb{R}^{m+1}$, we find that
\begin{equation}
    \vct{f} = \DCT \{ \vct{\mu} \} \implies \vct{\mu} = \DCT^{-1}\{ \vct{f} \}.
    \label{equ:2-chebyshev-chebyshev-DCT}
\end{equation}
In short, computing the coefficients of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion-general}
of the function $f$ amounts to evaluating this function at $m+1$ well
chosen points and computing the inverse \gls{DCT}.
This procedure is usually inexpensive and can be done in $\mathcal{O}(m \log(m))$
operations \cite{makhoul1980fct}.\\

\begin{algo}{Chebyshev expansion}{2-chebyshev-chebyshev-expansion}
    \input{algorithms/chebyshev_expansion.tex}
\end{algo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic trace estimation}
\label{sec:2-chebyshev-stochastic-trace-estimation}

\todo{introduce stochastic trace estimation in mat-vec model}
%Assembling the expanded matrix function involved in \refequ{equ:2-chebyshev-spectral-density-as-trace-expansion}
%explicitly in order to compute its trace can incur significant computational cost.
%To avoid this cost, we may resort to stochastically estimating the trace through
%matrix-vector products.
The quadratic form $\vct{\psi}^{\top} \mtx{B} \vct{\psi}$
for a symmetric matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ and a standard Gaussian
random vector $\vct{\psi} \in \mathbb{R}^n$ %, $\mathbb{E}[\vct{\psi}] = \vct{0}$, $\mathbb{E}[\vct{\psi}\vct{\psi}^{\top}] = \mtx{I}$ 
is an unbiased estimate of the trace:
\begin{equation}
    \mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}]
        = \mathbb{E}\left[\sum_{i=1}^n\sum_{j=1}^n \psi_i B_{ij} \psi_j\right]
        = \sum_{i=1}^n\sum_{j=1}^n B_{ij} \mathbb{E}[\psi_i\psi_j]
        = \sum_{i=1}^n B_{ii}
        = \Tr(\mtx{B}).
    \label{equ:2-chebyshev-DGC-hutchinson}
\end{equation}

\todo{Take time dependent Hutchinson result from preprint}
%Variance of Hutchinson \cite[proposition~1]{hutchinson1990trace} \textcolor{red}{(only for Rademacher, else 2 times Frobenius norm)}
\begin{align}
    \MSE(\vct{\psi}^{\top} \mtx{B} \vct{\psi}) &= \Var(\vct{\psi}^{\top} \mtx{B} \vct{\psi}) + (\mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}] - \Tr(\mtx{B}))^2 && \text{(mean squared error)} \notag \\
        &= \Var(\vct{\psi}^{\top} \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} \vct{\psi}) && \text{(spectral decomposition of $\mtx{B}$)} \notag \\
        &= \Var(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}) && \text{(Gaussian orthogonal invariance)} \notag \\
        &= \mathbb{E}[(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}})^2] - \mathbb{E}[\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}]^2 && \text{(definition of variance)} \notag \\
        &= \mathbb{E}[(\sum_{i=1}^{n} \widetilde{\psi}_i^2 \lambda_i)^2] - \Tr(\mtx{B})^2 && \text{($\mtx{\Lambda}$ is diagonal and \refequ{equ:2-chebyshev-DGC-hutchinson})} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j \mathbb{E}[\widetilde{\psi}_i^2 \widetilde{\psi}_j^2] - \Tr(\mtx{B})^2 && \text{(linearity of expectation value)} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j + 2 \sum_{i=1}^{n} \lambda_i^2 - \Tr(\mtx{B})^2 && \text{($\mathbb{E}[\widetilde{\psi}_i^2]=1$ and $\mathbb{E}[\widetilde{\psi}_i^4]=3$)} \notag \\
        &= \Tr(\mtx{B})^2 + 2 \lVert \mtx{B} \rVert _F^2 - \Tr(\mtx{B})^2 && \text{(definition of Frobenius norm)} \notag \\
        &= 2 \lVert \mtx{B} \rVert _F^2.
    \label{equ:2-chebyshev-DGC-hutchinson-variance}
\end{align}

If we now take a certain \glsfirst{num-hutchinson-queries} to form the
\glsfirst{random-matrix} $= [\vct{\psi}_1, \dots, \vct{\psi}_{n_v}] \in \mathbb{R}^{n \times n_v}$
The Hutchinson's trace estimator can then be written as \cite{hutchinson1990trace}
\begin{equation}
    \Hutch_{n_v}(\mtx{B}) = \frac{1}{n_v} \sum_{j=1}^{n_v} \vct{\psi}_j^{\top} \mtx{B} \vct{\psi}_j = \frac{1}{n_v} \Tr(\mtx{\Psi}^{\top} \mtx{B} \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator}
\end{equation}
It is not hard to see that the mean squared error
\begin{equation}
    \MSE(\Hutch_{n_v}(\mtx{B})) = \frac{2}{\sqrt{n_v}} \lVert \mtx{B} \rVert _F^2 \leq \frac{2}{\sqrt{n_v}}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Delta-Gauss-Chebyshev method}
\label{sec:2-chebyshev-delta-gauss-chebyshev}

The developments from \refsec{sec:2-chebyshev-interpolation} can now be applied
to a Chebyshev expansion of the matrix function appearing in \refequ{equ:1-introduction-spectral-density-as-trace},
such that
\begin{equation}
    g_{\sigma}^m(t\mtx{I} - \mtx{A}) = \sum_{l=0}^m \mu_l(t) T_l(\mtx{A}).
    \label{equ:2-chebyshev-chebyshev-expansion}
\end{equation}
This, however, is only well-defined for matrices whose spectra are fully
contained in $[-1, 1]$. For matrices whose spectra we know (or can estimate) to
be within $[a, b]$, we may define a \gls{spectral-transformation} as follows
\begin{equation}
    \tau(t) = \frac{2t - a - b}{b - a},
    \label{equ:2-chebyshev-spectral-transformation}
\end{equation}
and subsequently expand $\bar{g}_{\bar{\sigma}}^m = g_{\sigma}^m \circ \tau^{-1}$ 
from which we can retrieve $g_{\sigma}^m = \bar{g}_{\bar{\sigma}}^m \circ \tau$.
Note that under this transformation also the \gls{smoothing-parameter} needs to be
adjusted to
\begin{equation}
    \bar{\sigma} = |\tau'| \sigma = \frac{2\sigma}{b - a}
    \label{equ:2-chebyshev-sigma-transformation}
\end{equation}
before the approximation in order to get a consistent result.\\
%\textcolor{red}{Contrary to popular belief \cite[algorithm~1]{lin2017randomized},
%the coefficients should not be computed using a quadrature rule for the integral
%expression of the coefficients \cite[theorem~3.1]{trefethen2019chebyshev}, since
%in general this is not guaranteed to be exact and is furthermore more complicated
%and slower than the \gls{DCT}.}

Plugging the expansion \refequ{equ:2-chebyshev-chebyshev-expansion} into 
\refequ{equ:1-introduction-spectral-density-as-trace} gives us the expanded
spectral density
\begin{equation}
    \phi_{\sigma}^m(t) = \Tr(g_{\sigma}^m(t\mtx{I} - \mtx{A})).
    \label{equ:2-chebyshev-spectral-density-as-trace-expansion}
\end{equation}

The convergence of the expansion to the actual spectral density is exponential, 
which is claimed by the following theorem:
\begin{theorem}{Chebyshev interpolation error for Gaussian smoothing kernel}{chebyshev-error}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix whose spectrum
    is contained in $(-1, 1)$. For any $t \in \mathbb{R}$ it holds
    \begin{equation}
        \left|  \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right| \leq \frac{C_1}{\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-error}
    \end{equation}
    with constants $C_1, C_2 > 0$ independent of $\sigma$, $m$, and $t$.
\end{theorem}
This result is a consequence of \refthm{thm:2-chebyshev-bernstein}.
A proof can be found in \cite[theorem~2]{lin2017randomized}. However, since our
result slightly deviates from the aforementioned proof, we reproduce it hereafter.
\begin{proof}
    Using basic properties of matrix functions, we obtain
    \begin{align}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        &= \left| \Tr(g_{\sigma}(t\mtx{I} - \mtx{A})) - \Tr(g_{\sigma}^m(t\mtx{I} - \mtx{A})) \right|
        && \text{(definitions \refequ{equ:1-introduction-spectral-density-as-trace} and \refequ{equ:2-chebyshev-chebyshev-expansion})} \notag \\
        &= \left| \sum_{i=1}^n \left(g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i)\right) \right|
        && \text{(property of matrix function)} \notag \\
        &\leq n \sup_{i = 1, \dots, n} \left| g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i) \right|
        && \text{(pessimistic upper bound)} \notag \\
        &\leq n \sup_{s \in (-1, 1)} \left| g_{\sigma}(t - s) - g_{\sigma}^m(t - s) \right|
        && \text{(extension of domain)} \notag \\
        &\leq n \frac{2}{\chi^m(\chi - 1)} \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)|
        && \text{(Bernstein \cite[theorem~73]{meinardus1967approximation})}
        \label{equ:2-chebyshev-proof-bernstein-general-expression}
    \end{align}
    where in the last step we define the ellipse $\mathcal{E}_{\chi}$
    with foci $\{-1, 1\}$ and with sum of half-axes $\chi = a + b > 1$
    (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Since $g_{\sigma}(t - \cdot)$ of the form \refequ{equ:1-introduction-def-gaussian-kernel}
    is holomorphic, $\chi$ may be chosen arbitrarily.

    Writing $z = x + iy$ for $x,y \in \mathbb{R}$, we estimate (using $|e^z| = e^{\Real(z)}$)
    \begin{equation}
        |g_{\sigma}(t - (x + iy))| %&= \frac{1}{n \sqrt{2 \pi \sigma^2}} \left| e^{- \frac{(t - (x + iy))^2}{2 \sigma^2}} \right| \notag \\
        = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{- \frac{(t - x)^2 - y^2}{2 \sigma^2}}
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{y^2}{2 \sigma^2}}.
    \end{equation}

    Expressing $\chi = 1 + \alpha \sigma$ for any $\alpha > 0$,
    we can estimate $\chi - \chi^{-1} \leq 2\alpha\sigma$.
    This can be established by observing
    $h(\chi) = 2\alpha\sigma - \chi + \chi^{-1} = \chi + \chi^{-1} - 2 \geq 0$
    for which $h(1) = 0$ and $h'(\chi) \geq 0$ for all $\chi > 1$.
    Furthermore, because $z$ is
    contained in $\mathcal{E}_{\chi}$ we know that the absolute value of its
    imaginary part is upper bound by the imaginary half axis $b$, which we can
    express in terms of $\chi$ to get \todo{[cite or proof]}
    \begin{equation}
        |y| \leq b = \frac{\chi - \chi^{-1}}{2} \leq \alpha\sigma.
    \end{equation}

    Consequently, for all $t \in \mathbb{R}$
    \begin{equation}
        \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)| 
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}.
    \end{equation}

    Plugging this estimate into \refequ{equ:2-chebyshev-proof-bernstein-general-expression}, we get
    \begin{equation}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        \leq n \frac{2}{(1 + \alpha\sigma)^m\alpha \sigma} \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}
        = \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m},
    \end{equation}
    where $C_1=\sqrt{\frac{2}{\pi}}\frac{1}{\alpha}e^{\frac{\alpha^2}{2}}$ and $C_2=\alpha$.
\end{proof}

Finally,  we can now put together the 
Chebyshev expansion results
the stochastic trace estimation from \refsec{sec:2-chebyshev-stochastic-trace-estimation},
we end up with the \glsfirst{DGC} method \cite[algorithm~2]{lin2017randomized}
which approximates the \glsfirst{smooth-spectral-density} as
\begin{equation}
    \widetilde{\phi}_{\sigma}^m(t) = H_{n_v}(g_{\sigma}^m(t\mtx{I} - \mtx{A})).
    \label{equ:2-chebyshev-DGC-final-estimator}
\end{equation}
An efficient implementation is realizable thanks to recurrence relation for the
Chebyshev polynomials \refequ{equ:2-chebyshev-chebyshev-recursion}. The pseudocode
for this method can be found in \refalg{alg:DGC}.

\begin{algo}{Delta-Gauss-Chebyshev method}{DGC}
    \input{algorithms/delta_gauss_chebyshev.tex}
\end{algo}

One additional speed-up can be achieved by smartly computing the trace of the 
product of two matrices $\mtx{B}, \mtx{C} \in \mathbb{R}^{N \times M}$ in
$\mathcal{O}(MN)$ instead of $\mathcal{O}(M^2N)$ time, due to the relation
\begin{equation}
    \Tr(\mtx{B}^{\top}\mtx{C}) = \sum_{i=1}^{N} \sum_{j=1}^{M} B_{ij} C_{ij}.
    \label{equ:2-chebyshev-fast-trace}
\end{equation}

Denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and
$\mathcal{O}(c(n)) = n$ for sparse matrices, we determine the computational
complexity of the \gls{DGC} method to be $\mathcal{O}(m \log(m) n_t + m n_v c(n))$,
with $\mathcal{O}(m n_t + n n_v)$ required additional storage.
