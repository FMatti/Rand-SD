\chapter{Interpolation and trace estimation}
\label{chp:2-chebyshev}

For many \glsfirst{smoothing-kernel}, as is for example the case with Gaussian smoothing
\refequ{equ:1-introduction-def-gaussian-kernel}, we cannot compute the matrix function
\begin{equation}
    g_{\sigma}(t\mtx{I} - \mtx{A})
    \label{equ:2-chebyshev-matrix-function}
\end{equation}
involved in \refequ{equ:1-introduction-spectral-density-as-trace} without
diagonalizing $\mtx{A}$, which can be prohibitively expensive for large
$\mtx{A}$. A way around this problem is to refrain from trying to assemble the
matrix function explicitly but instead using the fact that we are only interested
in its trace. Approximating the trace can be done by multiplying the matrix
with random vectors to then -- for example -- evaluate the Hutchinson's trace estimator \cite{hutchinson1990trace}.
The multiplication of a matrix function with a vector can
often be determined quite efficiently using Krylov subspace methods
\cite[chapter~13.2]{higham2008functions}. Another way in which products of matrix
functions with vectors can be computed involves
expanding the function in terms of a finite set of orthogonal polynomials
and subsequently using a recurrence relation to efficiently construct the result.
This approach turns out to be particularly effective when we work with matrix
functions which smoothly depend on a parameter within a bounded interval, if we
want to evaluate the function at a large number of values of this parameter.
In this chapter we will analyze one such expansion, the Chebyshev expansion,
which gives us an efficient method for approximating the spectral density.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chebyshev interpolation}
\label{sec:2-chebyshev-interpolation}

The Chebyshev interpolation framework is best known for its stability, beneficial
convergence properties, and simple three-term recurrence relation
\refequ{equ:2-chebyshev-chebyshev-recursion} which can
be exploited to efficiently compute products of Chebyshev polynomials with
vectors.\\

At the foundation of Chebyshev interpolation lie the \glspl{chebyshev-polynomial}.
They are defined as \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    T_l: [-1, 1] \to [-1, 1],~T_l(s) = \cos(l \arccos(s)).
    \label{equ:2-chebyshev-chebyshev-definition}
\end{equation}
They satisfy the recurrence relation
\begin{equation}
    \begin{cases}
        T_0(s) = 1, & l = 0 \\
        T_1(s) = s, & l = 1 \\
        T_{l}(s) = 2s T_{l-1}(s) - T_{l-2}(s), & l \geq 2,
    \end{cases}
    \label{equ:2-chebyshev-chebyshev-recursion}
\end{equation}
which can be shown using their definition \refequ{equ:2-chebyshev-chebyshev-definition}
and a standard trigonometric identity.\\

A function $f:[-1, 1] \to \mathbb{R}$ can be expanded in a basis of Chebyshev
polynomials up to \gls{chebyshev-degree} \cite[chapter~3]{trefethen2019chebyshev}
\begin{equation}
    f^m(s) = \sum_{l=0}^m \mu_l T_l(s).
    \label{equ:2-chebyshev-chebyshev-expansion-general}
\end{equation}
For functions $f$ which can be analytically extended in a certain neighborhood
of $[-1, 1]$ in the complex plane, Bernstein's theorem \cite[theorem~73]{meinardus1967approximation}
establishes that the convergence of this expansion in the supremum-norm is
exponential.
\begin{theorem}{Bernstein's theorem}{2-chebyshev-bernstein}
    Let $f:[-1, 1] \to \mathbb{R}$ be analytic and bounded in the ellipse $\mathcal{E}_{\chi} \subset \mathbb{C}$
    with foci $\pm 1$ and sum of half axes $\chi = a + b$ (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Then
    \begin{equation}
        \sup_{s \in [-1, 1]} |f(s) - f^m(s)| \leq \frac{2}{\chi^m(\chi-1)} \sup_{z \in \mathcal{E}_{\chi}} |f(z)|.
        \label{equ:2-chebyshev-bernstein-convergence-result}
    \end{equation}
\end{theorem}

\begin{figure}[ht]
    \centering
    \input{figures/bernstein_ellipse.tex}
    \caption{A Bernstein ellipse $\mathcal{E}_{\chi}$ with half axes lengths $a$ and
        $b$ visualized in the complex plane $\mathbb{C}$.}
    \label{fig:2-chebyshev-proof-bernstein-ellipse}
\end{figure}

The coefficients $\{\mu_l\}_{l=0}^m$ in \refequ{equ:2-chebyshev-chebyshev-expansion-general}
could be computed using the orthogonality of the Chebyshev polynomials with respect
to a certain inner product, and some authors indeed suggest approximating the
involved integral using a quadrature rule \cite[algorithm~1]{lin2017randomized}. However, we
have found a significantly simpler and faster method: Note that polynomials of
degree $m$ are uniquely defined by $m+1$ function evaluations at distinct points \cite{gauss1799demonstratio}.
Hence, if we know the values $f^m(s_i)$ at some $m+1$ distinct points 
$\{s_i\}_{i=0}^m$, we can uniquely determine the coefficients $\{\mu_l\}_{l=0}^m$
of this polynomial. For convenience, we may choose $s_i = \cos(2 \pi i/m), i=0,\dots,m$.
In that case,
\begin{equation}
    f^m(s_i) = \sum_{l=0}^{m} \mu_l \cos\left(\frac{2 \pi l i}{m}\right),
    \label{equ:2-chebyshev-chebyshev-nodes-evaluation}
\end{equation}
which coincides with a \glsfirst{DCT}\footnote{There exist multiple conventions for the DCT.
The one which we use is (up to scaling of the first and last coefficient)
referred to as a type 1 DCT, and is efficiently implemented in the SciPy Python package:
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html}} of the coefficients $\{\mu_l\}_{l=0}^m$.
If we collect the coefficients $\{\mu_l\}_{l=0}^{m}$ in a vector $\vct{\mu} \in \mathbb{R}^{m+1}$ 
and the function evaluations $\{f^m(s_i)\}_{i=0}^{m}$ in another
vector $\vct{f} \in \mathbb{R}^{m+1}$, we find that
\begin{equation}
    \vct{f} = \DCT \{ \vct{\mu} \} \implies \vct{\mu} = \DCT^{-1}\{ \vct{f} \}.
    \label{equ:2-chebyshev-chebyshev-DCT}
\end{equation}
In short, computing the coefficients of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion-general}
of the function $f$ amounts to evaluating this function at $m+1$ well
chosen points and computing the inverse \gls{DCT}. The corresponding algorithm
can be found in \refalg{alg:2-chebyshev-chebyshev-expansion}
This procedure is usually inexpensive and can be done in $\mathcal{O}(m \log(m))$
operations \cite{makhoul1980fct}.

\begin{algo}{Chebyshev expansion}{2-chebyshev-chebyshev-expansion}
    \input{algorithms/chebyshev_expansion.tex}
\end{algo}

\todo{Find result for $\int_{a}^{b} |\cdot| \mathrm{d}t$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic trace estimation}
\label{sec:2-chebyshev-stochastic-trace-estimation}

Matrix-free stochastic trace estimation is most useful when a matrix is not given
explicitly, but computing products of this matrix with vectors can be implemented
efficiently. Examples of such scenarios are traces of matrix functions
\cite{ubaru2017lanczos,epperly2023xtrace} or of matrices which can be easily
queried with matrix-vector products \cite{ghorbani2019investigation,adepu2021hessian}.
Most algorithms for stochastic trace estimation are based on the Hutchinson's
trace estimator, which we will discuss in the following paragraphs.\\

For a symmetric matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ and a standard Gaussian
random vector $\vct{\psi} \in \mathbb{R}^n$, the quadratic form $\vct{\psi}^{\top} \mtx{B} \vct{\psi}$ 
is an unbiased estimate of the trace:
\begin{equation}
    \mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}]
        = \mathbb{E}\left[\sum_{i=1}^n\sum_{j=1}^n \psi_i B_{ij} \psi_j\right]
        = \sum_{i=1}^n\sum_{j=1}^n B_{ij} \mathbb{E}[\psi_i\psi_j]
        = \sum_{i=1}^n B_{ii}
        = \Tr(\mtx{B}).
    \label{equ:2-chebyshev-DGC-hutchinson}
\end{equation}
Furthermore, the \gls{MSE} of this estimate is bounded by the Frobenius norm of the matrix
$\mtx{B}$:
%Variance of Hutchinson \cite[proposition~1]{hutchinson1990trace} \textcolor{red}{(only for Rademacher, else 2 times Frobenius norm)}
\begin{align}
    \MSE(\vct{\psi}^{\top} \mtx{B} \vct{\psi}) &= \Var(\vct{\psi}^{\top} \mtx{B} \vct{\psi}) + (\mathbb{E}[\vct{\psi}^{\top} \mtx{B} \vct{\psi}] - \Tr(\mtx{B}))^2 && \text{(mean squared error)} \notag \\
        &= \Var(\vct{\psi}^{\top} \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} \vct{\psi}) && \text{(spectral decomposition of $\mtx{B}$)} \notag \\
        &= \Var(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}) && \text{(Gaussian orthogonal invariance)} \notag \\
        &= \mathbb{E}[(\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}})^2] - \mathbb{E}[\widetilde{\vct{\psi}}^{\top} \mtx{\Lambda} \widetilde{\vct{\psi}}]^2 && \text{(definition of variance)} \notag \\
        &= \mathbb{E}[(\sum_{i=1}^{n} \widetilde{\psi}_i^2 \lambda_i)^2] - \Tr(\mtx{B})^2 && \text{($\mtx{\Lambda}$ is diagonal and \refequ{equ:2-chebyshev-DGC-hutchinson})} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j \mathbb{E}[\widetilde{\psi}_i^2 \widetilde{\psi}_j^2] - \Tr(\mtx{B})^2 && \text{(linearity of expectation value)} \notag \\
        &= \sum_{i=1}^{n} \lambda_i \sum_{j=1}^{n} \lambda_j + 2 \sum_{i=1}^{n} \lambda_i^2 - \Tr(\mtx{B})^2 && \text{($\mathbb{E}[\widetilde{\psi}_i^2]=1$ and $\mathbb{E}[\widetilde{\psi}_i^4]=3$)} \notag \\
        &= \Tr(\mtx{B})^2 + 2 \lVert \mtx{B} \rVert _F^2 - \Tr(\mtx{B})^2 && \text{(definition of Frobenius norm)} \notag \\
        &= 2 \lVert \mtx{B} \rVert _F^2.
    \label{equ:2-chebyshev-DGC-hutchinson-variance}
\end{align}
The idea of the Hutchinson's trace estimator is to compute multiple such estimates
for different, independent random vectors and take the average. This will again
be an unbiased estimate of the trace, but with the reduced \gls{MSE}
\begin{equation}
    \MSE\left( \frac{1}{n_{\mtx{\Psi}}} \sum_{j=1}^{n_{\mtx{\Psi}}}\vct{\psi}_j^{\top} \mtx{B} \vct{\psi}_j\right) = \frac{2}{\sqrt{n_{\mtx{\Psi}}}} \lVert \mtx{B} \rVert _F^2.
    \label{equ:2-chebyshev-hutchinson-mse}
\end{equation}
Collecting the random vectors in the
\glsfirst{random-matrix} $= [\vct{\psi}_1, \dots, \vct{\psi}_{n_{\mtx{\Psi}}}] \in \mathbb{R}^{n \times n_{\mtx{\Psi}}}$,
the Hutchinson's trace estimator can then be written as
\begin{equation}
    \Hutch_{n_{\mtx{\Psi}}}(\mtx{B}) = \frac{1}{n_{\mtx{\Psi}}} \Tr(\mtx{\Psi}^{\top} \mtx{B} \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator}
\end{equation}\\

In the case where the matrix continuously depends on a parameter in a bounded 
interval, we can analogously define the Hutchinson's estimator for parameter-dependent
matrices
\begin{equation}
    \Hutch_{n_{\mtx{\Psi}}}(\mtx{B}(t)) = \frac{1}{n_{\mtx{\Psi}}} \Tr(\mtx{\Psi}^{\top} \mtx{B}(t) \mtx{\Psi}).
    \label{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}
\end{equation}
As the counterpart of the \gls{MSE} in the parametrized case, we measure the
error of this estimate in the $1$-norm, for which we can use the result
from \todo{[cite in preparation]}, which we will state as the following lemma:
\begin{lemma}{Hutchinson}{2-chebyshev-parameter-hutchinson}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ symmetric and continuous in
    $t \in [a, b]$, $\delta \in (0, e^{-1})$, and $n_{\mtx{\Psi}} \in \mathbb{N}$.
    Let $\Hutch_{n_{\mtx{\Psi}}}(\mtx{B}(t))$ be the $n_{\mtx{\Psi}}$-query
    Hutchinson estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
    For a constant $C_{\mtx{\Psi}}$, it holds with probability $\geq 1 - \delta$
    \begin{equation}
        \int_{a}^{b} \left| \Hutch_{n_{\mtx{\Psi}}}(\mtx{B}(t))  - \Tr(\mtx{B}(t)) \right| \mathrm{d}t \leq C_{\mtx{\Psi}} \sqrt{\frac{\log(2/\delta)}{n_{\mtx{\Psi}}}} \int_{a}^{b} \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    \end{equation}
    %So, if $n_{\mtx{\Psi}} = \mathcal{O}\left( \frac{\log(2/\delta)}{\varepsilon^2} \right)$ then, with probability $\geq 1 - \delta$
    %\begin{equation}
    %    \int |H_l(\mtx{B}(t)) - \Tr(\mtx{B}(t))| \mathrm{d}t \leq \varepsilon \int \lVert \mtx{B}(t) \rVert _F \mathrm{d}t.
    %\end{equation}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Delta-Gauss-Chebyshev method}
\label{sec:2-chebyshev-delta-gauss-chebyshev}

Now we have all the ingredients for constructing a first algorithm to approximate
the expression \refequ{equ:1-introduction-spectral-density-as-trace}:
The Chebyshev expansion of a function \refalg{alg:2-chebyshev-chebyshev-expansion}
and the Hutchinson's trace estimator \refequ{equ:2-chebyshev-DGC-hutchionson-estimator-parameter}.
We expand the \glsfirst{smoothing-kernel} in terms of Chebyshev polynomials, such that
\begin{equation}
    g_{\sigma}^m(t\mtx{I} - \mtx{A}) = \sum_{l=0}^m \mu_l(t) T_l(\mtx{A}).
    \label{equ:2-chebyshev-chebyshev-expansion}
\end{equation}
This, however, is only well-defined for matrices whose spectra are fully
contained in $[-1, 1]$. To solve this issue for matrices whose spectra we know (or can estimate) to
be within $[a, b]$, we can define a \gls{spectral-transformation} as follows
\begin{equation}
    \tau(t) = \frac{2t - a - b}{b - a},
    \label{equ:2-chebyshev-spectral-transformation}
\end{equation}
and instead expand $\bar{g}_{\bar{\sigma}}^m = g_{\sigma}^m \circ \tau^{-1}$ 
from which we can retrieve $g_{\sigma}^m = \bar{g}_{\bar{\sigma}}^m \circ \tau$.
Note that under this transformation also the \gls{smoothing-parameter} needs to be
rescaled to
\begin{equation}
    \bar{\sigma} = |\tau'| \sigma = \frac{2\sigma}{b - a}
    \label{equ:2-chebyshev-sigma-transformation}
\end{equation}
in order to get a consistent result.\\

Plugging the expansion \refequ{equ:2-chebyshev-chebyshev-expansion} into 
\refequ{equ:1-introduction-spectral-density-as-trace} gives us the expanded
spectral density
\begin{equation}
    \phi_{\sigma}^m(t) = \Tr(g_{\sigma}^m(t\mtx{I} - \mtx{A})).
    \label{equ:2-chebyshev-spectral-density-as-trace-expansion}
\end{equation}

The convergence of the expansion to the actual spectral density is exponential, 
which is shown in the following theorem:
\begin{theorem}{Chebyshev interpolation error for Gaussian smoothing kernel}{chebyshev-error}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix whose spectrum
    is contained in $(-1, 1)$. For any $t \in \mathbb{R}$ it holds
    \begin{equation}
        \left|  \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right| \leq \frac{C_1}{\sigma^2}(1 + C_2 \sigma)^{-m}
        \label{equ:2-chebyshev-interpolation-error}
    \end{equation}
    with constants $C_1, C_2 > 0$ independent of $\sigma$, $m$, and $t$.
\end{theorem}
This result is a consequence of \refthm{thm:2-chebyshev-bernstein}.
A proof can be found in \cite[theorem~2]{lin2017randomized}. However, since our
result slightly deviates from the aforementioned proof, we reproduce it hereafter.
\begin{proof}
    Using basic properties of matrix functions, we obtain
    \begin{align}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        &= \left| \Tr(g_{\sigma}(t\mtx{I} - \mtx{A})) - \Tr(g_{\sigma}^m(t\mtx{I} - \mtx{A})) \right|
        && \text{(definitions \refequ{equ:1-introduction-spectral-density-as-trace} and \refequ{equ:2-chebyshev-chebyshev-expansion})} \notag \\
        &= \left| \sum_{i=1}^n \left(g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i)\right) \right|
        && \text{(property of matrix function)} \notag \\
        &\leq n \sup_{i = 1, \dots, n} \left| g_{\sigma}(t - \lambda_i) - g_{\sigma}^m(t - \lambda_i) \right|
        && \text{(pessimistic upper bound)} \notag \\
        &\leq n \sup_{s \in (-1, 1)} \left| g_{\sigma}(t - s) - g_{\sigma}^m(t - s) \right|
        && \text{(extension of domain)} \notag \\
        &\leq n \frac{2}{\chi^m(\chi - 1)} \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)|
        && \text{(Bernstein \cite[theorem~73]{meinardus1967approximation})}
        \label{equ:2-chebyshev-proof-bernstein-general-expression}
    \end{align}
    where in the last step we define the ellipse $\mathcal{E}_{\chi}$
    with foci $\{-1, 1\}$ and with sum of half-axes $\chi = a + b > 1$
    (see \reffig{fig:2-chebyshev-proof-bernstein-ellipse}).
    Since $g_{\sigma}(t - \cdot)$ of the form \refequ{equ:1-introduction-def-gaussian-kernel}
    is holomorphic, $\chi$ may be chosen arbitrarily.

    Writing $z = x + iy$ for $x,y \in \mathbb{R}$, we estimate (using $|e^z| = e^{\Real(z)}$)
    \begin{equation}
        |g_{\sigma}(t - (x + iy))| %&= \frac{1}{n \sqrt{2 \pi \sigma^2}} \left| e^{- \frac{(t - (x + iy))^2}{2 \sigma^2}} \right| \notag \\
        = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{- \frac{(t - x)^2 - y^2}{2 \sigma^2}}
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{y^2}{2 \sigma^2}}.
    \end{equation}

    Expressing $\chi = 1 + \alpha \sigma$ for any $\alpha > 0$,
    we can estimate $\chi - \chi^{-1} \leq 2\alpha\sigma$.
    This can be established by observing
    $h(\chi) = 2\alpha\sigma - \chi + \chi^{-1} = \chi + \chi^{-1} - 2 \geq 0$
    for which $h(1) = 0$ and $h'(\chi) \geq 0$ for all $\chi > 1$.
    Furthermore, because $z$ is
    contained in $\mathcal{E}_{\chi}$ we know that the absolute value of its
    imaginary part is upper bound by the imaginary half axis $b$, which can be
    express in terms of $\chi$ to get
    \begin{equation}
        |y| \leq b = \frac{\chi - \chi^{-1}}{2} \leq \alpha\sigma.
    \end{equation}

    Consequently, for all $t \in \mathbb{R}$
    \begin{equation}
        \sup_{z \in \mathcal{E}_{\chi}} |g_{\sigma}(t - z)| 
        \leq \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}.
    \end{equation}

    Plugging this estimate into \refequ{equ:2-chebyshev-proof-bernstein-general-expression}, we get
    \begin{equation}
        \left| \phi_{\sigma}(t) - \phi_{\sigma}^m(t) \right|
        \leq n \frac{2}{(1 + \alpha\sigma)^m\alpha \sigma} \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{\frac{\alpha^2}{2}}
        = \frac{C_1}{\sigma^2} (1 + C_2 \sigma)^{-m},
    \end{equation}
    where $C_1=\sqrt{\frac{2}{\pi}}\frac{1}{\alpha}e^{\frac{\alpha^2}{2}}$ and $C_2=\alpha$.
\end{proof}

Finally, combining Chebyshev expansion with stochastic trace estimation
we end up with the \glsfirst{DGC} method \cite[algorithm~2]{lin2017randomized}
which approximates the \glsfirst{smooth-spectral-density} as
\begin{equation}
    \widetilde{\phi}_{\sigma}^m(t) = H_{n_{\mtx{\Psi}}}(g_{\sigma}^m(t\mtx{I} - \mtx{A})).
    \label{equ:2-chebyshev-DGC-final-estimator}
\end{equation}\\

An efficient implementation can be achieved thanks to the recurrence relation for the
Chebyshev polynomials \refequ{equ:2-chebyshev-chebyshev-recursion}. The pseudocode
for this method can be found in \refalg{alg:2-chebyshev-DGC}.
\begin{algo}{Delta-Gauss-Chebyshev method}{2-chebyshev-DGC}
    \input{algorithms/delta_gauss_chebyshev.tex}
\end{algo}

One additional speed-up can be achieved by smartly computing the trace of the 
product of two matrices $\mtx{B}, \mtx{C} \in \mathbb{R}^{N \times M}$ in
$\mathcal{O}(MN)$ instead of $\mathcal{O}(M^2N)$ time, due to the relation
\begin{equation}
    \Tr(\mtx{B}^{\top}\mtx{C}) = \sum_{i=1}^{N} \sum_{j=1}^{M} B_{ij} C_{ij}.
    \label{equ:2-chebyshev-fast-trace}
\end{equation}
Thus, denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and
$\mathcal{O}(c(n)) = n$ for sparse matrices, we determine the computational
complexity of the \gls{DGC} method to be $\mathcal{O}(m \log(m) n_t + m n_{\mtx{\Psi}} c(n))$,
with $\mathcal{O}(m n_t + n n_{\mtx{\Psi}})$ required additional storage.
