\chapter{Introduction}
\label{chp:1-introduction}

Define eigenvalues

\begin{equation}
    \mtx{A} \vct{v} = \lambda \vct{v}
    \label{equ:1-introduction-eigenvalue}
\end{equation}

Often prohibitively expensive to compute, hence often alternatively

Hermitian matrix ($\mtx{A}^{\top} = \mtx{A}$): real eigenvalues 

\begin{definition}{Spectral density of a matrix}{spectral-density}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a Hermitian matrix with eigenvalues $\lambda_i, i=1, \dots, N$.
    We define the \glsfirst{spectral-density} as
    \begin{equation}
        \phi(t) = \frac{1}{n} \sum_{i=1}^{n} \delta(t - \lambda_i),
        \label{equ:1-introduction-def-spectral-density}
    \end{equation}
    where we used the \gls{dirac-delta} \cite[Chapter~15]{dirac1947quantum}
    and the \gls{spectral-parameter} $\in \mathbb{R}$.
\end{definition}

Relevance in science:

Generally useful when exact individual eigenvalues are of little interest, but
more their relative locations.

Electronic structure calculations \cite{ducastelle1970charge, haydock1972electronic, lin2017randomized}

Hessian matrix in neural network optimization \cite{ghorbani2019investigation}

More examples (Phonons, \dots)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Properties of the spectral density}
\label{sec:1-introduction-properties}

Properties of spectral density (probability density, count eigenvalues, ...)

No convergence in p-norms [cite],

A priori any \gls{smoothing-kernel} [cite, kernel density estimation?]

Gaussian kernel width $\sigma > 0$
\begin{equation}
    g_{\sigma}(t) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{t^2}{2\sigma^2}}
    \label{equ:1-introduction-def-gaussian-kernel}
\end{equation}

\gls{smooth-spectral-density}
\begin{equation}
    \phi_{\sigma}(t) = (\phi \ast g_{\sigma})(t) = \frac{1}{n} \sum_{i=1}^{n} g_{\sigma}(t - \lambda_i)
    \label{equ:1-introduction-def-smooth-spectral-density}
\end{equation}

\begin{figure}[ht]
    \begin{subfigure}[b]{0.33\columnwidth}
        \input{plots/spectral_density_example_0.01.pgf}
        \caption{small $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.01}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\columnwidth}
        \input{plots/spectral_density_example_0.02.pgf}
        \caption{medium $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.02}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\columnwidth}
        \input{plots/spectral_density_example_0.05.pgf}
        \caption{large $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.05}
    \end{subfigure}
    \caption{Schematic depiction of the spectral density of a simple matrix with
    10 eigenvalues for different values of the smothing parameter $\sigma$.}
    \label{fig:1-introduction-smoothened-spectral-density}
\end{figure}

Apparently, compute eigenvalues directly often expensive
eigenvalue decomposition $\mtx{U} \mtx{\Lambda} \mtx{U}^{\top}$,
$\mtx{\Lambda}$ diagonal matrix with eigenvalues of $\mtx{A}$ on its diagonal and $\mtx{U}^{\top} \mtx{U} = \mtx{I}$ \cite[Theorem~4.1.5]{horn1985matrix},
matrix function $f(\mtx{A}) = \mtx{U}^{\top} f(\mtx{\Lambda}) \mtx{U}$,
with $f(\mtx{\Lambda}) = \diag(f(\lambda_1), \dots, f(\lambda_N))$ \cite[Definition~1.2]{higham2008functions}.
Using cyclic property of trace and $\mtx{U}$ unitary
\gls{identity-matrix}
Absorbed \gls{matrix-size} in $g_{\sigma}$

\begin{equation}
    \phi_{\sigma}(t) = \Tr(g_{\sigma}(t\mtx{I} - \mtx{A})) 
    \label{equ:1-introduction-spectral-density-as-trace}
\end{equation}

Usually two problems

\begin{enumerate}
    \item For most kernel $g_{\sigma}(t\mtx{I} - \mtx{A})$ cannot be directly evaluated for large $A$
    \item Computing trace of a matrix which we can only access through matvecs
\end{enumerate}

Hutch\pp

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
\label{sec:1-introduction-related}

Spectral density \cite{lin2017randomized}

Overview of different methods 

Lanczos based: Haydock \cite{lin2016review}, SLQ \cite{chen2021slq}
Polynomial expansion: KPM \cite{weisse2006kpm} (maybe also Lanczos related), DGC \cite{lin2017randomized}, DGL \cite{lin2016review}

[Visualize history in a tree, time vs. methods]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main contributions}
\label{sec:1-introduction-contributions}

Couple of small cleaning touches to \cite{lin2017randomized}.

\begin{itemize}
    \item Corrected proof
    \item Consistent and fast interpolation
    \item Fix rank 0 issue
    \item Theoretical rank estimate
    \item Generalization to other approximations
    \item Theorem $\rfrac{1}{\epsilon}$
    \item Implementation
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Structure of the thesis}
\label{sec:1-introduction-structure}

Chebyshev interpolation and randomized trace estimation \refchp{chp:2-chebyshev}
Randomized low-rank approximation \refchp{chp:3-nystrom}
Putting the two together for a powerful algorithm \refchp{chp:4-nystromchebyshev}
Numerical experiments \refchp{chp:5-experiments}
Conclusion \refchp{chp:6-conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notation}
\label{sec:1-introduction-notation}

We follow the notation which appears in contemporary literature in this field:

\begin{itemize}
    \item We only work with objects over the real numbers $\mathbb{R}$.
    \item Scalar variables are represented by lower case Greek or Latin letters ($x$, $\varepsilon$, \dots),
          vectors are additionally printed in bold ($\vct{v}$, $\vct{\mu}$, \dots),
          and matrices are additionally capitalized ($\mtx{A}$, $\mtx{\Omega}$, \dots).
    \item The identity matrix is $\mtx{I}$.
    \item The transpose of a matrix $\mtx{A}$ is denoted with $\mtx{A}^{\top}$.
          We only consider Hermitian matrices $\mtx{A}^{\top} = \mtx{A}$.
    \item The eigenvalues of a square matrix $\mtx{A} \in \mathbb{n \times n}$
          are denoted with $\lambda_1 \geq \dots \geq \lambda_n$.
    \item Norms are denoted with $\lVert \rVert _{\cdot}$. The spectral norm is
          $\lVert \mtx{A} \rVert _2 = \lambda_1$, the Frobenius norm
          is $\lVert \mtx{A} \rVert _2 = (\sum_{i=1}^{n} \lambda_i^2)^{\frac{1}{2}}$.
    \item The pseudoinverse of a matrix is $\mtx{A}^{\dagger}$ \cite{penrose1955pseudo}.
    \item The Euler's number is printed as $e$ and its corresponding natural logarithm as $\log$.
    \item Dirac's delta distribution goes as $\delta$ \cite[Chapter~15]{dirac1947quantum}.
    \item We use $\mathcal{O}$ to describe the asymptotic lower- and upper bounds
          on the complexity of a computation. For those more familiar with the
          Bachmann-Landau notation \cite[Section~3.2]{cormen2009algorithms},
          allow us -- for simplicity -- to use $\mathcal{O}$ as though it was $\Theta$.
\end{itemize}
