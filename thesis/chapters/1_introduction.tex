\chapter{Introduction}
\label{chp:1-introduction}

In many problems in engineering, physics, chemistry, and computer science, 
the eigenvalues of certain matrices help understand the nature of a system:
In electronic structure calculations they represent the allowed energy levels 
electrons can occupy \cite{ducastelle1970charge, haydock1972electronic, lin2017randomized};
in neural network optimization they are indicative of the optimization speed \cite{ghorbani2019investigation,chen2021slq,adepu2021hessian};
and in graph processing they can uncover hidden graph motifs \cite{kruzick2018graph,huang2021kernels,patane2022filter}.\\
\todo{relevance for fundamental linear algebraic tasks: computing matrix functions (Fan et al., 2019)
and eigensolvers (Polizzi, 2009; Li et al., 2019, FEAST)}

The eigenvalues of a matrix $\mtx{A}$ are all scalars $\lambda$ which, together
with some non-zero vectors $\vct{v}$, satisfy the equation
\begin{equation}
    \mtx{A} \vct{v} = \lambda \vct{v}.
    \label{equ:1-introduction-eigenvalue}
\end{equation}
In most applications, the matrices are real, i.e. $\mtx{A} \in \mathbb{R}^{n \times n}$,
and symmetric, i.e. $\mtx{A}^{\top} = \mtx{A}$, such that their eigenvalues 
$\lambda_1, \dots, \lambda_n$ are all real. This allows us to define its
spectral density.

\begin{definition}{Spectral density of a matrix}{spectral-density}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix with \glspl{eigenvalue}
    $\in \mathbb{R}, i=1, \dots, n$. We define the \gls{spectral-density} as
    \begin{equation}
        \phi(t) = \frac{1}{n} \sum_{i=1}^{n} \delta(t - \lambda_i),
        \label{equ:1-introduction-def-spectral-density}
    \end{equation}
    where we used the \gls{dirac-delta} \cite[Chapter~15]{dirac1947quantum}
    and the \gls{spectral-parameter} $\in \mathbb{R}$.
\end{definition}

\gls{spectral-density} is the probability density \cite{klenke2013probability}
of the cumulative empirical spectral measure \cite{chen2021slq}. Knowing \gls{spectral-density}
of a matrix would in theory allow us to reconstruct its eigenvalues.
Furthermore, the spectral density of a matrix can for example be used to count
the number of eigenvalues which lie within an interval $[a, b]$
\begin{equation}
    \nu_{[a, b]} = n \int_{a}^{b} \phi(t) \mathrm{d}t,
    \label{equ:1-introduction-eigenvalue-counting}
\end{equation}
or to compute the trace of a matrix function \cite{lin2017randomized}
\begin{equation}
    \Tr(f(\mtx{A})) = \sum_{i=1}^n f(\lambda_i) = n \int_{a}^{b} f(t) \phi(t) \mathrm{d}t.
    \label{equ:1-introduction-matrix-trace}
\end{equation}\\

However, constructing the spectral density of a matrix amounts to knowing all
the eigenvalues of the same matrix, which are often prohibitively expensive to
compute. In many of the above mentioned applications, not the exact individual
eigenvalues are of interest, but their approximate, relative locations to each
other. Hence, it is sufficient to consider approximations of \refequ{equ:1-introduction-def-spectral-density}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Smoothened spectral density}
\label{sec:1-introduction-properties}

Since we cannot hope to measure the convergence of any smooth function to \gls{spectral-density},
we first regularize \gls{spectral-density} with a suitable \gls{smoothing-kernel}
and define the \gls{smooth-spectral-density} as the convolution
\begin{equation}
    \phi_{\sigma}(t) = (\phi \ast g_{\sigma})(t) = \int_{-\infty}^{\infty} \phi(s) g_{\sigma}(t - s) \mathrm{d}s = \frac{1}{n} \sum_{i=1}^{n} g_{\sigma}(t - \lambda_i).
    \label{equ:1-introduction-def-smooth-spectral-density}
\end{equation}
The \gls{smoothing-parameter} $>0$ controls by how much \gls{spectral-density} is
smoothened (see \reffig{fig:1-introduction-smoothened-spectral-density}). Typically,
small \gls{smoothing-parameter} allow for easier approximations of \gls{smooth-spectral-density}
but at the cost of losing many of the finer characteristics of the spectrum.

\begin{figure}[ht]
    \begin{subfigure}[b]{0.33\columnwidth}
        \input{plots/spectral_density_example_0.01.pgf}
        \caption{small $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.01}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\columnwidth}
        \input{plots/spectral_density_example_0.02.pgf}
        \caption{medium $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.02}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\columnwidth}
        \input{plots/spectral_density_example_0.05.pgf}
        \caption{large $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.05}
    \end{subfigure}
    \caption{Schematic depiction of the spectral density of a simple matrix with
    10 eigenvalues for different values of the smothing parameter $\sigma$.}
    \label{fig:1-introduction-smoothened-spectral-density}
\end{figure}

In many applications \gls{smoothing-kernel} is chosen to be a Gaussian of width \gls{smoothing-parameter}
\begin{equation}
    g_{\sigma}(t) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{t^2}{2\sigma^2}},
    \label{equ:1-introduction-def-gaussian-kernel}
\end{equation}
but there are many other choices possible for \gls{smoothing-kernel}. Ideally,
\gls{smoothing-kernel} should be symmetric, non-negative, and tend towards the
Dirac delta function in the limit of \gls{smoothing-parameter} $\to 0$. Another
commonly used kernel will be discussed in \refsec{sec:5-experiments-haydock-method}.\\

Because we only consider symmetric matrices, 
we may represent $\mtx{A} \in \mathbb{R}^{n \times n}$ using its spectral
decomposition $\mtx{U} \mtx{\Lambda} \mtx{U}^{\top}$ where
$\mtx{\Lambda} \in \mathbb{R}^{n \times n}$ is the diagonal matrix which carries
the eigenvalues of $\mtx{A}$ on its diagonal and $\mtx{U} \in \mathbb{R}^{n \times n}$
such that $\mtx{U}^{\top} \mtx{U} = \mtx{I}$ \cite[Theorem~4.1.5]{horn1985matrix}.
Thus, if we absorb the \gls{matrix-size} in \gls{smoothing-kernel},
apply the definition of a smooth function $f$ applied to a matrix $f(\mtx{A}) = \mtx{U}^{\top} f(\mtx{\Lambda}) \mtx{U}$,
with $f(\mtx{\Lambda}) = \diag(f(\lambda_1), \dots, f(\lambda_n))$ \cite[Definition~1.2]{higham2008functions},
and use the invariance of the trace under orthonormal transformations, we may write
\begin{equation}
    \phi_{\sigma}(t) = \sum_{i=1}^n g_{\sigma}(t - \lambda_i) = \Tr(g_{\sigma}(t\mtx{I} - \mtx{A})).
    \label{equ:1-introduction-spectral-density-as-trace}
\end{equation}
In this way, we have just converted the problem of computing the eigenvalues of
a matrix to computing the trace of a function applied to the same matrix,
for which -- we will see -- exist many efficient algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
\label{sec:1-introduction-related}

Multiple approaches have been taken for approximating the \glsfirst{spectral-density}
of large symmetric matrices. They can be grouped into two families of methods:

\begin{enumerate}
    \item Lanczos-based methods
    first partially tridiagonalize a matrix and subsequently either extract Ritz values
    \cite{lin2016review, chen2021slq} or directly exploit properties of the
    \glsfirst{smoothing-kernel} \cite{haydock1972electronic, lin2016review}
    to approximate \gls{spectral-density};
    \item and interpolation-based methods which either expand
    \gls{spectral-density} \cite{weisse2006kpm} or \gls{smoothing-kernel}
    \cite{lin2016review,lin2017randomized} in a polynomial and subsequently
    use trace estimation to approximate \gls{spectral-density}.
\end{enumerate}

\todo{Mention methods explicitly}

It turns out that these two families intersect, and many of these methods
can be shown to be equivalent to each other \cite{chen2023kpm}.\\

\todo{Broaden view to general trace estimation}

Our work bases on the methods presented in \cite{lin2017randomized}.

\begin{figure}[ht]
    \centering
    \input{figures/literature_overview.tex}
    \caption{Overview of methods for approximating the spectral density.}
    \label{fig:1-introduction-literature-overview}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main contributions}
\label{sec:1-introduction-contributions}

Besides a couple of small clarifications and improvements to the algorithms 
from \cite{lin2017randomized}, we see our main contributions to be the following:% Corrected proof / quadrature free

\begin{itemize}
    \item The development of a consistent \gls{DCT}-based interpolation scheme which allows us
          to speed up \cite[Algorithm~5]{lin2017randomized} by orders of magnitude
          at no loss of accuracy;
    \item The derivation of an a priori guarantee for the number of matrix-vector
          products needed to get an accurate approximation of the spectral
          density under certain assumptions on the matrix;
    \item A simple generalization of the presented methods to other
          commonly used randomized low-rank approximation schemes;
    \item \todo{How to formulate this: The $1/\epsilon$-result for the paramter-dependent Nystr\"om++ approximation}
    \item A fast and rigorously documented implementation of all the algorithms
          following the notation and conventions of this thesis, which can be used to
          reproduce every plot, table, and much more\footnote{GitHub repository with
          code available under \url{https://github.com/FMatti/Rand-SD}}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Structure of the thesis}
\label{sec:1-introduction-structure}

This paragraph finishes off the introductory \refchp{chp:1-introduction}.
In \refchp{chp:2-chebyshev} we introduce the Chebyshev expansion, present an
efficient method for computing it, and study its convergence. We also take a
look at stochastic trace estimation to then construct a first algorithm.
\Refchp{chp:3-nystrom} is dedicated to the use of randomized low-rank approximation
for computing spectral densities, which gives rise to a second algorithm.
Putting the ideas from the two previously discussed algorithms together,
we end up with a fast and general algorithm for computing spectral densities
of large matrices in \refchp{chp:4-nystromchebyshev}. In \refchp{chp:5-experiments},
we study the accuracy and computational time of these algorithms on numerous
numerical experiments and compare them to other methods found in literature.
We conclude the thesis in \refchp{chp:6-conclusion}.
