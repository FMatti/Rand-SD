\chapter{Introduction}
\label{chp:1-introduction}

In many problems in physics, chemistry, engineering, and computer science, 
the eigenvalues of certain matrices help understand the nature of a system:
In electronic structure calculations they represent the allowed energy levels 
electrons can occupy \cite{ducastelle1970charge, haydock1972electronic, lin2017randomized};
in neural network optimization they are indicative of the optimization speed \cite{ghorbani2019investigation,chen2021slq,adepu2021hessian};
and in graph processing they can uncover hidden graph motifs \cite{kruzick2018graph,huang2021kernels,patane2022filter}.
However, computing the eigenvalues of a matrix can be prohibitively expensive.
Furthermore, when analyzing these systems, it is often not crucial to know the
exact individual eigenvalues, but more so their approximate locations with respect
to each other, such as eigenvalue clusters or spectral gaps.\\

The goal of spectral density theory is to find the approximate distribution of
the eigenvalues of large matrices. In this introductory chapter we define the
spectral density of a matrix, give an overview of the most common ways of
approximating it, and show how the work in this thesis is embedded in current
research on spectral density approximations.

\section{Spectral density}
\label{sec:1-introduction-spectral-density}

In most applications, the studied matrices are real,
i.e. $\mtx{A} \in \mathbb{R}^{n \times n}$, and symmetric, i.e. $\mtx{A}^{\top} = \mtx{A}$,
such that their eigenvalues $\lambda_1, \dots, \lambda_n$ are all real.
This allows us to define their spectral density on the real line.
\begin{definition}{Spectral density of a matrix}{1-introduction-spectral-density}
    Let $\mtx{A} \in \mathbb{R}^{n \times n}$ be a symmetric matrix with \glspl{eigenvalue}
    $\in \mathbb{R}, i=1, \dots, n$. We define the \gls{spectral-density} as
    the functional
    \begin{equation}
        \phi(t) = \frac{1}{n} \sum_{i=1}^{n} \delta(t - \lambda_i),
        \label{equ:1-introduction-def-spectral-density}
    \end{equation}
    involving the \gls{dirac-delta}
    and a \gls{spectral-parameter} $\in \mathbb{R}$.
\end{definition}

\gls{spectral-density} is the probability density \cite{klenke2013probability}
of the cumulative empirical spectral measure \cite{chen2021slq}. Knowing \gls{spectral-density}
of a matrix would -- in theory -- allow us to reconstruct its eigenvalues.
Furthermore, \gls{spectral-density} can for example be used to count
the number of eigenvalues of a matrix which lie within an interval $[a, b]$
\begin{equation}
    \nu_{[a, b]} = n \int_{a}^{b} \phi(t) \mathrm{d}t,
    \label{equ:1-introduction-eigenvalue-counting}
\end{equation}
or to compute the trace of a matrix function \cite{lin2017randomized}
\begin{equation}
    \Tr(f(\mtx{A})) = \sum_{i=1}^n f(\lambda_i) = n \int_{-\infty}^{\infty} f(t) \phi(t) \mathrm{d}t.
    \label{equ:1-introduction-matrix-trace}
\end{equation}\\

Constructing the spectral density (\refdef{def:1-introduction-spectral-density})
of a matrix amounts to knowing all its eigenvalues, which are often prohibitively
expensive to compute. Furthermore, neither can we hope to measure the convergence
of a smooth approximation to \gls{spectral-density} in any of the conventionally
used $L^p$-norms, nor can we visualize the spectral density in a simple
and easily interpretable plot. For these reasons, we proceed by working with a
smoothened spectral density.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Smooth spectral density}
\label{sec:1-introduction-properties}

We regularize \gls{spectral-density} with a suitable \gls{smoothing-kernel}
to define the \gls{smooth-spectral-density} as the convolution
\begin{equation}
    \phi_{\sigma}(t) = (\phi \ast g_{\sigma})(t) = \int_{-\infty}^{\infty} \phi(s) g_{\sigma}(t - s) \mathrm{d}s = \frac{1}{n} \sum_{i=1}^{n} g_{\sigma}(t - \lambda_i).
    \label{equ:1-introduction-def-smooth-spectral-density}
\end{equation}
The \gls{smoothing-parameter} $>0$ controls by how much \gls{spectral-density} is
smoothened (see \reffig{fig:1-introduction-smoothened-spectral-density}). Typically,
small \gls{smoothing-parameter} allow for easier approximations of \gls{smooth-spectral-density}
but at the cost of losing many of the finer characteristics of the spectrum.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/spectral_density_example_0.01.pgf}
        \caption{small $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.01}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/spectral_density_example_0.02.pgf}
        \caption{medium $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.02}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/spectral_density_example_0.05.pgf}
        \caption{large $\sigma$}
        \label{fig:1-introduction-spectral-density-example-0.05}
    \end{subfigure}
    \caption{Schematic depiction of the spectral density of a simple matrix with
    10 eigenvalues for different values of the smothing parameter $\sigma$.}
    \label{fig:1-introduction-smoothened-spectral-density}
\end{figure}

Commonly, \gls{smoothing-kernel} is chosen to be a Gaussian of width \gls{smoothing-parameter}
\begin{equation}
    g_{\sigma}(s) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{s^2}{2\sigma^2}},
    \label{equ:1-introduction-def-gaussian-kernel}
\end{equation}
due to its rapidly decaying tail and desirable interpolation properties \cite{lin2017randomized}.
There are many other choices possible for \gls{smoothing-kernel}. Ideally,
\gls{smoothing-kernel} should be smooth, symmetric, non-negative, and tend -- in a weak sense --
towards the \glsfirst{dirac-delta} in the limit of \gls{smoothing-parameter} $\to 0$. Another
commonly used kernel is the Lorentzian, which will be discussed in an example
in \refsec{sec:5-experiments-haydock-method}.\\

Because we only consider symmetric matrices, 
we may represent $\mtx{A} \in \mathbb{R}^{n \times n}$ using its spectral
decomposition $\mtx{A} = \mtx{U} \mtx{\Lambda} \mtx{U}^{\top}$ where
$\mtx{\Lambda} \in \mathbb{R}^{n \times n}$ is the diagonal matrix which carries
the eigenvalues of $\mtx{A}$ on its diagonal and $\mtx{U} \in \mathbb{R}^{n \times n}$
is orthonormal, i.e. $\mtx{U}^{\top} \mtx{U} = \mtx{I}_n$ \cite[theorem~4.1.5]{horn1985matrix}.
Thus, if we absorb the factor in \refequ{equ:1-introduction-def-smooth-spectral-density}
involving the \gls{matrix-size} in \gls{smoothing-kernel},
use the definition of a smooth function $f$ applied to a symmetric matrix
$f(\mtx{A}) = \mtx{U}^{\top} f(\mtx{\Lambda}) \mtx{U}$,
with $f(\mtx{\Lambda}) = \diag(f(\lambda_1), \dots, f(\lambda_n))$ \cite[definition~1.2]{higham2008functions},
and finally use the invariance of the trace under orthonormal transformations, we may write
\begin{equation}
    \phi_{\sigma}(t) = \sum_{i=1}^n g_{\sigma}(t - \lambda_i) = \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})).
    \label{equ:1-introduction-spectral-density-as-trace}
\end{equation}
In this way, we have just converted the problem of computing the eigenvalues of
a matrix to computing the trace of a function applied to the same matrix,
for which -- we will see -- exist many efficient algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview of the methods developed in this thesis}
\label{sec:1-introduction-overview}

The three methods we will develop in this thesis are based on \cite{lin2017randomized}.
All of them are closely related to each other. In a first stage, they all expand the matrix
function $g_{\sigma}(t\mtx{I}_n - \mtx{A})$ from
\refequ{equ:1-introduction-spectral-density-as-trace} in a truncated basis of
Chebyshev polynomials. The \gls{DGC} method then proceeds by approximating the
trace in \refequ{equ:1-introduction-spectral-density-as-trace} with a stochastic
trace estimator. The \gls{NC} method instead expresses the expanded matrix function
as a product of smaller matrices, for which we can directly compute the trace.
Finally, the \gls{NCPP} method combines the \gls{DGC} and \gls{NC} method
by first factorizing the matrix and subsequently correcting for the approximation
error by estimating the trace of the residual of this approximation with a stochastic trace estimator.
In fact, if we denote with $\widetilde{\phi}_{\sigma}^{(m)}(t)$ the approximation
of \gls{smooth-spectral-density} computed with the \gls{DGC} method, $\widehat{\phi}_{\sigma}^{(m)}(t)$ the
one computed with the \gls{NC} method, and $\breve{\phi}_{\sigma}^{(m)}(t)$ the one
with \gls{NCPP}, we can show that
\begin{equation}
    \breve{\phi}_{\sigma}^{(m)}(t) = \widetilde{\phi}_{\sigma}^{(m)}(t) + \widehat{\phi}_{\sigma}^{(m)}(t) - \text{some additional term}
    \label{equ:1-introduction-relation-methods}
\end{equation}
It will turn out that the additional term is determined by applying
the \gls{DGC} method to the matrix factorization produced in the \gls{NC} method.
A schematic overview of the relation between these methods can be found
in \reffig{fig:1-introduction-methods-overview}.

\begin{figure}[ht]
    \centering
    \input{figures/methods_overview.tex}
    \caption{Overview of the techniques used in the three methods presented
    in this thesis: the \glsfirst{DGC}, \glsfirst{NC}, and \glsfirst{NCPP} methods.}
    \label{fig:1-introduction-methods-overview}
\end{figure}

The \gls{DGC} method from \cite{lin2017randomized} is very similar to our version
of this method, except that we employ a more rigorous and faster approach when
computing the Chebyshev expansion. \cite{lin2017randomized} also proposes the
\gls{SS} method, as well as the \gls{RESS} method. We decide to not discuss the
\gls{SS} method, since we have found an equivalent version of this method which
incorporates the \enquote{efficient} part from the \gls{RESS} method at no loss
of accuracy. We call it the \gls{NC} method.
Finally, the \gls{NCPP} method is derived from the \gls{RESS} method. Both the
\gls{NC} and \gls{NCPP} methods feature major algorithmic improvements compared
to their ancestors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
\label{sec:1-introduction-related}

Multiple approaches have been taken for approximating the \glsfirst{spectral-density}
of large symmetric matrices. They can loosely be grouped into two big families
of methods (see also \reffig{fig:1-introduction-literature-overview} for an
illustrated overview of the methods):

\begin{enumerate}
    \item Lanczos-based methods first partially tridiagonalize the matrix and
    subsequently either extract Ritz values or directly exploit properties of the
    \glsfirst{smoothing-kernel} to approximate \gls{spectral-density};
    \item and expansion-based methods, which either compute a truncated expansion of
    \gls{spectral-density} or \gls{smoothing-kernel} in a polynomial basis and subsequently
    make use of trace estimation to approximate \gls{spectral-density}.
\end{enumerate}

\begin{figure}[ht]
    \centering
    \input{figures/literature_overview.tex}
    \caption{An overview of the most popular methods for approximating
        spectral densities of large symmetric matrices. For each method, the publication
        where it was originally introduced, the type of smoothing it applies,
        what kind of expansion is used, and how the trace in
        \refequ{equ:1-introduction-spectral-density-as-trace} is approximated.
        Relations between the methods are indicated with dashed lines.}
    \label{fig:1-introduction-literature-overview}
\end{figure}

Among the Lanczos-based methods features the Haydock method \cite{haydock1972electronic, lin2016review}.
It stochastically estimates the trace of the Lorentzian kernel applied to a matrix
by first tridiagonalizing the matrix with a few iterations of the Lanczos algorithm
\cite{lanczos1950iteration} and subsequently evaluating the matrix function using
a continued fraction formula. The stochastic Lanczos quadrature \cite{lin2016review, ubaru2017lanczos,chen2021slq}
also first tridiagonalizes the matrix. It then extracts the nodes and weights
for the corresponding Gaussian quadrature to build an approximate spectral density.
The expansion-based methods encompass the kernel polynomial methods \cite{silver1994kpm, wang1994kpm, weisse2006kpm}
which involve the formal expansion of a suitable modification of \gls{spectral-density}
in a basis of orthogonal polynomials. The Delta-Gauss-Legendre \cite{lin2016review}
and Delta-Gauss-Chebyshev \cite{lin2017randomized} methods instead expand \gls{smoothing-kernel}
and use this expansion to efficiently evaluate a stochastic trace estimator.\\

It turns out that these two families are not distinct, and in fact some of these
methods can be shown to be equivalent to each other: The Lanczos algorithm
can be used as an engine for the kernel polynomial method \cite{chen2023kpm},
while smoothing the approximation resulting from the kernel polynomial method 
will give the same result as the Delta-Gauss-Legendre and Delta-Gauss-Chebyshev
methods \cite{lin2016review}.\\ 

Spectral densities of matrices have found use in priming the computation of matrix
functions \cite{fan2020spectrum} and in parallelized eigenvalue solvers
\cite{polizzi2009density, li2019slicing}. In recent years, significant progress
in the theory of stochastic trace estimation, which is the backbone of most
expansion-based methods for approximating spectral densities, has been made \cite{meyer2021hutch, persson2022hutch}.
These developments involved demonstrating the reciprocal decrease
of the approximation error with the number of matrix-vector multiplication
queries used, which is a highly desirable algorithmic property.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Main contributions}
\label{sec:1-introduction-contributions}

Besides a couple of small clarifications to \cite{lin2017randomized}, we see our
main contributions to be

\begin{itemize}
    \item the development of a simple and consistent \gls{DCT}-based interpolation
          scheme which allows us to speed up \cite[algorithm~5]{lin2017randomized}
          by orders of magnitude at no loss of accuracy;
    \item the proposal of multiple algorithmic improvements and speed-ups
          of the algorithms from \cite{lin2017randomized};
    %\item the derivation of an a priori guarantee for the number of matrix-vector
    %      products needed to get an accurate approximation of the spectral
    %      density under certain assumptions on the matrix;
    \item an analogoue result for the reciprocal decrease of the approximation
          error with the number of queries as seen in \cite[theorem~1]{meyer2021hutch}
          but for the trace of parameter dependent matrices;
    \item proposal of a simple generalization of the presented methods to other
          commonly used randomized low-rank approximation schemes;
    \item a fast and rigorously documented implementation of all the algorithms
          following the notation and conventions of this thesis, which can be used to
          reproduce every plot, table, and much more\footnote{A GitHub repository
          with the code which allows to easily reproduce this entire document
          is available at \url{https://github.com/FMatti/Rand-SD}.}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Structure of the thesis}
\label{sec:1-introduction-structure}

This paragraph finishes off the introductory \refchp{chp:1-introduction}.
In \refchp{chp:2-chebyshev} we introduce the Chebyshev expansion, present an
efficient method for computing it, and study its convergence. We also take a
look at stochastic trace estimation to then construct a first algorithm using these tools.
\Refchp{chp:3-nystrom} is dedicated to the use of randomized low-rank factorization
of matrices for computing spectral densities, which gives rise to a second algorithm.
Putting the ideas from the two previously discussed algorithms together,
we end up with a fast and general algorithm for computing spectral densities
of large matrices in \refchp{chp:4-nystromchebyshev}. In \refchp{chp:5-experiments},
we study the accuracy and computational time of these algorithms on numerous
numerical experiments. We conclude the thesis in \refchp{chp:6-conclusion}.
