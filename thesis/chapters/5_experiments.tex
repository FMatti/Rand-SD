\chapter{Numerical experiments}
\label{chp:5-experiments}

In the previous chapters we have developed three methods for approximating the
spectral density of a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$.
All methods compute the trace in \refequ{equ:2-chebyshev-spectral-density-as-trace-expansion}
in slightly different ways. The \glsfirst{DGC} method uses the Girard-Hutchinson
stochastic trace estimator (\refsec{sec:2-chebyshev-delta-gauss-chebyshev}), the
\glsfirst{NC} method computes a randomized low-rank factorization of the matrix
function (\refsec{sec:3-nystrom-nystrom-chebyshev}), while the \glsfirst{NCPP}
method combines the two aforementioned approaches into a variance-reduced
stochastic trace estimator (\refsec{sec:4-nystromchebyshev-nystromchebyshev-pp}).
For each of these methods we have presented a number of algorithmic improvements,
which distinguish them from the methods of \cite{lin2017randomized}.
In the following numerical experiments -- where applicable -- we use these improvements with
\glsfirst{threshold-factor} $=10^{-7}$, \glsfirst{filter-tolerance} $=10^{-3}$, and
\glsfirst{short-circuit-threshold} $=10^{-5}$ (\refsec{subsec:3-nystrom-implementation-details}),
as we have seen them to be crucial for obtaining a reasonable approximation
on the full spectrum \reffig{fig:3-nystrom-improved-algorithm}.\\

Our goal is to compare these methods with each other and with other related methods
in terms of their accuracy and speed. In order to do so, we apply these algorithms
in multiple scenarios. We first consider a model problem from density functional theory
\cite{lin2017randomized} and discuss the convergence properties of our algorithms
in this setting (\refsec{sec:5-experiments-density-function}). Subsequently,
we show that the developed methods are also effective for other choices of
kernels, for example the Lorentzian kernel (\refsec{sec:5-experiments-haydock-method}).
Finally, we test the methods on various other matrices which we have found to
be commonly used in literature (\refsec{sec:5-experiments-various-matrices}).\\

The accuracy is measured in terms of the discrete relative $L^1$-error of the approximated
spectral density $\widetilde{\phi}_{\sigma}^{(m)}$ (also denoted $\widehat{\phi}_{\sigma}^{(m)}$ and
$\breve{\phi}_{\sigma}^{(m)}$) from the spectral density $\phi_{\sigma}$
which we obtain using standard eigenvalue solvers\footnote{We use the
standard symmetric eigenvalue solver from the NumPy Python package: \url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvalsh.html}.}:
\begin{equation}
    \frac{\sum_{i=1}^{n_t} |\widetilde{\phi}_{\sigma}^{(m)}(t_i) - \phi_{\sigma}(t_i)|}{\sum_{i=1}^{n_t} |\phi_{\sigma}(t_i)|}.
    \label{equ:5-experiments-L1-error}
\end{equation}
We use $n_t=100$ evenly spaced evaluation points which cover the whole spectrum of
$\mtx{A}$. The choice of this metric can be justified by the fact that this
error roughly corresponds to the midpoint quadrature rule
applied to the continuous $L^1$-norm, for which our theoretical results hold.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model problem from density functional theory}
\label{sec:5-experiments-density-function}

For our first example, we consider the matrix which arises from the second order
finite difference discretization of the Laplace operator $\Delta$ in a potential
field $V$,
\begin{equation}
    \mathcal{A} u(\vct{x}) = - \Delta u(\vct{x}) + V(\vct{x}) u(\vct{x}),
    \label{equ:5-experiments-electronic-hamiltonian}
\end{equation}
for a uniform mesh of size $h=0.6$. The potential $V$ results from a
lattice whose primitive cell is of side-length $L=6$ and in whose center a
potential
\begin{equation}
    \alpha \exp(-\frac{\lVert \vct{x} \rVert _2^2}{ 2 \beta^2 })
    \label{equ:5-experiments-gaussian-cell}
\end{equation}
with $\alpha = -4$, $\beta = 2$ is located. The computational domain is chosen
to span $n_c \in \mathbb{N}$ primitive cells in every spatial dimension, hence, yielding
discretization matrices which are growing in size with $n_c$. In our experiments
we consider the three-dimensional case, but for visualization purposes, we
illustrate the potential in \reffig{fig:5-experiments-periodic-gaussian-well}
in two dimensions.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_1.pgf}
        \caption{$n_c=1$}
        \label{fig:5-experiments-periodic-gaussian-well-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_2.pgf}
        \caption{$n_c=2$}
        \label{fig:5-experiments-periodic-gaussian-well-2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_5.pgf}
        \caption{$n_c=5$}
        \label{fig:5-experiments-periodic-gaussian-well-5}
    \end{subfigure}
    \caption{Two dimensional periodic potential $V$ for different sizes $n_c$ of the computational domain.}
    \label{fig:5-experiments-periodic-gaussian-well}
\end{figure}

For Gaussian \gls{smoothing-kernel} \refequ{equ:1-introduction-def-gaussian-kernel}
with \gls{smoothing-parameter} $=0.05$ we plot for $n_c=1$ and two choices of \gls{chebyshev-degree}
the convergence of the error with \gls{sketch-size} in \reffig{fig:5-experiments-electronic-structure-convergence-nv}
and equally for two choices of \gls{sketch-size} $+$ \gls{num-hutchinson-queries} the convergence of the
error with \gls{chebyshev-degree} in \reffig{fig:5-experiments-electronic-structure-convergence-nv}.
In our experiments, we always use \gls{sketch-size} $=$ \gls{num-hutchinson-queries} for
the \gls{NCPP} method.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_nv_m800.pgf}
        \caption{\gls{chebyshev-degree} $=800$}
        \label{fig:5-experiments-electronic-structure-convergence-nv-m800}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_nv_m2400.pgf}
        \caption{\gls{chebyshev-degree} $=2400$}
        \label{fig:5-experiments-electronic-structure-convergence-nv-m2400}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem with \gls{smoothing-parameter} $=0.05$.
    We also indicate orders of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    as a function of the relative $L^1$ error $\varepsilon$.}
    \label{fig:5-experiments-electronic-structure-convergence-nv}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_m_nv40.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=40$}
        \label{fig:5-experiments-electronic-structure-convergence-m-nv40}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_m_nv160.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=160$}
        \label{fig:5-experiments-electronic-structure-convergence-m-nv160}
    \end{subfigure}
    \caption{For increasing values of \gls{chebyshev-degree} but fixed
    \gls{sketch-size} $+$ \gls{num-hutchinson-queries} we plot the $L^1$ relative
    approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-electronic-structure-convergence-m}
\end{figure}

In \reffig{fig:5-experiments-electronic-structure-convergence-nv-m800} the
Chebyshev expansion is clearly not accurate enough for a good approximation
of the spectral density. This is confirmed by \reffig{fig:5-experiments-electronic-structure-convergence-m}:
unless a Chebyshev expansion of degree \gls{chebyshev-degree} $\gtrapprox 1000$ is used,
we cannot hope for high accuracy approximations.
\Reffig{fig:5-experiments-electronic-structure-convergence-nv-m2400} allows us to
make an interesting observation: The approximation error for the \gls{NCPP} method
first decays quite slowly compared to the \gls{NC} method.
To achieve an $\varepsilon$-error we require \gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=\mathcal{O}(\varepsilon^{-1})$
as suggested by \refthm{thm:4-nystromchebyshev-trace-correction-parameter-dependent}.
However, after \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
exceeds a certain value, the approximation error shoots down quickly to where it
saturates. The reason is that at this point \gls{sketch-size} starts exceeding
the \glsfirst{numerical-rank} of the model matrix \reffig{fig:3-nystrom-singular-value-decay},
which, as a consequence of
\refthm{thm:3-nystrom-frobenius-norm}, means that the approximation error is
expected to be significantly smaller than \refthm{thm:4-nystromchebyshev-final}
guarantees in general. In fact, it seems that after this point, \gls{NC} and \gls{NCPP}
behave almost identically, with the exception that the \gls{NC} uses 
an \gls{sketch-size} which is twice as large as the one in \gls{NCPP} by design
of the experiment, while the contribution from the Girard-Hutchinson correction part
in the \gls{NCPP} method \refequ{equ:4-nystromchebyshev-hutch-pp} seems to be
insignificant.\\

In \reftab{tab:5-experiments-timing-DGC} we list the wall-clock time each method
takes to compute an approximate \gls{spectral-density} at \gls{num-evaluation-points} $=100$ points
for different values of \gls{sketch-size} and \gls{chebyshev-degree}.\\

\begin{table}[ht]
    \caption{Comparison of the runtime in seconds of the algorithms applied to the model problem
        for approximating the \glsfirst{smooth-spectral-density} with 
        \gls{smoothing-parameter} $=0.05$ at \gls{num-evaluation-points} $=100$
        points for various choices of \gls{chebyshev-degree} and \gls{sketch-size} $+$ \gls{num-hutchinson-queries}.
        The mean and standard deviation of 7 runs is given.}
    \label{tab:5-experiments-timing-DGC}
    \input{tables/timing_DGC.tex}
\end{table}

We have seen that the \gls{NCPP} method is a hybrid method between the \gls{DGC} and \gls{NC} methods.
In fact, for \gls{sketch-size} $=0$, the \gls{NCPP} is equivalent to the \gls{DGC}
method, while for \gls{num-hutchinson-queries} $=0$, it is equivalent to the \gls{NC} method.
Back in \refchp{chp:3-nystrom} we already saw that for small values of \gls{smoothing-parameter}
the Nystr\"om approximation will only need a small \gls{sketch-size} in order
to achieve an accurate approximation. On the other hand, for large choices of
\gls{smoothing-parameter} the low-rank approximation will -- by itself -- not suffice.
The interplay between the two parts which make up the \gls{NCPP} method,
on one hand the low-rank approximation and on the other hand the
trace estimation on the residual, is illustrated well in
\reffig{fig:5-experiments-electronic-structure-matvec-mixture}.
For various values of \gls{smoothing-parameter} and a simultaneously changing
\gls{chebyshev-degree} $=120 / \sigma$ to keep an approximately equal expansion
accuracy, the behavior of the error for fixed \gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=80$ is plotted.

\begin{figure}[ht]
    \centering
    \input{plots/electronic_structure_matvec_mixture.pgf}
    \caption{The \gls{NCPP} method for different ways of allocations a 
    total of \gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=80$ random vectors
    to either the Nystr\"om low-rank approximation or the Girard-Hutchinson trace estimation
    for the Gaussian \gls{smoothing-kernel} with multiple different values of
    the \gls{smoothing-parameter}. We make the approximation error made in the
    Chebyshev expansion negligible by rescaling \gls{chebyshev-degree} $=120 / \sigma$.}
    \label{fig:5-experiments-electronic-structure-matvec-mixture}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Benchmark against Haydock's method}
\label{sec:5-experiments-haydock-method}

Haydock's method \cite{haydock1972electronic,lin2016review} is a specialized technique for approximating \gls{smooth-spectral-density}
in the case where a Lorentzian smoothing kernel
\begin{equation}
    g_{\sigma}(s) = \frac{1}{\pi} \frac{\sigma}{s^2 + \sigma^2}
    \label{equ:5-experiments-cauchy-kernel}
\end{equation}
is used. A comparison of this kernel with the Gaussian kernel \refequ{equ:1-introduction-def-gaussian-kernel}
is provided in \reffig{fig:5-experiments-haydock-kernel}.\\
\begin{figure}[ht]
    \centering
    \input{plots/haydock_kernel.pgf}
    \caption{Comparison of the Gaussian with the Lorentzian \glsfirst{smoothing-kernel}
        for \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-haydock-kernel}
\end{figure}

We repeat the same experiments as in \refsec{sec:5-experiments-density-function}
but this time for a Lorentzian kernel with Haydock's method to demonstrate that
also in the case of non-Gaussian \gls{smoothing-kernel} our theoretical guarantees
apply. In this method, we use Lanczos with reorthogonalization and
fix the number of Lanczos iterations to \gls{chebyshev-degree}
and the amount of random vectors used in the Monte-Carlo estimate to \gls{sketch-size} $+$ \gls{num-hutchinson-queries}.
We plot the results in \reffig{fig:5-experiments-haydock-convergence-nv} and
\reffig{fig:5-experiments-haydock-convergence-m}, and compare the wall-clock time
between the methods in \reftab{tab:5-experiments-timing-haydock}.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_nv_m800.pgf}
        \caption{\gls{chebyshev-degree} $=800$}
        \label{fig:5-experiments-haydock-convergence-nv-m800}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_nv_m2400.pgf}
        \caption{\gls{chebyshev-degree} $=2400$}
        \label{fig:5-experiments-haydock-convergence-nv-m2400}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem from \refsec{sec:5-experiments-density-function} with
    the Lorentzian kernel with \gls{smoothing-parameter} $=0.05$.
    We also indicate orders of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    as a function of the relative $L^1$ error $\varepsilon$.}
    \label{fig:5-experiments-haydock-convergence-nv}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_m_nv40.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=40$}
        \label{fig:5-experiments-haydock-convergence-m-nv40}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_m_nv160.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=160$}
        \label{fig:5-experiments-haydock-convergence-m-nv160}
    \end{subfigure}
    \caption{For increasing values of \gls{chebyshev-degree} but fixed
    \gls{sketch-size} $+$ \gls{num-hutchinson-queries} we plot the $L^1$ relative
    approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem from \refsec{sec:5-experiments-density-function} with
    the Lorentzian kernel with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-haydock-convergence-m}
\end{figure}

On one hand the low-rank factorization for the Lorentzian \gls{smoothing-kernel}
is not as effective as it was for the Gaussian case, since the decay to zero
is noticeably slower (see \reffig{fig:5-experiments-haydock-kernel}). On the
other, the Lorentzian \gls{smoothing-kernel} has a pole at $s = \pm \iota$, which
has as a consequence that the Chebyshev expansion is not guaranteed to converge
as fast as it does in the Gaussian case.
Due to these reasons, the convergence of the \gls{NC} and \gls{NCPP} methods
are slower than they used to be in \refsec{sec:5-experiments-density-function}.
Nevertheless, this choice of \gls{smoothing-kernel} exhibits perfectly the 
$\mathcal{O}(\varepsilon^{-1})$ and $\mathcal{O}(\varepsilon^{-2})$
dependence of the $L^1$-approximation error on \gls{num-hutchinson-queries}
and \gls{sketch-size}, which the Haydock and \gls{NCPP}
methods respectively show in \reffig{fig:5-experiments-haydock-convergence-nv-m2400}.\\

\begin{table}[ht]
    \caption{Comparison of the runtime in seconds of the algorithms applied to the model problem
    from \refsec{sec:5-experiments-density-function}
    for approximating the \glsfirst{smooth-spectral-density} with a Lorentzian kernel \gls{smoothing-kernel} with
    \gls{smoothing-parameter} $=0.05$ at \gls{num-evaluation-points} $=100$
    points for various choices of \gls{chebyshev-degree} and \gls{sketch-size} $+$ \gls{num-hutchinson-queries}.
    The mean and standard deviation of 7 runs is given.}
    \label{tab:5-experiments-timing-haydock}
   \input{tables/timing_haydock.tex}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Experiments with various matrices}
\label{sec:5-experiments-various-matrices}

We test the algorithms on various problems encountered in literature.
We take a synthetic sparse matrix with $2000$ uniformly spaced eigenvalues
in $[-1, 1]$ \cite{chen2021slq};
GOE, a matrix $\mtx{A} = (\mtx{G} + \mtx{G}^{\top})/\sqrt{2}$ with standard
normal $\mtx{G} \in \mathbb{R}^{1000 \times 1000}$ from the Gaussian Orthogonal Ensemble;
the matrix ModES3D\_8, an $8000 \times 8000$ sparse matrix resulting
from the same problem as in \refsec{sec:5-experiments-density-function} but with
$n_c=2$, i.e. a larger computational domain \cite{lin2017randomized}; and
Erdos992\footnote{Downloaded in the matrix marked format from: \url{https://sparse.tamu.edu/Pajek/Erdos992}.},
a $6100 \times 6100$ sparse matrix representing the collaboration network of the
Hungarian mathematician P\'al Erd\H{o}s from \cite{chen2021slq}.
All these
matrices are symmetric. For all of them, we compute, for
fixed \gls{chebyshev-degree} $=2400$ and increasing \gls{sketch-size} $+$ \gls{num-hutchinson-queries},
the relative $L^1$ approximation error
of the spectral density for a Gaussian \glsfirst{smoothing-kernel} with
\gls{smoothing-parameter} $=0.05$. The resulting plots are displayed in 
\reffig{fig:5-experiments-multi-matrix-convergence}.\\ 

We observe that for the two matrices which have an evenly distributed spectrum
(\reffig{fig:5-experiments-multi-matrix-convergence-uniform}) or an approximately
evenly distributed spectrum (\reffig{fig:5-experiments-multi-matrix-convergence-ModES3D}),
the \gls{NC} method by itself can achieve a good approximation once \gls{sketch-size}
exceeds the numerical rank of the matrix. On the other hand, for matrices where
the spectrum is very concentrated around a certain point (\reffig{fig:5-experiments-multi-matrix-convergence-Erdos})
or approximately describes a semi-circle (\reffig{fig:5-experiments-multi-matrix-convergence-goe}) \cite{wigner1958distribution},
the \gls{NC} is not as effective, and the correction part in the \gls{NCPP} makes
a significant difference.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_uniform.pgf}
        \caption{uniform}
        \label{fig:5-experiments-multi-matrix-convergence-uniform}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_goe.pgf}
        \caption{GOE}
        \label{fig:5-experiments-multi-matrix-convergence-goe}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_ModES3D_8.pgf}
        \caption{ModES3D\_8}
        \label{fig:5-experiments-multi-matrix-convergence-ModES3D}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_Erdos992.pgf}
        \caption{Erdos992}
        \label{fig:5-experiments-multi-matrix-convergence-Erdos}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} $=2400$ we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for multiple different matrices. We use a
    Gaussian \glsfirst{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-multi-matrix-convergence}
\end{figure}
