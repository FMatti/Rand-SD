\chapter{Numerical experiments}
\label{chp:5-experiments}

In the previous chapters we have introduced multiple methods for approximating the
spectral density of a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$. Now,
our goal is to compare these methods with each other and with other related methods
in terms of their accuracy and speed. In order to do so, we apply these algorithms
in multiple scenarios. We consider a specific example from density functional theory
\cite{lin2017randomized} for two different \gls{smoothing-kernel}, and
subsequently test the methods on various other matrices.\\

The accuracy is measured in terms of the discrete relative $L^1$ error of the approximated
spectral density $\widetilde{\phi}_{\sigma}^m$ from the spectral density $\phi_{\sigma}$
which we obtain using standard eigenvalue solvers\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvalsh.html}}.
\begin{equation}
    \frac{\sum_{i=1}^{n_t} |\widetilde{\phi}_{\sigma}^m(t_i) - \phi_{\sigma}(t_i)|}{\sum_{i=1}^{n_t} |\phi_{\sigma}(t_i)|}.
    \label{equ:5-experiments-L1-error}
\end{equation}
We always use $n_t=100$ evenly spaced evaluation points which cover the whole spectrum of
$\mtx{A}$. The choice of this metric can be justified by the fact that this
error is the midpoint quadrature rule \cite[chapter~9.2.1]{quarteroni2007numerical}
applied to the $L^1$-norm, for which our theoretical results hold.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model problem from density functional theory}
\label{sec:5-experiments-density-function}

For our first example, we assemble the matrix which arises from the second order
finite difference discretization of the differential operator
\begin{equation}
    \mathcal{A} u(\vct{x}) = - \Delta u(\vct{x}) + V(\vct{x}) u(\vct{x})
    \label{equ:5-experiments-electronic-hamiltonian}
\end{equation}
for a uniform mesh of size $h=0.6$. The potential $V$ results from a
lattice whose primitive cell is of side-length $L=6$ and in whose center the
charge
\begin{equation}
    \alpha \exp(-\frac{\lVert \vct{x} \rVert _2^2}{ 2 \beta^2 })
    \label{equ:5-experiments-gaussian-cell}
\end{equation}
with $\alpha = 4$, $\beta = 2$ is located. The computational domain is chosen
to span $c$ primitive cells in every spatial dimension, hence, yielding
discretization matrices which are growing in size with $c$. In our experiments
we consider the three-dimensional case, but for visualization purposes, we
illustrate the potential in \reffig{fig:5-experiments-periodic-gaussian-well}
in two dimensions.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_1.pgf}
        \caption{$c=1$}
        \label{fig:5-experiments-periodic-gaussian-well-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_2.pgf}
        \caption{$c=2$}
        \label{fig:5-experiments-periodic-gaussian-well-2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_5.pgf}
        \caption{$c=5$}
        \label{fig:5-experiments-periodic-gaussian-well-5}
    \end{subfigure}
    \caption{Two dimensional periodic potential $V$ for different sizes $c$ of the computational domain.}
    \label{fig:5-experiments-periodic-gaussian-well}
\end{figure}

For Gaussian \gls{smoothing-kernel} \refequ{equ:1-introduction-def-gaussian-kernel}
with \gls{smoothing-parameter} $=0.05$ we plot for two choices of \gls{chebyshev-degree}
the convergence of the error with \gls{sketch-size} in \reffig{fig:5-experiments-electronic-structure-convergence-nv}
and equally for two choices of \gls{sketch-size}$+$\gls{num-hutchinson-queries} the convergence of the
error with \gls{chebyshev-degree} in \reffig{fig:5-experiments-electronic-structure-convergence-nv}.
In our experiments, we always use \gls{sketch-size}$=$\gls{num-hutchinson-queries} for
the \gls{NCPP} method.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_nv_m800.pgf}
        \caption{\gls{chebyshev-degree}$=800$}
        \label{fig:5-experiments-electronic-structure-convergence-nv-m800}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_nv_m2400.pgf}
        \caption{\gls{chebyshev-degree}$=2400$}
        \label{fig:5-experiments-electronic-structure-convergence-nv-m2400}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size}$+$\gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem with \gls{smoothing-parameter}$=0.05$.}
    \label{fig:5-experiments-electronic-structure-convergence-nv}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_m_nv40.pgf}
        \caption{\gls{sketch-size}$+$\gls{num-hutchinson-queries}$=40$}
        \label{fig:5-experiments-electronic-structure-convergence-m-nv40}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_m_nv160.pgf}
        \caption{\gls{sketch-size}$+$\gls{num-hutchinson-queries}$=160$}
        \label{fig:5-experiments-electronic-structure-convergence-m-nv160}
    \end{subfigure}
    \caption{For increasing values of \gls{chebyshev-degree} but fixed
    \gls{sketch-size}$+$\gls{num-hutchinson-queries} we plot the $L^1$ relative
    approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem with \gls{smoothing-parameter}$=0.05$.}
    \label{fig:5-experiments-electronic-structure-convergence-m}
\end{figure}

\Reffig{fig:5-experiments-electronic-structure-convergence-nv-m2400} allows us to
make an interesting observation: the approximation error for the \gls{NCPP} method
first decays quite slowly compared to the \gls{NC} method. However, 
after \gls{sketch-size}$+$\gls{num-hutchinson-queries}
exceeds a certain value, the approximation error shoots down quickly to where it
saturates. The reason is that at this point \gls{sketch-size} starts exceeding
the \gls{numerical-rank} of the model matrix, which, as a consequence of
\refthm{thm:3-nystrom-frobenius-norm}, means that the approximation error is
expected to be significantly smaller than \refthm{thm:4-nystromchebyshev-final}
guarantees in general. In fact, it seems that after this point, \gls{NC} and \gls{NCPP}
behave almost identically, with the exception that the \gls{NC} uses 
a \gls{sketch-size} which is twice as large as the one in \gls{NCPP} by design
of the experiment, while the contribution from the Hutchinson's correction part
in \refequ{equ:4-nystromchebyshev-hutch-pp} seems to be insignificant.\\

In \reftab{tab:5-experiments-timing-DGC} we list the wall time each method
takes to compute an approximate \gls{spectral-density} at \gls{num-evaluation-points}$=100$ points
for different values of \gls{sketch-size} and \gls{chebyshev-degree}.\\

\begin{table}[ht]
    \caption{Runtime comparison of the algorithms applied to the model problem
        for approximating the \glsfirst{smooth-spectral-density} with 
        \gls{smoothing-parameter}$=0.05$ at \gls{num-evaluation-points}$=100$
        points for various choices of \gls{chebyshev-degree} and \gls{sketch-size}$+$\gls{num-hutchinson-queries}.
        The mean and standard deviation of 7 runs is given.}
    \label{tab:5-experiments-timing-DGC}
    \input{tables/timing_DGC.tex}
\end{table}

The \gls{NCPP} is a hybrid method between the \gls{DGC} and \gls{NC} methods.
In fact, for \gls{sketch-size}$=0$, the \gls{NCPP} is equivalent to the \gls{DGC}
method, while for \gls{num-hutchinson-queries}$=0$, it is equivalent to the \gls{NC} method.
Back in \refchp{chp:3-nystrom} we already saw that for small values of \gls{smoothing-parameter}
the Nystr\"om approximation will only need a small \gls{sketching-matrix} in order
to achieve an accurate approximation. On the other hand, for large choices of
\gls{smoothing-parameter} the low-rank approximation will by itself not suffice.
The interplay between the two parts which make up the \gls{NCPP},
on one hand the low-rank approximation and on the other hand the
trace estimation on the residual, is illustrated well in
\reffig{fig:5-experiments-electronic-structure-matvec-mixture}.
For various values of \gls{smoothing-parameter} and a simultaneously changing
\gls{chebyshev-degree}$=120 / \sigma$ to keep an approximately equal interpolation
accuracy, the behavior of the error for fixed \gls{sketch-size}$+$\gls{num-hutchinson-queries}$=80$ is plotted.

\begin{figure}[ht]
    \centering
    \input{plots/electronic_structure_matvec_mixture.pgf}
    \caption{The \gls{NCPP} method for different ways of allocations a 
    total of \gls{sketch-size}$+$\gls{num-hutchinson-queries}$=80$ random vectors
    to either the Nystr\"om low-rank approximation or the Hutchinson's trace estimation
    for the Gaussian \gls{smoothing-kernel} with multiple different values of
    the \gls{smoothing-parameter}. We make the approximation error made in the
    Chebyshev expansion negligible by rescaling \gls{chebyshev-degree}$=120 / \sigma$.}
    \label{fig:5-experiments-electronic-structure-matvec-mixture}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Benchmark against Haydock's method}
\label{sec:5-experiments-haydock-method}

Haydock's method \cite{haydock1972electronic,lin2016review} is a specialized technique for approximating \gls{smooth-spectral-density}
in the case where a Lorentzian smoothing kernel
\begin{equation}
    g_{\sigma}(s) = \frac{1}{\pi} \frac{\sigma}{s^2 + \sigma^2} = -\frac{1}{\pi} \Im\left\{ \frac{1}{s + i\sigma} \right\}
    \label{equ:5-experiments-cauchy-kernel}
\end{equation}
is used. A comparison of this kernel with the Gaussian kernel \refequ{equ:1-introduction-def-gaussian-kernel}
is provided in \reffig{fig:5-experiments-haydock-kernel}.\\
\begin{figure}[ht]
    \centering
    \input{plots/haydock_kernel.pgf}
    \caption{Comparison of the Gaussian with the Lorentzian \glsfirst{smoothing-kernel}
        for \gls{smoothing-parameter}$=0.05$.}
    \label{fig:5-experiments-haydock-kernel}
\end{figure}
%Estimating \gls{smooth-spectral-density} then becomes the
%trace estimation problem
%\begin{equation}
%    \phi_{\sigma}(t) = \Tr(g_{\sigma}(t\mtx{I} - \mtx{A})) = - \frac{1}{n \pi} \Im \left\{ \Tr\left[((t + i\sigma)I - A)^{-1}\right]  \right\}.
%    \label{equ:5-experiments-haydock-trace}
%\end{equation}
%Similarly to the \gls{DGC} method (see \refsec{sec:2-chebyshev-delta-gauss-chebyshev}),
%the Hutchinson's trace estimator with standard Gaussian random vectors
%$\vct{\psi} \in \mathbb{R}^n$ is used to
%approximate the trace
%\begin{equation}
%    \Tr\left[((t + i\sigma)I - A)^{-1}\right] \approx \frac{1}{n_{\Omega}} \sum_{j=1}^{n_{\Omega}} \left( \vct{\psi}_j \right)^{\top} ((t - i\sigma)\mtx{I} - \mtx{A})^{-1} \vct{\psi}_j.
%    \label{equ:5-experiments-haydock-hutchinson}
%\end{equation}
%It turns out that each summand in \refequ{5-experiments-haydock-hutchinson} can
%be efficiently evaluated for multiple $t$ by running Lanczos
%on $\mtx{A}$ with starting vector $\vct{\psi}_j$
%\begin{equation}
%    \vct{\psi}_j^{\top} ((t - i\sigma)\mtx{I} - \mtx{A})^{-1} \vct{\psi}_j% &\approx \vct{e}_1^{\top} ((t - i\sigma)\mtx{I} - \mtx{H}_k)^{-1} \vct{e}_1 \notag \\
%    \approx \cfrac{1}{(t - i\sigma) - \alpha_1 - \cfrac{\beta_2^2}{(t - i\sigma) - \alpha_2 - \dots}}
%    \label{equ:5-experiments-haydock-recursion}
%\end{equation}

We repeat the same experiments as in \refsec{sec:5-experiments-density-function}
but this time for a Lorentzian kernel with Haydock's method. We
plot the results in \reffig{fig:5-experiments-haydock-convergence-nv} and
\reffig{fig:5-experiments-haydock-convergence-m}, and compare the wall time
between the methods in \reftab{tab:5-experiments-timing-haydock}.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_nv_m800.pgf}
        \caption{\gls{chebyshev-degree}$=800$}
        \label{fig:5-experiments-haydock-convergence-nv-m800}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_nv_m2400.pgf}
        \caption{\gls{chebyshev-degree}$=2400$}
        \label{fig:5-experiments-haydock-convergence-nv-m2400}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size}$+$\gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem from \refsec{sec:5-experiments-density-function} with
    the Lorentzian kernel with \gls{smoothing-parameter}$=0.05$.}
    \label{fig:5-experiments-haydock-convergence-nv}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_m_nv40.pgf}
        \caption{\gls{sketch-size}$+$\gls{num-hutchinson-queries}$=40$}
        \label{fig:5-experiments-haydock-convergence-m-nv40}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_m_nv160.pgf}
        \caption{\gls{sketch-size}$+$\gls{num-hutchinson-queries}$=160$}
        \label{fig:5-experiments-haydock-convergence-m-nv160}
    \end{subfigure}
    \caption{For increasing values of \gls{chebyshev-degree} but fixed
    \gls{sketch-size}$+$\gls{num-hutchinson-queries} we plot the $L^1$ relative
    approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem from \refsec{sec:5-experiments-density-function} with
    the Lorentzian kernel with \gls{smoothing-parameter}$=0.05$.}
    \label{fig:5-experiments-haydock-convergence-m}
\end{figure}

On one hand the low-rank factorization for the Lorentzian \gls{smoothing-kernel}
is not as effective as it was for the Gaussian case, since the decay to zero
is noticeably slower (see \reffig{fig:5-experiments-haydock-kernel}). On the
other, the Lorentzian \gls{smoothing-kernel} has a pole at $s = \pm i$, which
has as a consequence that the Chebyshev expansion is not guaranteed to converge
as fast as it does in the Gaussian case according to \refthm{thm:2-chebyshev-bernstein}.
Due to these reasons, the convergence of the \gls{NC} and \gls{NCPP} methods
are slower than they used to be in \refsec{sec:5-experiments-density-function}.
Nevertheless, this choice of \gls{smoothing-kernel} exhibits perfectly the 
improved convergence order for the \gls{NCPP} method with respect to
\gls{sketch-size}$+$\gls{num-hutchinson-queries} in \reffig{fig:5-experiments-haydock-convergence-nv-m2400}. \\

\begin{table}[ht]
    \caption{Runtime comparison of the algorithms applied to the model problem
    from \refsec{sec:5-experiments-density-function}
    for approximating the \glsfirst{smooth-spectral-density} with a Lorentzian kernel with
    \gls{smoothing-parameter}$=0.05$ at \gls{num-evaluation-points}$=100$
    points for various choices of \gls{chebyshev-degree} and \gls{sketch-size}$+$\gls{num-hutchinson-queries}.
    The mean and standard deviation of 7 runs is given.}
    \label{tab:5-experiments-timing-haydock}
   \input{tables/timing_haydock.tex}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Application to various matrices}
\label{sec:5-experiments-various-matrices}

\todo{[Same plots for multiple matrices]}

%\begin{figure}[ht]
%    \centering
%    \input{plots/multi_matrix_convergence_ModES3D_8.pgf}
%    \caption{\gls{sketch-size}$+$\gls{num-hutchinson-queries}$=160$}
%    \label{fig:5-experiments-multi-matrix-convergence}
%\end{figure}
