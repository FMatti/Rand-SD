\chapter{Numerical experiments}
\label{chp:5-experiments}

In the previous chapters we have introduced multiple methods for approximating the
spectral density of a symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$. Now,
our goal is to compare these methods with each other and with other related methods
in terms of their accuracy and speed. In order to do so, we apply these algorithms
in multiple scenarios. We first consider a model problem from density functional theory
\cite{lin2017randomized} and discuss the convergence properties of our algorithms
in this setting (\refsec{sec:5-experiments-density-function}). Subsequently,
we show that the developed methods are also effective for other choices of
kernels, for example the Lorentzian kernel (\refsec{sec:5-experiments-haydock-method}).
Finally, we test the methods on various other matrices which we have found to
be commonly used in literature (\refsec{sec:5-experiments-various-matrices}).\\

The accuracy is measured in terms of the discrete relative $L^1$-error of the approximated
spectral density $\widetilde{\phi}_{\sigma}^{(m)}$ (also denoted $\widehat{\phi}_{\sigma}^{(m)}$ and
$\breve{\phi}_{\sigma}^{(m)}$) from the spectral density $\phi_{\sigma}$
which we obtain using standard eigenvalue solvers\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvalsh.html}}:
\begin{equation}
    \frac{\sum_{i=1}^{n_t} |\widetilde{\phi}_{\sigma}^{(m)}(t_i) - \phi_{\sigma}(t_i)|}{\sum_{i=1}^{n_t} |\phi_{\sigma}(t_i)|}.
    \label{equ:5-experiments-L1-error}
\end{equation}
We always use $n_t=100$ evenly spaced evaluation points which cover the whole spectrum of
$\mtx{A}$. The choice of this metric can be justified by the fact that this
error roughly corresponds to the midpoint quadrature rule \cite[chapter~9.2.1]{quarteroni2007numerical}
applied to the $L^1$-norm, for which our theoretical results hold.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model problem from density functional theory}
\label{sec:5-experiments-density-function}

For our first example, we assemble the matrix which arises from the second order
finite difference discretization of the differential operator
\begin{equation}
    \mathcal{A} u(\vct{x}) = - \Delta u(\vct{x}) + V(\vct{x}) u(\vct{x})
    \label{equ:5-experiments-electronic-hamiltonian}
\end{equation}
for a uniform mesh of size $h=0.6$. The potential $V$ results from a
lattice whose primitive cell is of side-length $L=6$ and in whose center the
charge
\begin{equation}
    \alpha \exp(-\frac{\lVert \vct{x} \rVert _2^2}{ 2 \beta^2 })
    \label{equ:5-experiments-gaussian-cell}
\end{equation}
with $\alpha = 4$, $\beta = 2$ is located. The computational domain is chosen
to span $c \in \mathbb{N}$ primitive cells in every spatial dimension, hence, yielding
discretization matrices which are growing in size with $c$. In our experiments
we consider the three-dimensional case, but for visualization purposes, we
illustrate the potential in \reffig{fig:5-experiments-periodic-gaussian-well}
in two dimensions.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_1.pgf}
        \caption{$c=1$}
        \label{fig:5-experiments-periodic-gaussian-well-1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_2.pgf}
        \caption{$c=2$}
        \label{fig:5-experiments-periodic-gaussian-well-2}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\columnwidth}
        \input{plots/periodic_gaussian_well_5.pgf}
        \caption{$c=5$}
        \label{fig:5-experiments-periodic-gaussian-well-5}
    \end{subfigure}
    \caption{Two dimensional periodic potential $V$ for different sizes $c$ of the computational domain.}
    \label{fig:5-experiments-periodic-gaussian-well}
\end{figure}

For Gaussian \gls{smoothing-kernel} \refequ{equ:1-introduction-def-gaussian-kernel}
with \gls{smoothing-parameter} $=0.05$ we plot for two choices of \gls{chebyshev-degree}
the convergence of the error with \gls{sketch-size} in \reffig{fig:5-experiments-electronic-structure-convergence-nv}
and equally for two choices of \gls{sketch-size} $+$ \gls{num-hutchinson-queries} the convergence of the
error with \gls{chebyshev-degree} in \reffig{fig:5-experiments-electronic-structure-convergence-nv}.
In our experiments, we always use \gls{sketch-size} $=$ \gls{num-hutchinson-queries} for
the \gls{NCPP} method.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_nv_m800.pgf}
        \caption{\gls{chebyshev-degree} $=800$}
        \label{fig:5-experiments-electronic-structure-convergence-nv-m800}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_nv_m2400.pgf}
        \caption{\gls{chebyshev-degree} $=2400$}
        \label{fig:5-experiments-electronic-structure-convergence-nv-m2400}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-electronic-structure-convergence-nv}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_m_nv40.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=40$}
        \label{fig:5-experiments-electronic-structure-convergence-m-nv40}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/electronic_structure_convergence_m_nv160.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=160$}
        \label{fig:5-experiments-electronic-structure-convergence-m-nv160}
    \end{subfigure}
    \caption{For increasing values of \gls{chebyshev-degree} but fixed
    \gls{sketch-size} $+$ \gls{num-hutchinson-queries} we plot the $L^1$ relative
    approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-electronic-structure-convergence-m}
\end{figure}

In \reffig{fig:5-experiments-electronic-structure-convergence-nv-m800} the
Chebyshev expansion is clearly not accurate enough for a good approximation
of the spectral density. This is confirmed by \reffig{fig:5-experiments-electronic-structure-convergence-m}:
unless a Chebyshev expansion of degree \gls{chebyshev-degree} $>1000$ is used,
we cannot hope for high accuracy approximations.
\Reffig{fig:5-experiments-electronic-structure-convergence-nv-m2400} allows us to
make an interesting observation: the approximation error for the \gls{NCPP} method
first decays quite slowly compared to the \gls{NC} method.
The convergence is approximately of order $\mathcal{\varepsilon^{-1}}$
as suggested by \refthm{thm:4-nystromchebyshev-trace-correction-parameter-dependent}.
However, after \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
exceeds a certain value, the approximation error shoots down quickly to where it
saturates. The reason is that at this point \gls{sketch-size} starts exceeding
the \gls{numerical-rank} of the model matrix, which, as a consequence of
\refthm{thm:3-nystrom-frobenius-norm}, means that the approximation error is
expected to be significantly smaller than \refthm{thm:4-nystromchebyshev-final}
guarantees in general. In fact, it seems that after this point, \gls{NC} and \gls{NCPP}
behave almost identically, with the exception that the \gls{NC} uses 
a \gls{sketch-size} which is twice as large as the one in \gls{NCPP} by design
of the experiment, while the contribution from the Hutchinson's correction part
in \refequ{equ:4-nystromchebyshev-hutch-pp} seems to be insignificant.\\

In \reftab{tab:5-experiments-timing-DGC} we list the wall time each method
takes to compute an approximate \gls{spectral-density} at \gls{num-evaluation-points} $=100$ points
for different values of \gls{sketch-size} and \gls{chebyshev-degree}.\\

\begin{table}[ht]
    \caption{Comparison of the runtime in seconds of the algorithms applied to the model problem
        for approximating the \glsfirst{smooth-spectral-density} with 
        \gls{smoothing-parameter} $=0.05$ at \gls{num-evaluation-points} $=100$
        points for various choices of \gls{chebyshev-degree} and \gls{sketch-size} $+$ \gls{num-hutchinson-queries}.
        The mean and standard deviation of 7 runs is given.}
    \label{tab:5-experiments-timing-DGC}
    \input{tables/timing_DGC.tex}
\end{table}

The \gls{NCPP} is a hybrid method between the \gls{DGC} and \gls{NC} methods.
In fact, for \gls{sketch-size} $=0$, the \gls{NCPP} is equivalent to the \gls{DGC}
method, while for \gls{num-hutchinson-queries} $=0$, it is equivalent to the \gls{NC} method.
Back in \refchp{chp:3-nystrom} we already saw that for small values of \gls{smoothing-parameter}
the Nystr\"om approximation will only need a small \gls{sketching-matrix} in order
to achieve an accurate approximation. On the other hand, for large choices of
\gls{smoothing-parameter} the low-rank approximation will by itself not suffice.
The interplay between the two parts which make up the \gls{NCPP},
on one hand the low-rank approximation and on the other hand the
trace estimation on the residual, is illustrated well in
\reffig{fig:5-experiments-electronic-structure-matvec-mixture}.
For various values of \gls{smoothing-parameter} and a simultaneously changing
\gls{chebyshev-degree} $=120 / \sigma$ to keep an approximately equal interpolation
accuracy, the behavior of the error for fixed \gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=80$ is plotted.

\begin{figure}[ht]
    \centering
    \input{plots/electronic_structure_matvec_mixture.pgf}
    \caption{The \gls{NCPP} method for different ways of allocations a 
    total of \gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=80$ random vectors
    to either the Nystr\"om low-rank approximation or the Hutchinson's trace estimation
    for the Gaussian \gls{smoothing-kernel} with multiple different values of
    the \gls{smoothing-parameter}. We make the approximation error made in the
    Chebyshev expansion negligible by rescaling \gls{chebyshev-degree} $=120 / \sigma$.}
    \label{fig:5-experiments-electronic-structure-matvec-mixture}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Benchmark against Haydock's method}
\label{sec:5-experiments-haydock-method}

Haydock's method \cite{haydock1972electronic,lin2016review} is a specialized technique for approximating \gls{smooth-spectral-density}
in the case where a Lorentzian smoothing kernel
\begin{equation}
    g_{\sigma}(s) = \frac{1}{\pi} \frac{\sigma}{s^2 + \sigma^2} = -\frac{1}{\pi} \Im\left\{ \frac{1}{s + \iota \sigma} \right\}
    \label{equ:5-experiments-cauchy-kernel}
\end{equation}
is used. A comparison of this kernel with the Gaussian kernel \refequ{equ:1-introduction-def-gaussian-kernel}
is provided in \reffig{fig:5-experiments-haydock-kernel}.\\
\begin{figure}[ht]
    \centering
    \input{plots/haydock_kernel.pgf}
    \caption{Comparison of the Gaussian with the Lorentzian \glsfirst{smoothing-kernel}
        for \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-haydock-kernel}
\end{figure}
%Estimating \gls{smooth-spectral-density} then becomes the
%trace estimation problem
%\begin{equation}
%    \phi_{\sigma}(t) = \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A})) = - \frac{1}{n \pi} \Im \left\{ \Tr\left[((t + i\sigma)I - A)^{-1}\right]  \right\}.
%    \label{equ:5-experiments-haydock-trace}
%\end{equation}
%Similarly to the \gls{DGC} method (see \refsec{sec:2-chebyshev-delta-gauss-chebyshev}),
%the Hutchinson's trace estimator with standard Gaussian random vectors
%$\vct{\psi} \in \mathbb{R}^n$ is used to
%approximate the trace
%\begin{equation}
%    \Tr\left[((t + i\sigma)I - A)^{-1}\right] \approx \frac{1}{n_{\Omega}} \sum_{j=1}^{n_{\Omega}} \left( \vct{\psi}_j \right)^{\top} ((t - i\sigma)\mtx{I}_n - \mtx{A})^{-1} \vct{\psi}_j.
%    \label{equ:5-experiments-haydock-hutchinson}
%\end{equation}
%It turns out that each summand in \refequ{5-experiments-haydock-hutchinson} can
%be efficiently evaluated for multiple $t$ by running Lanczos
%on $\mtx{A}$ with starting vector $\vct{\psi}_j$
%\begin{equation}
%    \vct{\psi}_j^{\top} ((t - i\sigma)\mtx{I}_n - \mtx{A})^{-1} \vct{\psi}_j% &\approx \vct{e}_1^{\top} ((t - i\sigma)\mtx{I} - \mtx{H}_k)^{-1} \vct{e}_1 \notag \\
%    \approx \cfrac{1}{(t - i\sigma) - \alpha_1 - \cfrac{\beta_2^2}{(t - i\sigma) - \alpha_2 - \dots}}
%    \label{equ:5-experiments-haydock-recursion}
%\end{equation}

We repeat the same experiments as in \refsec{sec:5-experiments-density-function}
but this time for a Lorentzian kernel with Haydock's method. We
plot the results in \reffig{fig:5-experiments-haydock-convergence-nv} and
\reffig{fig:5-experiments-haydock-convergence-m}, and compare the wall time
between the methods in \reftab{tab:5-experiments-timing-haydock}.\\

\begin{figure}[ht]
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_nv_m800.pgf}
        \caption{\gls{chebyshev-degree} $=800$}
        \label{fig:5-experiments-haydock-convergence-nv-m800}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_nv_m2400.pgf}
        \caption{\gls{chebyshev-degree} $=2400$}
        \label{fig:5-experiments-haydock-convergence-nv-m2400}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem from \refsec{sec:5-experiments-density-function} with
    the Lorentzian kernel with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-haydock-convergence-nv}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_m_nv40.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=40$}
        \label{fig:5-experiments-haydock-convergence-m-nv40}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/haydock_convergence_m_nv160.pgf}
        \caption{\gls{sketch-size} $+$ \gls{num-hutchinson-queries} $=160$}
        \label{fig:5-experiments-haydock-convergence-m-nv160}
    \end{subfigure}
    \caption{For increasing values of \gls{chebyshev-degree} but fixed
    \gls{sketch-size} $+$ \gls{num-hutchinson-queries} we plot the $L^1$ relative
    approximation error \refequ{equ:5-experiments-L1-error}
    for the model problem from \refsec{sec:5-experiments-density-function} with
    the Lorentzian kernel with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-haydock-convergence-m}
\end{figure}

On one hand the low-rank factorization for the Lorentzian \gls{smoothing-kernel}
is not as effective as it was for the Gaussian case, since the decay to zero
is noticeably slower (see \reffig{fig:5-experiments-haydock-kernel}). On the
other, the Lorentzian \gls{smoothing-kernel} has a pole at $s = \pm \iota$, which
has as a consequence that the Chebyshev expansion is not guaranteed to converge
as fast as it does in the Gaussian case according to \refthm{thm:2-chebyshev-bernstein}.
Due to these reasons, the convergence of the \gls{NC} and \gls{NCPP} methods
are slower than they used to be in \refsec{sec:5-experiments-density-function}.
Nevertheless, this choice of \gls{smoothing-kernel} exhibits perfectly the 
$\mathcal{O}(\varepsilon^{-1})$ and $\mathcal{O}(\varepsilon^{-2})$ convergence
orders of the $L^1$ approximation error, which the \gls{DGC} and \gls{NCPP}
methods respectively guarantee with respect to
\gls{sketch-size} $+$ \gls{num-hutchinson-queries} in \reffig{fig:5-experiments-haydock-convergence-nv-m2400}.\\

\begin{table}[ht]
    \caption{Comparison of the runtime in seconds of the algorithms applied to the model problem
    from \refsec{sec:5-experiments-density-function}
    for approximating the \glsfirst{smooth-spectral-density} with a Lorentzian kernel with
    \gls{smoothing-parameter} $=0.05$ at \gls{num-evaluation-points} $=100$
    points for various choices of \gls{chebyshev-degree} and \gls{sketch-size} $+$ \gls{num-hutchinson-queries}.
    The mean and standard deviation of 7 runs is given.}
    \label{tab:5-experiments-timing-haydock}
   \input{tables/timing_haydock.tex}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Application to various matrices}
\label{sec:5-experiments-various-matrices}

We test the algorithms on various problems encountered in literature.
We take a synthetic sparse matrix with $2000$ uniformly distributed eigenvalues
in $[-1, 1]$ \cite{chen2021slq};
GOE, a matrix $\mtx{A} = (\mtx{G} + \mtx{G}^{\top})/\sqrt{2}$ with standard
normal $\mtx{G} \in \mathbb{R}^{1000 \times 1000}$ from the Gaussian Orthogonal Ensemble;
the matrix ModES3D\_8, an $8000 \times 8000$ sparse matrix resulting
from the same problem as in \refsec{sec:5-experiments-density-function} but with
a larger computational domain \cite{lin2017randomized}; and
Erdos992\footnote{\url{https://sparse.tamu.edu/Pajek/Erdos992}},
an $6100 \times 6100$ sparse matrix representing the collaboration network of the
Hungarian mathematician P\'al Erd\H{o}s from \cite{chen2021slq}.
%the matrix nd3k\footnote{\url{https://sparse.tamu.edu/ND/nd3k}},
%an $9000 \times 9000$ matrix
%which originates from modelling the vibrational modes of a polyethylene molecule
%with 3000 atoms \cite{lin2016review},
All these
matrices are symmetric. For all these matrices, we compute, for
fixed \gls{chebyshev-degree} $=2400$ and increasing \gls{sketch-size} $+$ \gls{num-hutchinson-queries},
the relative $L^1$ approximation error
of the spectral density for a Gaussian \glsfirst{smoothing-kernel} with
\gls{smoothing-parameter} $=0.05$. The resulting plots are printed in 
\reffig{fig:5-experiments-multi-matrix-convergence}.\\ 

We observe that for the two matrices which have an evenly distributed spectrum
(\reffig{fig:5-experiments-multi-matrix-convergence-uniform}) or an approximately
evenly distributed spectrum (\reffig{fig:5-experiments-multi-matrix-convergence-ModES3D}),
the \gls{NC} method by itself can achieve a good approximation once \gls{sketch-size}
exceeds the numerical rank of the matrix. On the other hand, for matrices where
the spectrum is very concentrated around a certain point (\reffig{fig:5-experiments-multi-matrix-convergence-Erdos})
or approximately describes a semi-circle (\reffig{fig:5-experiments-multi-matrix-convergence-goe}) \cite{wigner1958distribution},
%or exhibits large gaps (\reffig{fig:5-experiments-multi-matrix-convergence-nd3k}),
the \gls{NC} is not as effective, and the correction part in the \gls{NCPP} makes
a significant difference.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_uniform.pgf}
        \caption{uniform}
        \label{fig:5-experiments-multi-matrix-convergence-uniform}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_goe.pgf}
        \caption{GOE}
        \label{fig:5-experiments-multi-matrix-convergence-goe}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_ModES3D_8.pgf}
        \caption{ModES3D\_8}
        \label{fig:5-experiments-multi-matrix-convergence-ModES3D}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \input{plots/multi_matrix_convergence_Erdos992.pgf}
        \caption{Erdos992}
        \label{fig:5-experiments-multi-matrix-convergence-Erdos}
    \end{subfigure}
    \caption{For increasing values of \gls{sketch-size} $+$ \gls{num-hutchinson-queries}
    but fixed \gls{chebyshev-degree} $=2400$ we plot the $L^1$ relative approximation error \refequ{equ:5-experiments-L1-error}
    for multiple different matrices. As usual, we use
    Gaussian smoothing with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:5-experiments-multi-matrix-convergence}
\end{figure}
