\chapter*{Notation}
\label{chp:0-notation}

We try to follow the notation which is used in contemporary literature in this field:

\begin{itemize}
    \item we exclusively work with objects over the real numbers $\mathbb{R}$ and non-zero integers $\mathbb{N}$;
    \item scalar are represented by lower case Greek or Latin letters ($s$, $\varepsilon$, \dots),
          vectors are additionally printed in bold ($\vct{v}$, $\vct{\mu}$, \dots),
          and matrices are additionally capitalized ($\mtx{A}$, $\mtx{\Omega}$, \dots);
    \item the components of a vector $\vct{v} \in \mathbb{R}^{n}$ are $v_i \in \mathbb{R}, i = 1, \dots, n$,
          and the elements of a matrix $\mtx{A} \in \mathbb{R}^{n \times n}$ are $a_{ij} \in \mathbb{R}, i, j = 1, \dots, n$;
    \item the identity matrix $\mtx{I}_n = \diag(1, \dots, 1) \in \mathbb{R}^{n \times n}$
          carries ones on its diagonal and zeros everywhere else. The zero matrix
          $\mtx{0}_{n \times m} \in \mathbb{R}^{n \times m}$ consists of only zero entries;
    \item the eigenvalues of a square matrix $\mtx{A} \in \mathbb{R}^{n \times n}$
          are all scalars $\lambda$ which, together with some non-zero vectors
          $\vct{v}$, satisfy the condition $\mtx{A} \vct{v} = \lambda \vct{v}$.
          We denote them as $\lambda_1 \geq \dots \geq \lambda_n$ or if we know
          them to be non-negative as $\sigma_1 \geq \dots \geq \sigma_n \geq 0$;
    \item the trace of a square matrix $\mtx{A} \in \mathbb{R}^{n \times n}$ is
          the sum of its diagonal elements $\Tr(\mtx{A}) = \sum_{i=1}^n a_{ii}$
          but also the sum of its eigenvalues $\Tr(\mtx{A}) = \sum_{i=1}^n \lambda_i$;
    \item the transpose of a matrix $\mtx{A}$ is denoted with $\mtx{A}^{\top}$.
          Symmetric matrices satisfy $\mtx{A}^{\top} = \mtx{A}$ and
          allow for a spectral decomposition $\mtx{A} = \mtx{U} \mtx{\Lambda} \mtx{U}^{\top}$
          with $\mtx{\Lambda} = \diag(\lambda_1, \dots, \lambda_n) \in \mathbb{R}^{n \times n}$ and 
          $\mtx{U} \in \mathbb{R}^{n \times n}$ such that $\mtx{U}^{\top} \mtx{U} = \mtx{I}_n$.
          You may assume every symbol $\mtx{A}$ you encounter in this thesis to
          represent a symmetric matrix;
    \item matrix norms are denoted with $\lVert \cdot \rVert _{\cdot}$.
          For symmetric matrices, the Schatten norm is
          $\lVert \mtx{A} \rVert _{(p)} = (\sum_{i=1}^n |\lambda_i|^p)^{1/p}$.
          We refer to the spectral norm as
          $\lVert \mtx{A} \rVert _2 = \lVert \mtx{A} \rVert _{(\infty)} = \lambda_1$, while the Frobenius norm
          is $\lVert \mtx{A} \rVert _F = \lVert \mtx{A} \rVert _{(2)} = (\sum_{i=1}^{n} \lambda_i^2)^{\frac{1}{2}}$;
    \item the pseudoinverse of a matrix is denoted with $\mtx{A}^{\dagger}$. It
          satisfies $\mtx{A}^{\dagger}=(\mtx{A}^{\top} \mtx{A})^{\dagger} \mtx{A}^{\top}$,
          and if $\mtx{A}$ has linearly independent columns, it acts as a left inverse
          in the sense that $\mtx{A}^{\dagger} \mtx{A} = \mtx{I}_n$ \cite{penrose1955pseudo};
    \item a matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ is said to be positive
          semi-definite if its eigenvalues are all non-negative. You may assume every
          symbol $\mtx{B}$ you encounter in this thesis to denote a positive
          semi-definite matrix;
    \item closed interval are written as $[a, b] \subset \mathbb{R}$ and open
          intervals as $(a, b) \subset \mathbb{R}$;
    \item the $L^p$-norm of a real-valued, measurable function $f$ on a
          domain $[a, b]$ is defined as
          $\lVert f \rVert _{p} = (\int_{a}^{b} |f(x)|^p \mathrm{d}x)^{1/p}$ with the Lebesgue integral;
    \item Euler's number is printed as $e$ and its corresponding natural logarithm as $\log$;
    \item Dirac's delta distribution is written $\delta$ \cite[chapter~15]{dirac1947quantum}
          and the Kronecker delta $\delta_{ij}$;
    \item the cosine trigonometric function is denoted with $\cos$ and its inverse is $\arccos$.
    \item the real part of a complex number $z$ is $\Re(z)$ while its imaginary part is $\Im(z)$.
          The complex unit is $\iota$;
    \item we use $\mathcal{O}$ to describe the asymptotic lower- and upper bounds
          on the complexity of a computation. For those more familiar with the
          Bachmann-Landau notation \cite[section~3.2]{cormen2009algorithms},
          please allow us -- for simplicity -- to use $\mathcal{O}$ as though it was $\Theta$;
    \item $c(n)$ denotes the computational complexity of a matrix-vector product;
    \item and finally, we call an operation \emph{prohibitively expensive}, if its
          complexity scales more than quadratically with the size of the problem.
\end{itemize}
