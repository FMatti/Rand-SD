\chapter{Randomized low-rank approximation}
\label{chp:3-nystrom}

We go back to \refequ{equ:1-introduction-spectral-density-as-trace}, and now
directly analyze the structure of the matrix
\begin{equation}
    g_{\sigma}(t\mtx{I} - \mtx{A}).
    \label{equ:3-nystrom-matrix-function}
\end{equation}
Intuitively, because the
Gaussian \glsfirst{smoothing-kernel} \refequ{equ:1-introduction-def-gaussian-kernel}
decays rather quickly to zero, the eigenvalues of $\mtx{A}$ which lie far away
from $t$ on the spectrum will nearly vanish under the matrix function \refequ{equ:1-introduction-spectral-density-as-trace}.
What results is a matrix which is \emph{numerically low-rank}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The numerical rank of a matrix}
\label{sec:3-nystrom-numerical-rank}

\todo{write proposition}

To account for the finite precision of floating point operations, we use the
notion of the \gls{numerical-rank} \cite[Definition~1.1]{noga2013rank}
of a matrix $\mtx{B} \in \mathbb{R}^{n \times n}$, which we define as
\begin{equation}
    r_{\varepsilon, \cdot}(\mtx{B}) = \min \{\rank(\mtx{C}): \mtx{C} \in \mathbb{R}^{n \times n}: \lVert \mtx{B} - \mtx{C} \rVert _{\cdot} \leq \varepsilon \}.
    \label{equ:3-nystrom-def-numerical-rank}
\end{equation}
A matrix is then said to be numerically low-rank, if $r_{\varepsilon, \cdot}(\mtx{B}) \ll n$.\\ 

For unitarily invariant norms $\lVert \cdot \rVert _{\cdot}$ and 
symmetric positive definite matrices $\mtx{B}$, we may use
\cite[Theorem~5]{mirsky1960truncation} to express \gls{numerical-rank} in terms
of the eigenvalues $\mu_1, \dots, \mu_n$ of $\mtx{B}$.
In particular, for the spectral norm $\lVert \cdot \rVert _2$ we have
\begin{equation}
    r_{\varepsilon, 2}(\mtx{B}) = \min \{1 \leq r \leq n: \mu_{r+1} \leq \varepsilon \},
    \label{equ:3-nystrom-numerical-rank-spectral-norm}
\end{equation}
while for the Frobenius norm $\lVert \cdot \rVert _F$ we obtain
\begin{equation}
    r_{\varepsilon, F}(\mtx{B}) = \min \{1 \leq r \leq n: \sum_{j=r+1}^n \mu_{j}^2 \leq \varepsilon^2 \}.
    \label{equ:3-nystrom-numerical-rank-frobenius-norm}
\end{equation}\\

The eigenvalues of $g_{\sigma}(t\mtx{I} - \mtx{A})$ are given by
\begin{equation}
    \mu_i(t) = g_{\sigma}(t - \lambda_{(i)}) = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{-\frac{(t - \lambda_{(i)})^2}{2 \sigma^2}}
    \label{equ:3-nystrom-kernel-function-eigenvalues}
\end{equation}
where $\lambda_{(i)}$ denote the eigenvalues of $\mtx{A}$ sorted by increasing
distance from $t$, i.e. such that $\mu_1(t) \geq \dots \geq \mu_n(t)$. Consequently,
we may upper bound the numerical rank of \refequ{equ:3-nystrom-matrix-function} as
\begin{equation}
    r_{\varepsilon, \cdot}(g_{\sigma}(t\mtx{I} - \mtx{A})) \leq \#\{1\leq i\leq n: |t - \mu_i(t)| \leq C_{\varepsilon, \cdot}(\sigma)\}
    \label{equ:3-nystrom-kernel-numerical-rank}
\end{equation}
where we define the distances
\begin{align}
    C_{\varepsilon, 2}(\sigma) = \sigma \sqrt{-2 \log(n \sqrt{2 \pi} \sigma \varepsilon)}, \label{equ:3-nystrom-kernel-numerical-rank-spectral-constant} \\
    C_{\varepsilon, F}(\sigma) = \sigma \sqrt{-2 \log(\sqrt{2 \pi} \sigma \varepsilon)}. \label{equ:3-nystrom-kernel-numerical-rank-frobenius-constant} 
\end{align}
For the spectral norm, \refequ{equ:3-nystrom-kernel-numerical-rank} even holds
as an equality.
The expression \refequ{equ:3-nystrom-kernel-numerical-rank} has a very
visual interpretation: The \glsfirst{numerical-rank} of a matrix is at most
equal to the number of eigenvalues which are closer to $t$ than $C_{\varepsilon, \cdot}(\sigma)$.
This is also illustrated in \reffig{fig:3-nystrom-numerical-rank-constant}.\\
\begin{figure}
    \centering
    \input{figures/numerical_rank.tex}
    \caption{The numerical rank.}
    \label{fig:3-nystrom-numerical-rank-constant}
\end{figure}

If we additionally assume the eigenvalues of the matrix $\mtx{A}$
to be evenly distributed within $[a, b]$, i.e. in any subinterval of fixed length in
$[a, b]$ we can expect to find roughly the same number of eigenvalues (see \reffig{fig:3-nystrom-evenly-distributed-spectrum}), then
we can estimate the numerical rank of $g_{\sigma}(t\mtx{I} - \mtx{A})$ as
\begin{equation}
    r_{\varepsilon, \cdot}(g_{\sigma}(t\mtx{I} - \mtx{A})) \lessapprox \frac{2 n}{b - a} C_{\varepsilon, \cdot}(\sigma).
    \label{equ:3-nystrom-kernel-numerical-rank-even-eigenvalues}
\end{equation}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.45\columnwidth}
        \input{figures/uneven_spectrum.tex}
        \caption{Unevenly distributed spectrum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\columnwidth}
        \input{figures/even_spectrum.tex}
        \caption{Evenly distributed spectrum}
    \end{subfigure}      
    \caption{Examples of an evenly and unevenly distributed spectrum.}
    \label{fig:3-nystrom-evenly-distributed-spectrum}
\end{figure}

In \reffig{fig:3-nystrom-singular-value-decay}, we numerically check the decay
of the eigenvalues for one of our example matrices which we use in the numerical
experiments (\refsec{sec:5-experiments-density-function}).
\begin{figure}[ht]
    \centering
    \input{figures/singular_value_decay.tex}
    \caption{Singular value decay. \todo{add spectrum to show why not lowrank}}
    \label{fig:3-nystrom-singular-value-decay}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Nystr\"om low-rank approximation}
\label{sec:3-nystrom-nystrom}

Sketching is a widely used technique for capturing the column space of a matrix
\cite{halko2011finding,woodruff2014sketching,lin2017randomized,tropp2017sketching,tropp2023randomized}.
Multiplying a matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ with a standard Gaussian
\gls{sketching-matrix} $ \in \mathbb{R}^{n \times n_v}$ with $n_v \ll n$, will
result in a significantly smaller matrix $\mtx{B}\mtx{\Omega}$ whose
columns roughly span the same range as the columns of $\mtx{B}$.
Closely following \cite[Section~2.1]{tropp2023randomized}, we illustrate the
reason why this approach works hereafter.\\
Suppose $\mtx{B}$ is symmetric and has a spectral decomposition
\begin{equation}
    \mtx{B}
        = \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} 
        = \sum_{i=1}^n \lambda_i \vct{u}_i \vct{u}_i^{\top}.
    \label{equ:3-nystrom-sketching-spectral-decomposition}
\end{equation}
If we left-multiply $\mtx{B}$ with a standard Gaussian random vector $\vct{\omega} \in \mathbb{R}^n$,
we obtain
\begin{equation}
    \mtx{B}\vct{\omega}
        = \sum_{i=1}^n \lambda_i \vct{u}_i (\vct{u}_i^{\top} \vct{\omega})
        = \sum_{i=1}^n (\lambda_i  z_i) \vct{u}_i,
        \label{equ:3-nystrom-sketching-random-multiplication}
\end{equation}
where $z_i$ again follows a standard Gaussian distribution, since $\vct{u}_i$ are
normalized. The component of $\mtx{B}\vct{\omega}$ in the direction of the
$j$-th eigenvector $\vct{u}_j$ is
\begin{equation}
    \vct{u}_j^{\top}\mtx{B}\vct{\omega}
        = \sum_{i=1}^n (\lambda_i  z_i) (\vct{u}_j^{\top} \vct{u}_i)
        = \lambda_j  z_j.
        \label{equ:3-nystrom-sketching-dominant-component}
\end{equation}
That is, $\mtx{B}\vct{\omega}$ is roughly aligned with the eigenvectors corresponding
to the largest eigenvalues: the dominant eigenvectors.\\

Based on the sketch of a matrix, we can compute a low-rank approximation:
the Nystr√∂m approximation \cite{gittens2013nystrom,lin2017randomized}
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger} (\mtx{B} \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom}
\end{equation}
In the case where we are only interested in the trace of this approximation,
we may use the cyclic property of the trace to rewrite
\begin{equation}
    \Tr(\widehat{\mtx{B}})
        = \Tr\left((\mtx{B} \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger} (\mtx{B} \mtx{\Omega})^{\top} \right)
        = \Tr\left((\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger} (\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega}) \right).
    \label{equ:3-nystrom-nystrom-trace}
\end{equation}
What results is a way of approximating \refequ{equ:1-introduction-spectral-density-as-trace}
using a low-rank approximation, which is the backbone of what is known
as the spectrum sweeping method \cite{lin2017randomized},
\begin{equation}
    \widetilde{\phi}_{\sigma}^m(t)
        = \Tr\left((\mtx{\Omega}^{\top} g_{\sigma}^m(t\mtx{I} - \mtx{A}) \mtx{\Omega})^{\dagger} (\mtx{\Omega}^{\top} (g_{\sigma}^m(t\mtx{I} - \mtx{A}))^2 \mtx{\Omega})\right).
    \label{equ:3-nystrom-spectral-density}
\end{equation}\\

With the interpolation framework introducted in \refsec{sec:2-chebyshev-interpolation},
we interpolate the \glsfirst{smoothing-kernel}. Instead of performing the extremely
expensive operation of squaring the matrix function in \refequ{equ:3-nystrom-spectral-density}
explicitly, \cite{lin2017randomized} suggests to instead interpolate the
squared \gls{smoothing-kernel}$^2$ in a consisten expansion with
\refequ{equ:2-chebyshev-chebyshev-expansion}:
\begin{equation}
    (g_{\sigma}^m(t\mtx{I} - \mtx{A}))^2 = \sum_{l=0}^{2m} \nu_l(t) T_l(A).
    \label{equ:3-nystrom-ESS-chebyshev-expansion}
\end{equation}
However, we have found that a separate expansion leads to the loss of the
square relation between the two expansions, i.e. the square of the
first expansion is not the same as the second expansion, and this inconsistency
decreases the numerical accuracy (see \reffig{fig:3-nystrom-interpolation-issue}).\\
\begin{figure}[ht]
    \centering
    \input{plots/interpolation_issue.pgf}
    \caption{Interpolation issue.}
    \label{fig:3-nystrom-interpolation-issue}
\end{figure}

In order to circumvent the above mentioned issue, we propose a way of consistently computing
an expansion for \gls{smoothing-kernel}$^2$ which is even faster and simpler:
The relation of the Chebyshev expansion to the \gls{DCT} shown in \refequ{equ:2-chebyshev-chebyshev-DCT}
can be exploited to design fast and exact multiplication algorithms between polynomials
of the form \refequ{equ:2-chebyshev-chebyshev-expansion} \cite[Proposition~3.1]{baszenski1997cosine}.
In particular, raising a Chebyshev expansion to an integer power can be achieved
very efficiently by chaining a \gls{DCT} with an inverse \gls{DCT}:
Suppose we have computed the coefficients $\vct{\mu} \in \mathbb{R}^{m+1}$
of a Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion}.
Then we may quickly compute the coefficients $\vct{\nu} \in \mathbb{R}^{km+1}$
of the same expansion raised to the power $k \geq 2$ by first zero-padding
$\widehat{\vct{\mu}} = [\vct{\mu}^{\top}, \vct{0}_{(k-1)m}^{\top}]^{\top} \in \mathbb{R}^{km+1}$
and subsequently computing
\begin{equation}
    \vct{\nu} = (km + 1)^{k - 1} \DCT^{-1}\left\{ \DCT\left\{\widehat{\vct{\mu}}^{k}\right\} \right\}
    \label{equ:3-nystrom-chebyshev-potentiate-coefficients}
\end{equation}
where the exponentiation of a vector is understood elementwise.
This approach is exactly equivalent to \cite[Algorithm~5]{lin2017randomized}, but
often orders of magnitude faster.\\

We call the corresponding algorithm the \glsfirst{NC} method, whose pseudocode
can be found in \refalg{alg:nystrom-chebyshev}.
\begin{algo}{Nystr\"om-Chebyshev method}{nystrom-chebyshev}
    Symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$, \glsfirst{chebyshev-degree}, \glsfirst{num-random-vectors},
    evaluation points $\{t_i\}_{i=1}^{n_t}$
    \input{algorithms/nystrom_chebyshev.tex}
\end{algo}

Again denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and $\mathcal{O}(c(n)) = n$
for sparse matrices, we find the computational complexity of the \gls{NC}
method to be $\mathcal{O}(m \log(m) n_t + m n_v^2 n + m n_t n_v^2 +  m c(n) n_v + n_t n_v^3)$, with
$\mathcal{O}(n n_v + n_v^2 n_t + m n_t)$ required additional storage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Theoretical analysis}
\label{sec:3-nystrom-theoretical-analysis}

\todo{Error guarantee from \cite[Theorem~9]{kressner2023randomized}, combine with Chebyshev.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation details}
\label{sec:3-nystrom-implementation-details}

$\Xi(t)$ permutation of eigenvalues of $g_{\sigma}(tI - A)$
Filtering $[0, 1 / (n \sqrt{2 \pi \sigma^2})]$ \cite{lin2017randomized}
Adding some tolerance to avoid filtering

Not solving the generalized eigenvalue problem if spectral density zero (rank matrix zero).

Computation of pseudo-inverse

- Generalized eigenvalue problem

- Eigenvalue filtering with tolerance adjustment
\begin{equation}
    (\mtx{\Omega}^{\top} (g_{\sigma}^m(t\mtx{I} - \mtx{A}))^2 \mtx{\Omega}) \mtx{C}(t) = (\mtx{\Omega}^{\top} g_{\sigma}^m(t\mtx{I} - \mtx{A}) \mtx{\Omega}) \mtx{C}(t) \mtx{\Xi}(t)
    \label{equ:3-nystrom-generalized-eigenproblem}
\end{equation}

Maybe proof of this GEP?

Short-circuit: Not a good idea to compute pseudoinverse if
both $\mtx{K}_1$ and $\mtx{K}_2$ are zero (noise divided by noise)
\begin{equation}
    \phi_{\sigma}^m(t_i) \approx \frac{1}{n_v} \Tr(\mtx{K}_1(t_i))
    \label{equ:3-nystrom-shortcircuit-hutchinson}
\end{equation}
If smaller than a threshold $\delta = 10^{-5}$

\begin{figure}[ht]
    \centering
    \input{plots/short_circuit_mechanism.pgf}
    \caption{Short-circuit mechanism.}
    \label{fig:3-nystrom-short-circuit-mechanism}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extension to other low-rank approximations}
\label{sec:3-nystrom-other-low-rank}

Notice that many other methods \cite{halko2011finding,tropp2023randomized} fit
into the scheme of \refequ{equ:3-nystrom-nystrom-trace}. In fact, if we
generalize
\begin{equation}
    \Tr^{k}(\widehat{\mtx{B}})
        = \Tr\left((\mtx{\Omega}^{\top} \mtx{B}^k \mtx{\Omega})^{\dagger} (\mtx{\Omega}^{\top} \mtx{B}^{k+1} \mtx{\Omega}) \right),
    \label{equ:3-nystrom-trace-generalization}
\end{equation}
the underlying low-rank approximation for $k=1$ is the Nystr\"om approximation
which we have discussed in \refsec{sec:3-nystrom-nystrom}.
For $k=2$, it is the randomized low-rank approximation \cite{halko2011finding, tropp2023randomized}
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) ((\mtx{B} \mtx{\Omega})^{\top} (\mtx{B} \mtx{\Omega}))^{\dagger} (\mtx{B} \mtx{\Omega})^{\top} \mtx{B},
    \label{equ:3-nystrom-RSVD}
\end{equation}
for $k=3$ the Nystr\"om approximation with one subspace iteration \cite{tropp2023randomized}
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B}^2 \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B}^3 \mtx{\Omega})^{\dagger} (\mtx{B}^2 \mtx{\Omega})^{\top},
    \label{equ:3-nystrom-nystrom-SI}
\end{equation}
and so on.\\

Generalizing \refalg{alg:nystrom-chebyshev} is straight forward and
due to the efficient exponentiation of Chebyshev polynomials using the \gls{DCT}
(see \refequ{equ:3-nystrom-chebyshev-potentiate-coefficients}), of the same
computational and storage complexity.
