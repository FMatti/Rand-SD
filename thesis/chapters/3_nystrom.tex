\chapter{Randomized low-rank factorization}
\label{chp:3-nystrom}

We again start at \refequ{equ:1-introduction-spectral-density-as-trace}, but now
directly analyze the structure of the matrix function
\begin{equation}
    g_{\sigma}(t\mtx{I}_n - \mtx{A}).
    \label{equ:3-nystrom-matrix-function}
\end{equation}
Suppose $\mtx{A}$ is symmetric and has a spectral decomposition
\begin{equation}
    \mtx{A}
        = \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} 
        = \sum_{i=1}^n \lambda_i \vct{u}_i \vct{u}_i^{\top}.
    \label{equ:3-nystrom-matrix-spectral-decomposition}
\end{equation}
Applying the matrix function to this expression, we see that
\begin{equation}
    g_{\sigma}(t\mtx{I}_n - \mtx{A})
        = \sum_{i=1}^n g_{\sigma}(t - \lambda_i) \vct{u}_i \vct{u}_i^{\top}.
    \label{equ:3-nystrom-matrix-function-spectral-decomposition}
\end{equation}
The \glsfirst{smoothing-kernel} typically decays
rapidly to zero when its argument deviates from zero, particularly when
\gls{smoothing-parameter} is small. This can, for example, be observed when using Gaussian
or Lorentzian smoothing (see \reffig{fig:5-experiments-haydock-kernel}).
Therefore, all terms in the sum \refequ{equ:3-nystrom-matrix-function-spectral-decomposition}
are suppressed except the ones for which \gls{eigenvalue} is close to \gls{spectral-parameter}.
Thus,
\begin{equation}
    g_{\sigma}(t\mtx{I}_n - \mtx{A})
        \approx \sum_{i: |t - \lambda_i|/\sigma~\text{small}} g_{\sigma}(t - \lambda_i) \vct{u}_i \vct{u}_i^{\top}
    \label{equ:3-nystrom-matrix-function-low-rank-approximation}
\end{equation}
usually exhibits an approximate low-rank structure for all $t$, unless of course \gls{smoothing-parameter} is chosen exceedingly big or all eigenvalues
are clustered. The matrix function can approximately 
be represented by a product of two or more matrices which are significantly smaller.\\

The aim of this chapter is to find a good approximate factorization of \refequ{equ:3-nystrom-matrix-function}
in terms of smaller matrices, which we can efficiently compute and for which we can
easily determine the trace \refequ{equ:1-introduction-spectral-density-as-trace}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The numerical rank of a matrix}
\label{sec:3-nystrom-numerical-rank}

To quantify by how much a matrix could potentially be compressed when being accurately
represented as the product of smaller matrices, we introduce the rank of a
matrix \cite[section~III.3]{hefferon2012linear}.
The rank of a matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ is defined as
\begin{equation}
    \rank(\mtx{B}) = \dimension(\range(\mtx{B})),
    \label{equ:3-nystrom-rank}
\end{equation}
where $\dimension$ is the dimension and $\range$ denotes the set of all elements
which can be attained by right-multiplying $\mtx{B}$ with a vector from $\mathbb{R}^n$.\\

To account for the finite precision of floating point operations
and for matrices which are almost low-rank matrices, we use the
notion of the \gls{numerical-rank} \cite[definition~1.1]{noga2013rank}.
\begin{definition}{Numerical rank of a matrix}{3-nystrom-numerical-rank}
    The \glsfirst{numerical-rank} of a matrix $\mtx{B} \in \mathbb{R}^{n \times n}$
    in the norm $\lVert \cdot \rVert _{\cdot}$ is the number
    \begin{equation}
        r_{\varepsilon, \cdot}(\mtx{B}) = \min \{\rank(\mtx{C}): \mtx{C} \in \mathbb{R}^{n \times n}: \lVert \mtx{B} - \mtx{C} \rVert _{\cdot} \leq \varepsilon \}.
        \label{equ:3-nystrom-def-numerical-rank}
    \end{equation}
    for a small tolerance $\varepsilon \geq 0$.
\end{definition}%
A matrix is said to be numerically low-rank, if $r_{\varepsilon, \cdot}(\mtx{B}) \ll n$.
$\varepsilon$ is usually taken to be the double machine precision, i.e. $10^{-16}$.\\

For unitarily invariant norms $\lVert \cdot \rVert _{\cdot}$ and 
symmetric \gls{PSD} matrices $\mtx{B}$, we may use
\cite[theorem~5]{mirsky1960truncation} to express \gls{numerical-rank} in terms
of its eigenvalues $\sigma_1 \geq \dots \geq \sigma_n \geq 0$.
In particular, for the spectral norm $\lVert \cdot \rVert _2$ we have
\begin{equation}
    r_{\varepsilon, 2}(\mtx{B}) = \min \{1 \leq r \leq n: \sigma_{r+1} \leq \varepsilon \},
    \label{equ:3-nystrom-numerical-rank-spectral-norm}
\end{equation}
for the nuclear norm $\lVert \cdot \rVert _{\ast}$
\begin{equation}
    r_{\varepsilon, \ast}(\mtx{B}) = \min \{1 \leq r \leq n: \sum_{j=r+1}^n \sigma_{j} \leq \varepsilon \},
    \label{equ:3-nystrom-numerical-rank-nuclear-norm}
\end{equation}
and for the Frobenius norm $\lVert \cdot \rVert _F$
\begin{equation}
    r_{\varepsilon, F}(\mtx{B}) = \min \{1 \leq r \leq n: \sqrt{\sum_{j=r+1}^n \sigma_{j}^2} \leq \varepsilon \}.
    \label{equ:3-nystrom-numerical-rank-frobenius-norm}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Low-rank factorization}
\label{sec:3-nystrom-nystrom}

We will now give an introduction to low-rank factorizations of
constant matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Constant matrices}
\label{subsec:3-nystrom-factorization-constant-matrices}

From \refsec{sec:3-nystrom-numerical-rank}, it is clear that the rank of a symmetric
matrix is also equal to the number of its eigenvalues which are non-zero.
Hence, for a symmetric \gls{PSD} matrix $\mtx{B} \in \mathbb{R}^{n \times n}$
of rank $r \ll n$, we can immediately obtain a factorization of the matrix in
terms of smaller matrices by looking at its spectral decomposition
\begin{equation}
    \mtx{B}
        = \mtx{V} \mtx{\Sigma} \mtx{V}^{\top} 
        = \begin{bmatrix} \mtx{V}_1 & \mtx{V}_2 \end{bmatrix} 
          \begin{bmatrix} \mtx{\Sigma}_1 & \mtx{0} \\ \mtx{0} & \mtx{0} \end{bmatrix} 
          \begin{bmatrix} \mtx{V}_1^{\top} \\ \mtx{V}_2^{\top} \end{bmatrix}
        = \mtx{V}_1 \mtx{\Sigma}_1 \mtx{V}_1^{\top}
    \label{equ:3-nystrom-eigenvalue-decoposition}
\end{equation}
with $\mtx{\Sigma}_1 \in \mathbb{R}^{r \times r}$ containing all the non-zero
eigenvalues $\sigma_1, \dots, \sigma_r$ of $\mtx{B}$ and $\mtx{V}_1 \in \mathbb{R}^{n \times r}$ the corresponding
eigenvectors. What's more, we see that
\begin{equation}
    \mtx{V}_1 \mtx{V}_1^{\top} \mtx{B} = \mtx{B},
    \label{equ:3-nystrom-range-captured}
\end{equation}
i.e. the columns of the matrix $\mtx{V}_1$ fully capture the range of $\mtx{B}$.\\

However, computing the spectral
decomposition is often prohibitively expensive and usually not necessary for
obtaining good approximate factorizations of a matrix. There exist multiple
factorization methods for low-rank matrices, many of which are based on sketching.
Sketching is a widely used technique for approximating the column space of a matrix
\cite{halko2011finding,woodruff2014sketching,lin2017randomized,tropp2017sketching,tropp2023randomized}.
Multiplying a low-rank matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ with a standard Gaussian
\gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$ with $n_{\Omega} \ll n$, will
result in a significantly smaller matrix $\mtx{B}\mtx{\Omega}$ -- the sketch -- whose
columns roughly span the same range as the columns of $\mtx{B}$.
Closely following \cite[section~2.1]{tropp2023randomized}, we illustrate the
reason why this approach works hereafter.\\

Suppose $\mtx{B} \in \mathbb{R}^{n \times n}$ is symmetric \gls{PSD} and admits a spectral decomposition
\begin{equation}
    \mtx{B}
        = \mtx{V} \mtx{\Sigma} \mtx{V}^{\top} 
        = \sum_{i=1}^n \sigma_i \vct{v}_i \vct{v}_i^{\top}.
    \label{equ:3-nystrom-sketching-spectral-decomposition}
\end{equation}
If we left-multiply $\mtx{B}$ with a standard Gaussian random vector $\vct{\omega} \in \mathbb{R}^n$,
we obtain
\begin{equation}
    \mtx{B}\vct{\omega}
        = \sum_{i=1}^n \sigma_i \vct{v}_i (\vct{v}_i^{\top} \vct{\omega})
        = \sum_{i=1}^n (\sigma_i  z_i) \vct{v}_i,
        \label{equ:3-nystrom-sketching-random-multiplication}
\end{equation}
where $z_i = \vct{v}_i^{\top} \vct{\omega} \in \mathbb{R}$ again follows a
standard Gaussian distribution, since the $\vct{v}_i$ have unit norm
\cite{klenke2013probability}. The component of $\mtx{B}\vct{\omega}$ in the
direction of the $j$-th eigenvector $\vct{v}_j$ of $\mtx{B}$ is
\begin{equation}
    \vct{v}_j^{\top}\mtx{B}\vct{\omega}
        = \sum_{i=1}^n (\sigma_i  z_i) (\vct{v}_j^{\top} \vct{v}_i)
        = \sigma_j  z_j.
        \label{equ:3-nystrom-sketching-dominant-component}
\end{equation}
The larger the eigenvalue $\sigma_j$ of an eigenvector $\vct{v}_j$ is, the
more prominent the component of $\mtx{B}\vct{\omega}$ in that direction usually is.
That is, $\mtx{B}\vct{\omega}$ is roughly aligned with the eigenvectors corresponding
to the largest eigenvalues: the dominant eigenvectors.
When repeating this procedure for multiple $\vct{\omega}$ by right-multiplying $\mtx{B}$
with a standard Gaussian \gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$,
it can be shown that $\mtx{B} \mtx{\Omega}$ approximates the range of $\mtx{B}$ well,
and consequently
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) (\mtx{B} \mtx{\Omega})^{\dagger} \mtx{B}
    \label{equ:3-nystrom-RSVD}
\end{equation}
is a good approximation of $\mtx{B}$ \cite{halko2011finding}.\\

Based on the sketch $\mtx{B}\mtx{\Omega}$, we can also compute low-rank approximations
of the form
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) \mtx{K} (\mtx{B} \mtx{\Omega})^{\top},
    \label{equ:3-nystrom-nystrom-general}
\end{equation}
for some matrix $\mtx{K} \in \mathbb{R}^{n_{\Omega} \times n_{\Omega}}$ \cite[section~3.1]{lin2017randomized}.
Ideally, we would want $\widehat{\mtx{B}}$ to be as close to $\mtx{B}$ as possible. Therefore, by multiplying \refequ{equ:3-nystrom-nystrom-general}
from the left with $\mtx{\Omega}^{\top}$ and from the right with $\mtx{\Omega}$,
and by imposing $\widehat{\mtx{B}} = \mtx{B}$ we get
\begin{equation}
    \mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega} = (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega}) \mtx{K} (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom-unknown}
\end{equation}
By the properties of the pseudoinverse of a matrix we see that
$\mtx{K} = (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}$ satisfies this
relation.
This approximation is often referred to as the Nystr\"om approximation \cite{gittens2013nystrom}
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger} (\mtx{B} \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom}
\end{equation}
It is shown in \cite[lemma~5.2]{tropp2023randomized} that for \gls{PSD} matrices
the Nystr\"om approximation is at least as accurate as the approximation in
\refequ{equ:3-nystrom-RSVD} when measured in the spectral
and Frobenius norms. The following theorem gives an upper bound on the error of the
Nystr\"om approximation in the Frobenius norm \cite[lemma~3.2]{persson2022hutch}.

\begin{theorem}{Frobenius norm error of Nystr\"om approximation}{3-nystrom-nystrom-constant}
    Let $\mtx{B} \in \mathbb{R}^{n \times n}$ be a symmetric \gls{PSD} matrix. Its Nystr\"om approximation
    $\widehat{\mtx{B}}$ \refequ{equ:3-nystrom-nystrom} with standard Gaussian
    \gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$ of even
    \gls{sketch-size} $\geq 10$
    satisfies with probability $\geq 1-6e^{-n_{\Omega}/2}$
    \begin{equation}
        \lVert \mtx{B} - \widehat{\mtx{B}} \rVert _F \leq 542\sqrt{\frac{2}{n_{\Omega}}} \Tr(\mtx{B}).
    \end{equation}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Parameter-dependent matrices}
\label{subsec:3-nystrom-factorization-parameter-matrices}

The Nystr\"om low-rank factorizations can be extended in a straightforward way
to the case where the symmetric \gls{PSD} matrix $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ depends
continuously on a parameter $t \in [a, b]$:
\begin{equation}
    \widehat{\mtx{B}}(t) = (\mtx{B}(t) \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B}(t) \mtx{\Omega})^{\dagger} (\mtx{B}(t) \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom-parameter}
\end{equation}
What's special in this formulation is that we reuse the same \glsfirst{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$
for all $t$. This will introduce remarkable gains in terms of computational
complexity of the approximation.\\

In \cite{he2023parameter}, a corresponding result to \refthm{thm:3-nystrom-nystrom-constant}
for the parameter-dependent case is shown:
\begin{lemma}{$L^1$-error of parameter-dependent Nystr\"om approximation}{3-nystrom-parameter-nystrom}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be symmetric \gls{PSD} and
    continuous in $t \in [a, b]$. Then the parameter-dependent Nystr\"om approximation
    $\widehat{\mtx{B}}(t)$ \refequ{equ:3-nystrom-nystrom-parameter} with standard
    Gaussian \gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$,
    even $n_{\Omega} \geq 8 \log(1/\delta)$, and constant $c_{\Omega} \geq 0$,
    satisfies with probability $\geq 1 - \delta$
    \begin{equation}
        \int_{a}^{b} \lVert \mtx{B}(t) - \widehat{\mtx{B}}(t) \rVert _F \mathrm{d}t \leq c_{\Omega} \frac{1}{\sqrt{n_{\Omega}}} \int_{a}^{b} \Tr(\mtx{B}(t)) \mathrm{d}t.
    \end{equation}
\end{lemma}

For the trace of the parameter-dependent Nystr\"om approximation,
a much stronger convergence result can be shown for matrix functions of the form $f(\mtx{A}, t)$
which continuously depend on $t$ and whose eigenvalues decay quickly.
An expression for the trace estimation error in
the $L^1$-norm is shown in a theorem from \cite{he2023parameter}.
\begin{theorem}{$L^1$-trace-error of parameter-dependent Nystr\"om approximation}{3-nystrom-frobenius-norm}
    Let $f(\mtx{A}, t)$ be a non-negative function of $\mtx{A} \in \mathbb{R}^{n \times n}$ which
    also continuously depends on $t \in [a,b]$. 
    Denote with $\sigma_1(t) \geq \cdots \geq \sigma_n(t) \geq 0$ the eigenvalues of $f(\mtx{A}, t)$ at $t$. Let the
    standard Gaussian \glsfirst{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$
    used in \refequ{equ:3-nystrom-nystrom} have
    $n_{\Omega} = r + p$ columns for some numbers $r \geq 2, p \geq 4$. Then
    for all $\gamma \geq 1$, the Nystr\"om approximation $\widehat{f}(\mtx{A}, t)$ satisfies
    \begin{equation}
        \int_{a}^{b} | \Tr(f(\mtx{A}, t)) - \Tr(\widehat{f}(\mtx{A}, t))| \mathrm{d}t
            \leq \gamma^2 (1 + r) \int_{a}^{b} \sum_{i = r+1}^n \sigma_i(t) \mathrm{d}t
    \end{equation}
    with probability $\geq 1 - \gamma^{-p}$.
\end{theorem}
Note that the \glsfirst{smooth-spectral-density} \refequ{equ:1-introduction-spectral-density-as-trace}
is exactly a trace of such a matrix function, which means that we can directly
apply this theorem to it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Nystr\"om-Chebyshev method}
\label{sec:3-nystrom-nystrom-chebyshev}

Motivated by the observations from the beginning of this chapter, we now present
an algorithm for estimating the \glsfirst{smooth-spectral-density}.
Since all \gls{smoothing-kernel} we consider are non-negative, the matrix
function \refequ{equ:3-nystrom-matrix-function} is symmetric \gls{PSD}.
Therefore, we opt for the Nystr\"om approximation \refequ{equ:3-nystrom-nystrom-parameter}
as our randomized low-rank factorization engine. Before we go into the details
of the method, we need to quantify the \glsfirst{numerical-rank} of this matrix
function, in order to get an a priori guarantee for the convergence of the 
method.\\

When using a Gaussian \gls{smoothing-kernel}, we can explicitly write
the eigenvalues of $g_{\sigma}(t\mtx{I}_n - \mtx{A})$ as
\begin{equation}
    \sigma_i(t) = g_{\sigma}(t - \lambda_{(i)}) = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{-\frac{(t - \lambda_{(i)})^2}{2 \sigma^2}}
    \label{equ:3-nystrom-kernel-function-eigenvalues}
\end{equation}
where $\lambda_{(1)}, \dots, \lambda_{(n)}$ denote the
eigenvalues of $\mtx{A}$ sorted by increasing
distance from \gls{spectral-parameter}, such that $\sigma_1(t) \geq \dots \geq \sigma_n(t)$.
Consequently, by using the closed-form expression of the eigenvalues \refequ{equ:3-nystrom-kernel-function-eigenvalues}
and inserting it into
\refequ{equ:3-nystrom-numerical-rank-spectral-norm}, \refequ{equ:3-nystrom-numerical-rank-nuclear-norm}, and \refequ{equ:3-nystrom-numerical-rank-frobenius-norm},
we may upper bound the numerical rank of \refequ{equ:3-nystrom-matrix-function}
for Gaussian \gls{smoothing-kernel} as
\begin{equation}
    r_{\varepsilon, \cdot}(g_{\sigma}(t\mtx{I}_n - \mtx{A})) \leq \#\{1\leq i\leq n: |t - \lambda_i| < C_{\varepsilon, \cdot}(\sigma)\}
    \label{equ:3-nystrom-kernel-numerical-rank}
\end{equation}
with the eigenvalues $\lambda_1, \dots, \lambda_n$ of $\mtx{A}$ and the constants
\begin{align}
    C_{\varepsilon, 2}(\sigma) = \sigma \sqrt{-2 \log(\sqrt{2 \pi} n \sigma \varepsilon)} && \text{(spectral norm)} \label{equ:3-nystrom-kernel-numerical-rank-spectral-constant} \\
    C_{\varepsilon, \ast}(\sigma) = \sigma \sqrt{-2 \log(\sqrt{2 \pi n} \sigma \varepsilon)} && \text{(nuclear norm)} \label{equ:3-nystrom-kernel-numerical-rank-nuclear-constant} \\
    C_{\varepsilon, F}(\sigma) = \sigma \sqrt{-2 \log(\sqrt{2 \pi} \sigma \varepsilon)} && \text{(Frobenius norm)} \label{equ:3-nystrom-kernel-numerical-rank-frobenius-constant} 
\end{align}
For the spectral norm, \refequ{equ:3-nystrom-kernel-numerical-rank} even holds
as an equality.
A derivation of expression \refequ{equ:3-nystrom-kernel-numerical-rank}
and the corresponding constants can be found in \refapp{chp:B-appendix}.\\

The expression \refequ{equ:3-nystrom-kernel-numerical-rank} has a very
visual interpretation: The \glsfirst{numerical-rank} of a matrix is at most
equal to the number of eigenvalues which are closer to \gls{spectral-parameter}
than $C_{\varepsilon, \cdot}(\sigma)$.
This is also illustrated in \reffig{fig:3-nystrom-numerical-rank-constant}.\\
\begin{figure}[ht]
    \centering
    \input{figures/numerical_rank.tex}
    \caption{The numerical rank of $g_{\sigma}(t\mtx{I}_n - \mtx{A})$ can be
        approximately computed by counting the number of eigenvalues
        $\lambda_{(1)}, \dots, \lambda_{(n)}$ of the matrix $\mtx{A}$ which lie less than
        a constant $C_{\varepsilon, \cdot}(\sigma)$ away from \gls{spectral-parameter}.}
    \label{fig:3-nystrom-numerical-rank-constant}
\end{figure}

If we additionally assume the eigenvalues of the matrix $\mtx{A}$
to be evenly distributed within $[a, b]$, that is, in any subinterval of fixed length in
$[a, b]$ we can expect to find roughly the same number of eigenvalues (see \reffig{fig:3-nystrom-evenly-distributed-spectrum}), then
we can estimate the numerical rank of $g_{\sigma}(t\mtx{I}_n - \mtx{A})$
for Gaussian \gls{smoothing-kernel} to be
\begin{equation}
    r_{\varepsilon, \cdot}(g_{\sigma}(t\mtx{I}_n - \mtx{A})) \lessapprox \frac{2 n}{b - a} C_{\varepsilon, \cdot}(\sigma).
    \label{equ:3-nystrom-kernel-numerical-rank-even-eigenvalues}
\end{equation}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.45\columnwidth}
        \input{figures/uneven_spectrum.tex}
        \caption{Unevenly distributed spectrum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\columnwidth}
        \input{figures/even_spectrum.tex}
        \caption{Evenly distributed spectrum}
    \end{subfigure}
    \caption{Examples of an unevenly and an evenly distributed spectrum
    with some comments about the expected numerical rank of $g_{\sigma}(t\mtx{I}_n - \mtx{A})$
    at a certain values of \gls{spectral-parameter}.
    The dots represent eigenvalues.}
    \label{fig:3-nystrom-evenly-distributed-spectrum}
\end{figure}

In \reffig{fig:3-nystrom-singular-value-decay}, we numerically check the decay
of the eigenvalues for one of our example matrices which we use in the numerical
experiments (\refsec{sec:5-experiments-density-function}). Clearly, the estimated
numerical rank of the matrix is not accurate for some values of \gls{spectral-parameter}
in this case, since the spectrum is
not evenly distributed. Nevertheless, it provides us with a good first
guess which can be used to decide on what values for \gls{sketch-size} to choose
in order to ensure a high accuracy.\\
\begin{figure}[ht]
    \centering
    \input{figures/singular_value_decay.tex}
    \caption{Singular value decay of a Gaussian \glsfirst{smoothing-kernel}
       with \gls{smoothing-parameter} $=0.05$ applied to the matrix introduced
       \refsec{sec:5-experiments-density-function} for $c=1$. For reference,
       the effective spectral density is plotted above the color bar which
       assigns a color to the values of \gls{spectral-parameter}.}
    \label{fig:3-nystrom-singular-value-decay}
\end{figure}

The backbone of what is also known as the \glsfirst{SS} method \cite{lin2017randomized}
is the Nystr\"om approximation of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion},
hence why we refer to it as the \glsfirst{NC} method. 
For some standard Gaussian \gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$,
the approximation reads
\begin{equation}
    \widehat{g}_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})
    = (g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \mtx{\Omega}) (\mtx{\Omega}^{\top} g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \mtx{\Omega})^{\dagger} (g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom-smoothing-kernel}
\end{equation}
Since we are only interested in the trace of this approximation,
we may use the cyclic property of the trace to obtain
\begin{equation}
    \widehat{\phi}_{\sigma}^{(m)}(t)
        = \Tr\big((\underbrace{\mtx{\Omega}^{\top} g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \mtx{\Omega}}_{=\mtx{K}_1(t)})^{\dagger} (\underbrace{\mtx{\Omega}^{\top} (g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}))^2 \mtx{\Omega}}_{=\mtx{K}_2(t)})\big).
    \label{equ:3-nystrom-spectral-density}
\end{equation}\\

The interpolation framework introduced in \refsec{sec:2-chebyshev-interpolation},
allows us to interpolate the matrices $\mtx{K}_1(t) \in \mathbb{R}^{n_{\Omega} \times n_{\Omega}}$
and $\mtx{K}_2(t) \in \mathbb{R}^{n_{\Omega} \times n_{\Omega}}$,
which appear inside the trace, efficiently.
However, when interpolating the
squared \gls{smoothing-kernel} in a separate expansion
\begin{equation}
    (g_{\sigma}(t\mtx{I}_n - \mtx{A})^2)^{(m)} = \sum_{l=0}^{2m} \nu_l(t) T_l(A),
    \label{equ:3-nystrom-ESS-chebyshev-expansion}
\end{equation}
as is suggested in \cite{lin2017randomized}, it is no longer guaranteed that
the expansion of the squared matrix function $(g_{\sigma}(t\mtx{I}_n - \mtx{A})^2)^{(m)}$
is equal to the square of the expanded matrix function $g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})$.
We have observed that the loss of this square relationship
decreases the numerical accuracy noticeably, which is
shown in \reffig{fig:3-nystrom-interpolation-issue}.\\
\begin{figure}[ht]
    \centering
    \input{plots/interpolation_issue.pgf}
    \caption{Difference in the accuracy when computing \refequ{equ:3-nystrom-spectral-density}
        using a separate expansion (interpolation) versus explicitly squaring
        the matrix function (squaring). With the \gls{NC}, which we will introduce
        on the next few pages, the spectral density of a 2D model problem from
        \refsec{sec:5-experiments-density-function}
        is computed for a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$
        and fixed \gls{sketch-size} $=80$. An estimate of the relative error
        in the $L^1$-norm is computed based on \gls{num-evaluation-points} $=100$
        evaluation of the spectral density.}
    \label{fig:3-nystrom-interpolation-issue}
\end{figure}

In order to circumvent the above mentioned issue, we propose an alternative way of computing
an expansion for (\gls{smoothing-kernel})$^2$ which is consistent,
more accurate (see \reffig{fig:3-nystrom-interpolation-issue}) and significantly
faster (see \reftab{tab:3-nystrom-timing-squared-interpolation}):
The relation of the Chebyshev expansion to the \gls{DCT} shown in \refequ{equ:2-chebyshev-chebyshev-DCT}
can be exploited to design fast and exact multiplication algorithms between polynomials
of the form \refequ{equ:2-chebyshev-chebyshev-expansion} \cite[proposition~3.1]{baszenski1997cosine}.
In particular, raising a Chebyshev expansion to an integer power can be achieved
very efficiently by chaining a \gls{DCT} with an inverse \gls{DCT}:
Suppose we know the coefficients $\vct{\mu} \in \mathbb{R}^{m+1}$
of a Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion}.
Then we may quickly compute the coefficients $\vct{\nu} \in \mathbb{R}^{km+1}$
of the same expansion raised to the power $k \geq 2$ by first extending
$\vct{\mu}$ with zeros to $\widehat{\vct{\mu}} \in \mathbb{R}^{km+1}$
and subsequently computing
\begin{equation}
    \vct{\nu} = \DCT^{-1}\left\{ \DCT\left\{\widehat{\vct{\mu}}\right\}^{k} \right\}
    \label{equ:3-nystrom-chebyshev-potentiate-coefficients}
\end{equation}
where the exponentiation of a vector is understood element-wise.
The corresponding algorithm is presented hereafter.
\begin{algo}{Fast and exact exponentiation of Chebyshev expansions}{3-nystrom-chebyshev-exponentiation}
    \input{algorithms/chebyshev_exponentiation.tex}
\end{algo}
The complexity of this procedure is $\mathcal{O}(km \log(km))$ \cite{makhoul1980fct}.\\
\begin{table}[ht]
    \caption{Comparison of the runtime in milliseconds of three approaches with which the coefficients
    of the Chebyshev expansion of a function can be computed. We average over 7 runs of the
    algorithms and repeat these runs 100 times to form the mean and standard
    deviation which are given in the below table. We refer to the interpolation
    of (\gls{smoothing-kernel})$^{2}$ with \cite[algorithm~1]{lin2017randomized} as \enquote{quadrature},
    to the interpolation of (\gls{smoothing-kernel})$^{2}$ with \refalg{alg:2-chebyshev-chebyshev-expansion} as \enquote{DCT},
    and finally to the consistent squaring algorithm \refalg{alg:3-nystrom-chebyshev-exponentiation} as \enquote{squaring}.
    For each algorithm, we interpolate a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$,
    at \gls{num-evaluation-points} $=1000$ points, for various values of \gls{chebyshev-degree}.}
    \label{tab:3-nystrom-timing-squared-interpolation}
    \input{tables/timing_squared_interpolation.tex}
\end{table}

The algorithm gives us the consistent expansion
\begin{equation}
    (g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}))^2 = \sum_{l=0}^{2m} \nu_l(t) T_l(A).
    \label{equ:3-nystrom-consistent-chebyshev-expansion}
\end{equation}
This way of expanding the squared matrix function
is exactly equivalent to the \glsfirst{SS}
algorithm \cite[algorithm~5]{lin2017randomized}, but
usually orders of magnitude faster because the matrices involved in the
generalized eigenvalue problem
on line 13 of this algorithm do not have to be assembled from the
product of two large matrices for every \gls{spectral-parameter}.\\

Putting all things together, we get the \glsfirst{NC} method, whose pseudocode
can be found in \refalg{alg:3-nystrom-nystrom-chebyshev}.
\begin{algo}{Nystr\"om-Chebyshev method}{3-nystrom-nystrom-chebyshev}
    \input{algorithms/nystrom_chebyshev.tex}
\end{algo}

Again denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and $\mathcal{O}(c(n)) = n$
for sparse matrices, we find the computational complexity of the \gls{NC}
method to be $\mathcal{O}(m \log(m) n_t + m n_{\Omega}^2 n + m n_t n_{\Omega}^2 +  m c(n) n_{\Omega} + n_t n_{\Omega}^3)$, with
$\mathcal{O}(m n_t + n n_{\Omega} + n_{\Omega}^2 n_t)$ required additional storage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation details}
\label{subsec:3-nystrom-implementation-details}

In this section, we propose a sequence of algorithmic improvements to
\refalg{alg:3-nystrom-nystrom-chebyshev}, which make the said algorithm more
robust and accurate.

\paragraph{Non-zero checking}

In \refalg{alg:3-nystrom-nystrom-chebyshev}, we have that
\begin{equation}
    \mtx{K}_1(t_i) = \mtx{\Omega}^{\top} g_{\sigma}^{(m)}(t_i\mtx{I}_n - \mtx{A}) \mtx{\Omega}.
    \label{equ:3-nystrom-interpolated-matrix}
\end{equation}
Hence, if $g_{\sigma}^{(m)}(t_i\mtx{I}_n - \mtx{A})$ is close to the zero matrix,
which we have seen in \refsec{sec:3-nystrom-nystrom-chebyshev} to happen when
$t_i$ is far away from any of the eigenvalues of $\mtx{A}$, $\mtx{K}_1(t_i)$ will
also be close to the zero matrix. In this case, it may not be a good idea to
compute the pseudoinverse of $\mtx{K}_1(t_i)$ in \reflin{lin:3-nystrom-pseudo-inverse}
of \refalg{alg:3-nystrom-nystrom-chebyshev}. Even less, since in that case we already know that
\begin{equation}
    \phi_{\sigma}^{(m)}(t_i) = \Tr(g_{\sigma}^{(m)}(t_i\mtx{I}_n - \mtx{A})) \approx \Tr(\mtx{0}) = 0.
    \label{equ:3-nystrom-short-circuit}
\end{equation}
Motivated by this observation we use a \enquote{non-zero} check which 
computes the \gls{sketch-size}-query Girard-Hutchinson estimate \refequ{equ:2-chebyshev-DGC-hutchionson-estimator}
\begin{equation}
    \Hutch_{n_{\Omega}}(g_{\sigma}^{(m)}(t_i \mtx{I}_n - \mtx{A})) = \frac{1}{n_{\Omega}} \Tr(\mtx{K}_1(t_i))
    \label{equ:3-nystrom-short-circuit-hutchinson}
\end{equation}
of $\Tr(g_{\sigma}^{(m)}(t_i\mtx{I}_n - \mtx{A}))$
before executing \reflin{lin:3-nystrom-pseudo-inverse} in \refalg{alg:3-nystrom-nystrom-chebyshev}.
If the result is smaller than a fixed \gls{short-circuit-threshold} $> 0$, it immediately sets
$\widetilde{\phi}_{\sigma}^{(m)}(t_i)=0$ and skips \reflin{lin:3-nystrom-pseudo-inverse}
for this $t_i$. In order to not accidentally remove parts of the spectrum with eigenvalues,
\gls{short-circuit-threshold} should not exceed $1/(n \sqrt{2\pi \sigma^2})$.
We usually use \gls{short-circuit-threshold} $= 10^{-5}$. Since $\mtx{K}_1(t_i)$
needs to be computed in any case in \refalg{alg:3-nystrom-nystrom-chebyshev},
this check can be incorporated with minimal additional computational cost. The effect this non-zero check
mechanism has on the approximation quality can be seen in \reffig{fig:3-nystrom-short-circuit-mechanism}.\\

\begin{figure}[ht]
    \centering
    \input{plots/short_circuit_mechanism.pgf}
    \caption{The difference the non-zero check can make when approximating
        a spectral density using \refalg{alg:3-nystrom-nystrom-chebyshev}. 
        Here, we used the matrix described in \refsec{sec:5-experiments-density-function}
        and ran the \gls{NC} method with and without \glsfirst{short-circuit-threshold} $= 10^{-5}$,
        \gls{chebyshev-degree} $=2000$, \gls{sketch-size} $=80$, and a
        Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:3-nystrom-short-circuit-mechanism}
\end{figure}

\paragraph{Pseudoinverse via eigenvalue problem}

There exists an alternative way of computing the result of \reflin{lin:3-nystrom-pseudo-inverse}
in \refalg{alg:3-nystrom-nystrom-chebyshev}, namely traces of the form
\begin{equation}
    \Tr((\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}(\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega})).
    \label{equ:3-nystrom-trace-pseudo-inverse}
\end{equation}

Rather than explicitly forming the pseudoinverse,
it converts the problem of computing such a trace into solving a generalized
eigenvalue problem by making use of the following theorem \cite[theorem~3]{lin2017randomized}.

\begin{theorem}{Generalized eigenvalue problem to compute pseudoinverse}{3-nystrom-eigenvalue-problem}
    Let $\mtx{B} \in \mathbb{R}^{n \times n}$ be symmetric \gls{PSD} with rank $r < n$ and
    spectral decomposition \refequ{equ:3-nystrom-eigenvalue-decoposition}
    \begin{equation}
        \mtx{B}
        = \begin{bmatrix} \mtx{V}_1 & \mtx{V}_2 \end{bmatrix} 
          \begin{bmatrix} \mtx{\Sigma}_1 & \mtx{0} \\ \mtx{0} & \mtx{0} \end{bmatrix} 
          \begin{bmatrix} \mtx{V}_1^{\top} \\ \mtx{V}_2^{\top} \end{bmatrix}
        = \mtx{V}_1 \mtx{\Sigma}_1 \mtx{V}_1^{\top},
        \label{equ:3-nystrom-eigenvalue-problem-spectral-decomposition}
    \end{equation}
    where $\mtx{\Sigma}_1 \in \mathbb{R}^{r \times r}$ and
    $\mtx{V}_1 \in \mathbb{R}^{n \times r}$ with $\mtx{V}_1^{\top} \mtx{V}_1 = \mtx{I}_r$.
    For $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\Omega}}, n_{\Omega} > r$,
    such that $\mtx{\Omega}^{\top} \mtx{V}_1$ has linearly independent columns,
    the solution of the generalized eigenvalue problem
    \begin{equation}
        (\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega}) \mtx{C} = (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega}) \mtx{C}  \mtx{\Xi}
        \label{equ:3-nystrom-low-rank-eigenvalue-problem}
    \end{equation}
    with $\mtx{C} \in \mathbb{R}^{n_{\Omega} \times r}$ and $\mtx{\Xi} \in \mathbb{R}^{r \times r}$ a 
    non-zero diagonal matrix is $\mtx{C} = (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger} \mtx{\Theta}$
    and $\mtx{\Xi} = \mtx{\Theta}^{\top} \mtx{\Sigma}_1 \mtx{\Theta}$
    for a permutation matrix $\mtx{\Theta} \in \mathbb{R}^{r \times r}$.

    Furthermore,
    \begin{equation}
        \Tr((\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}(\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega})) = \Tr(\mtx{\Xi}).
        \label{equ:3-nystrom-low-rank-trace-equalities}
    \end{equation}
\end{theorem}
A proof of this theorem can be found in \cite[theorem~3]{lin2017randomized}. We
choose to still include our own version which goes slightly more into detail.
\begin{proof}
    Since $\mtx{\Omega}^{\top} \mtx{V}_1$ has linearly independent columns, its
    pseudoinverse is its left inverse, meaning 
    $(\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1) = ( \mtx{V}_1^{\top}\mtx{\Omega}) ( \mtx{V}_1^{\top}\mtx{\Omega})^{\dagger} = \mtx{I}_r$.
    We use $\mtx{B} = \mtx{V}_1 \mtx{\Sigma}_1 \mtx{V}_1^{\top}$ and insert
    $\mtx{C} = (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger} \mtx{\Theta}^{\top}$
    and $\mtx{\Xi} = \mtx{\Theta} \mtx{\Sigma}_1 \mtx{\Theta}^{\top}$ into the
    left-hand side of \refequ{equ:3-nystrom-low-rank-eigenvalue-problem} to get
    \begin{equation}
        (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 \underbrace{(\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}}_{=\mtx{I}_r} \mtx{\Theta} = (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 \mtx{\Theta},
    \end{equation}
    and on the right-hand side of \refequ{equ:3-nystrom-low-rank-eigenvalue-problem} for
    \begin{equation}
        (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1 \underbrace{(\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}}_{=\mtx{I}_r} \underbrace{\mtx{\Theta} \mtx{\Theta}^{\top}}_{=\mtx{I}_r} \mtx{\Sigma}_1 \mtx{\Theta} = (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 \mtx{\Theta}.
    \end{equation}
    Now that $\mtx{\Omega}^{\top} \mtx{V}_1$ has linearly independent columns, $\mtx{\Sigma}_1$ is a non-zero
    diagonal matrix, and $\mtx{\Theta}$ is a permutation, we conclude that
    the given $\mtx{C}$ and $\mtx{\Xi}$ indeed solve the generalized eigenvalue problem.
    By the uniqueness of generalized eigenvalues and eigenvectors (up to
    permutation and scaling), these are indeed the only solutions.

    \refequ{equ:3-nystrom-low-rank-trace-equalities} can be shown by inserting the truncated
    spectral decomposition \refequ{equ:3-nystrom-eigenvalue-problem-spectral-decomposition}
    and using the properties of the pseudoinverse
    \begin{align*}
        &\Tr((\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}(\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega})) \notag \\
            &= \Tr((\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger} \mtx{\Sigma}_1^{-1} (\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 (\mtx{V}_1^{\top} \mtx{\Omega})) &&\text{(spectral decomposition of $\mtx{B}$)} \notag \\
            &= \Tr(\mtx{\Sigma}_1^{-1} (\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1)\mtx{\Sigma}_1^2 (\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}) &&\text{(cyclic property of trace)} \notag \\
            &= \Tr(\mtx{\Sigma}_1^{-1} \underbrace{(\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1)}_{=\mtx{I}_r} \mtx{\Sigma}_1^2 \underbrace{(\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}}_{=\mtx{I}_r}) &&\text{($\mtx{\Omega}^{\top} \mtx{V}_1$ independent columns)} \notag \\
            &= \Tr(\mtx{\Sigma}_1) &&\text{($\mtx{\Sigma}_1^{-1} \mtx{\Sigma}_1^{2} = \mtx{\Sigma}_1$)}  \notag \\
            &= \Tr(\mtx{\Xi}) &&\text{($\mtx{\Sigma}_1 = \mtx{\Xi}$ up to permutation)}
    \end{align*}
\end{proof}

A standard way of computing the generalized eigenvalue problem \refequ{equ:3-nystrom-low-rank-eigenvalue-problem}
starts with taking a spectral decomposition of the right-hand side
\begin{equation}
    \mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega}
    = \mtx{W} \mtx{\Gamma} \mtx{W}^{\top}
    = \begin{bmatrix} \mtx{W}_1 & \mtx{W}_2 \end{bmatrix} 
    \begin{bmatrix} \mtx{\Gamma}_1 & \mtx{0} \\ \mtx{0} & \mtx{\Gamma}_2 \ \end{bmatrix} 
    \begin{bmatrix} \mtx{W}_1^{\top} \\ \mtx{W}_2^{\top} \end{bmatrix}
\end{equation}
where $\mtx{\Gamma}_1$ only contains those eigenvalues $\gamma_1, \dots, \gamma_{n_{\Omega}}$
which satisfy $\gamma_i \geq \zeta \max_{j=1,\dots,n_{\Omega}} \gamma_j$ 
for some \gls{threshold-factor} $>0$ and $\mtx{W}_1$ the corresponding
eigenvectors. In our experiments, we choose \gls{threshold-factor} $=10^{-7}$.
It allows us to approximate \refequ{equ:3-nystrom-low-rank-eigenvalue-problem}
with the more stable standard eigenvalue problem
\begin{equation}
    \mtx{\Gamma}_1^{-1/2} \mtx{W}_1^{\top} (\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega}) \mtx{W}_1 \mtx{\Gamma}_1^{-1/2} \mtx{X} = \mtx{X} \mtx{\Xi},
    \label{equ:3-nystrom-converted-generalized-eigenvalue-problem}
\end{equation}
which projects $\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega}$ onto the space spanned
by the dominant eigenvectors of
$\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega}$.\\

\paragraph{Filter tolerance}

Since by \refthm{thm:3-nystrom-eigenvalue-problem} the diagonal of $\mtx{\Xi}(t)$
should merely be a permutation of the eigenvalues of $g_{\sigma}(t\mtx{I}_n - \mtx{A})$,
we expect its elements to be within the range of \gls{smoothing-kernel}. Hence,
for Gaussian \gls{smoothing-kernel} we may remove
all elements in $\mtx{\Xi}(t)$ which are outside of $[0, 1 / (n \sqrt{2 \pi \sigma^2})]$,
with \glsfirst{matrix-size} of $\mtx{A}$ and \glsfirst{smoothing-parameter} of \gls{smoothing-kernel},
as suggested in \cite{lin2017randomized}.
In this way, we can filter out computational artefacts which for example might result
from an inaccurate Chebyshev expansion and furthermore 
can enforce non-negativity of the resulting approximation of
\gls{smooth-spectral-density}. However, one needs to be careful not to accidentally
remove a valid element from $\mtx{\Xi}(t)$. In the case where \gls{spectral-parameter}
coincides with an eigenvalue of $\mtx{A}$, one of the elements in $\mtx{\Xi}(t)$
should correctly assume the value $1 / (n \sqrt{2 \pi \sigma^2})$. Hence, if due to
certain numerical inaccuracies this value slightly exceeds the above threshold,
it will mistakenly get removed. This problematic case can be avoided by introducing a
\gls{filter-tolerance} $>0$, such that only elements in $\mtx{\Xi}(t)$ which
exceed $ (1 + \eta) / (n \sqrt{2 \pi \sigma^2})$ are filtered out. We usually
choose \gls{filter-tolerance} $=10^{-3}$. To ensure
non-negativity, the filter tolerance is only applied from above. \Reffig{fig:3-nystrom-filter-tolerance}
shows the improvement this filter tolerance gives over the standard approach.
Close to the edges of the spectrum, there are particularly many inappropriate
filterings happening, which are visible as downward-spikes of the spectral density.

\begin{figure}[ht]
    \centering
    \input{plots/filter_tolerance.pgf}
    \caption{The difference the filter tolerance can make when approximating
        a spectral density using \refalg{alg:3-nystrom-nystrom-chebyshev}. 
        Here, we used the matrix described in \refsec{sec:5-experiments-density-function}
        and ran the \gls{NC} method with and without filter tolerance.
        We use \glsfirst{filter-tolerance} $= 10^{-3}$,
        \gls{chebyshev-degree} $=2000$, \gls{sketch-size} $=80$, and a
        Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:3-nystrom-filter-tolerance}
\end{figure}

We can summarize this alternative way of treating
\reflin{lin:3-nystrom-pseudo-inverse} of \refalg{alg:3-nystrom-nystrom-chebyshev}
in the following algorithm.
\begin{algo}{Trace through generalized eigenvalue problem}{3-nystrom-eigenvalue-problem}
    \input{algorithms/eigenvalue_problem.tex}
\end{algo}

In the end, this procedure slightly improves the accuracy
and only for small \gls{chebyshev-degree}, i.e. when the Chebyshev expansion has
not converged yet. This is shown with an example in \reffig{fig:3-nystrom-eigenvalue-problem}.\\

\begin{figure}[ht]
    \centering
    \input{plots/eigenvalue_problem.pgf}
    \caption{The difference between directly computing the pseudoinverse
        in \reflin{lin:3-nystrom-pseudo-inverse} of \refalg{alg:2-chebyshev-DGC}
        versus solving the eigenvalue problem \refequ{equ:3-nystrom-converted-generalized-eigenvalue-problem}.
        Here, we used the matrix described in \refsec{sec:5-experiments-density-function}
        and ran the \gls{NC} method for fixed \gls{sketch-size} $=80$, and a
        Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$.}
    \label{fig:3-nystrom-eigenvalue-problem}
\end{figure}

\paragraph{Combination of all improvements}

In the end, we want to see the combined effect the above proposed algorithmic
improvements have. To do so, we observe the accuracy in terms of the $L^1$-error
of the approximated spectral density for changing \gls{chebyshev-degree}. Once
for the raw \refalg{alg:3-nystrom-nystrom-chebyshev}, and once for \refalg{alg:3-nystrom-nystrom-chebyshev}
with non-zero check, computation of the pseudoinverse through
an eigenvalue problem, and added filter tolerance.

\begin{figure}[ht]
    \centering
    \input{plots/algorithmic_improvements.pgf}
    \caption{The effect the algorithmic improvements have on the accuracy when
        computing the spectral density.
        Here, we used the matrix described in \refsec{sec:5-experiments-density-function}
        and ran the \gls{NC} method for a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$
        and fixed \gls{sketch-size} $=80$. We use
        \glsfirst{threshold-factor} $=10^{-7}$,
        \glsfirst{short-circuit-threshold} $=10^{-5}$, and
        \glsfirst{filter-tolerance} $=10^{-3}$.}
    \label{fig:3-nystrom-improved-algorithm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Theoretical analysis}
\label{subsec:3-nystrom-theoretical-analysis}

Despite \gls{smoothing-kernel} being a non-negative function, its Chebyshev
expansion $g_{\sigma}^{(m)}$ is not guaranteed to retain this property.
Therefore, it can happen that the conditions from \refthm{thm:3-nystrom-frobenius-norm}
are not verified any more.
However, due to \reflem{lem:2-chebyshev-error}, we know that for large
\gls{chebyshev-degree}, $g_{\sigma}^{(m)}$ will be very close to \gls{smoothing-kernel}.
So if we instead interpolate the shifted kernel
\begin{equation}
    \underline{g}_{\sigma}(s) = g_{\sigma}(s) + \rho,
    \label{equ:3-nystrom-up-shifted-kernel}
\end{equation}
we may guarantee that for a large enough \gls{shift} $\geq 0$, its Chebyshev expansion
$\underline{g}_{\sigma}^{(m)}$ will be non-negative. Furthermore, given
the shifted smooth spectral density $\underline{\phi}_{\sigma}$ of a matrix $\mtx{A} \in \mathbb{R}^{n \times n}$,
we may retrieve the \glsfirst{smooth-spectral-density} using the linearity of the trace
\footnote{For the \gls{NC} method, this correction will have to be adjusted to
$\widehat{\phi}_{\sigma}(t) = \underline{\phi}_{\sigma}(t) - \rho n_{\Omega}$
since in the algorithm we compute the trace of an \gls{sketch-size} $\times$ \gls{sketch-size}
matrix.}:
\begin{equation}
    \phi_{\sigma}(t)
    = \Tr(g_{\sigma}(t\mtx{I}_n - \mtx{A}))
    = \Tr(\underline{g}_{\sigma}(t\mtx{I}_n - \mtx{A}) - \rho \mtx{I}_n)
    = \underline{\phi}_{\sigma}(t) - \rho n.
    \label{equ:3-nystrom-revert-shift}
\end{equation}
For the shifted smooth spectral density $\underline{\phi}_{\sigma}$ we can prove the following result.
\begin{theorem}{$L^1$-error of Nystr\"om-Chebyshev method with shift}{3-nystrom-nystrom-method}
    Let $\widehat{\underline{\phi}}_{\sigma}^{(m)}$ be the result from running
    \refalg{alg:3-nystrom-nystrom-chebyshev} on a symmetric matrix
    $\mtx{A} \in \mathbb{R}^{n \times n}$ with its spectrum contained in $[-1, 1]$
    using a shifted Gaussian smoothing kernel $\underline{g}_{\sigma} = g_{\sigma} + \rho$
    with \glsfirst{smoothing-parameter} $>0$, \glsfirst{chebyshev-degree} $\in \mathbb{N}$, and
    \gls{sketch-size} $=r + p$ for some numbers $r \geq 2$, $p \geq 4$.
    If \gls{shift} $\geq \frac{\sqrt{2}}{n \sigma^2}  (1 + \sigma)^{-m}$
    and $r \geq r_{\varepsilon, \ast}(g_{\sigma}(t\mtx{I}_n - \mtx{A}))$ for all
    $t \in [-1, 1]$, then for all $\gamma \geq 1$, the inequality
    \begin{equation}
        \lVert \underline{\phi}_{\sigma} - \widehat{\underline{\phi}}_{\sigma}^{(m)} \rVert _1
        \leq 2 \gamma^2(1 + r) (2 \varepsilon + 4 \rho (n-r))
        + \frac{2\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m}
    \end{equation}
    holds with probability $\geq 1-\gamma^{-p}$.
\end{theorem}

\begin{proof}
    We assume the conditions of the theorem are satisfied. Then by the triangle inequality
    \begin{equation}
        \lVert \underline{\phi}_{\sigma} - \widehat{\underline{\phi}}_{\sigma}^{(m)}  \rVert _1
        \leq \lVert \underline{\phi}_{\sigma} - \underline{\phi}_{\sigma}^{(m)} \rVert _1
        + \lVert \underline{\phi}_{\sigma}^{(m)} - \widehat{\underline{\phi}}_{\sigma}^{(m)} \rVert _1.
        \label{equ:3-nystrom-nystrom-chebyshev-proof-triangle}
    \end{equation}
    Since the approximation error of the Chebyshev expansion remains invariant
    under constant shifts, we deduce with \reflem{lem:2-chebyshev-error} that the
    first term in \refequ{equ:3-nystrom-nystrom-chebyshev-proof-triangle} is bounded by
    \begin{equation}
        \lVert \underline{\phi}_{\sigma} - \underline{\phi}_{\sigma}^{(m)} \rVert _1
        = \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1
        \leq  \frac{2\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m}
    \end{equation}
    The second term can be bounded with \refthm{thm:3-nystrom-frobenius-norm}:
    \begin{equation}
        \lVert \widehat{\underline{\phi}}_{\sigma} - \underline{\phi}_{\sigma}^{(m)} \rVert _1
        \leq \gamma^2(1 + r) \int_{-1}^{1} \sum_{i=r+1}^{n} \underline{\sigma}_i^{(m)}(t) \mathrm{d}t
        \label{equ:3-nystrom-nystrom-chebyshev-proof-bound}
    \end{equation}
    where $\underline{\sigma}_1^{(m)}(t) \geq \dots \geq \underline{\sigma}_n^{(m)}(t) \geq 0$
    are the eigenvalues of $\underline{g}_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})$.
    From the explicit expression for these eigenvalues \refequ{equ:3-nystrom-kernel-function-eigenvalues}
    and the condition on \gls{shift} based on the approximation error in 
    \reflem{lem:2-chebyshev-error} we can deduce that for all \gls{spectral-parameter}
    \begin{equation}
        \underline{\sigma}_i^{(m)}(t) = \sigma^{(m)}_i(t) + \rho \leq \sigma_i(t) + 2\rho
        \label{equ:3-nystrom-nystrom-chebyshev-proof-shifted-eigenvalues}
    \end{equation}
    where $\sigma^{(m)}_i(t)$ stand for the eigenvalues of $g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})$.
    Consequently
    \begin{align*}
        \lVert \widehat{\underline{\phi}}_{\sigma} - \underline{\phi}_{\sigma}^{(m)} \rVert _1
        &\leq \gamma^2(1 + r) \int_{-1}^{1} \sum_{i=r+1}^{n} (\sigma_i(t) + 2\rho) \mathrm{d}t
        && \text{(\refequ{equ:3-nystrom-nystrom-chebyshev-proof-shifted-eigenvalues} into \refequ{equ:3-nystrom-nystrom-chebyshev-proof-bound})} \notag \\
        &\leq \gamma^2(1 + r) \left( \int_{-1}^{1} \sum_{i=r+1}^{n} \sigma_i(t) \mathrm{d}t + 4\rho(n-r) \right)
        && \text{(linearity)} \notag \\
        &\leq 2 \gamma^2(1 + r) \left( 2\varepsilon + 4\rho(n-r) \right)
        && \text{(choice of $r \geq r_{\varepsilon, \ast}$)}
    \end{align*}
    The result follows by inserting the two derived bounds into
    \refequ{equ:3-nystrom-nystrom-chebyshev-proof-triangle}.
\end{proof}

For matrices $\mtx{A} \in \mathbb{R}^{n \times n}$ whose spectra are approximately
evenly distributed, we know from \refsec{sec:3-nystrom-numerical-rank} that
\begin{equation}
    r_{\varepsilon, \ast}(g_{\sigma}(t\mtx{I}_n - \mtx{A})) \lessapprox n \sigma \sqrt{-2\log(\sqrt{2 \pi n} \sigma \varepsilon)},
\end{equation}
which gives a tractable expression for the choice of $r$ in \refthm{thm:3-nystrom-nystrom-method}.\\

As \gls{chebyshev-degree} gets smaller and smaller, the Chebyshev expansion gets
more and more accurate, and the \glsfirst{shift} can be chosen smaller and smaller.
However, the non-linearity of most low-rank factorizations make this estimator
non-linear, and disallow extending this error guarantee to
$\lVert \phi_{\sigma} - \widehat{\phi}_{\sigma}^{(m)} \rVert$.\\

From \reffig{fig:3-nystrom-shift} it is clear that adding the shift harms the
convergence of the approximation. Therefore, this shifting operation is only
useful to guarantee theoretical results, but in practice it seems like the
non-negativity of the \glsfirst{smoothing-kernel} does not have a crucial
impact on the performance of the Nystr\"om approximation of the matrix function
$g_{\sigma}(t\mtx{I}_n - \mtx{A})$.

\begin{figure}[ht]
    \centering
    \input{plots/kernel_shift.pgf}
    \caption{For \gls{chebyshev-degree} $=2400$, we observe the influence of a
        \glsfirst{shift} of the kernel on the convergence with respect to \gls{sketch-size}.
        Here, we used the matrix described in \refsec{sec:5-experiments-density-function}
        and ran the \gls{NC} method (including the algorithmic improvements
        from \refsec{subsec:3-nystrom-implementation-details}) for a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$
        and fixed \gls{sketch-size} $=80$. Once in the usual way and once with \gls{shift} $=10^{-7}$,
        which is more than sufficient for the shifted kernel $\underline{g}_{\sigma}$ to be non-negative.}
    \label{fig:3-nystrom-shift}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Extension to other low-rank approximations}
\label{subsec:3-nystrom-other-low-rank}

Notice that many other low-rank factorization methods \cite{halko2011finding,tropp2023randomized}
also fit into the fast interpolation scheme used to approximate \refequ{equ:3-nystrom-spectral-density}.
In fact, we have found that if we generalize
\begin{equation}
    \Tr^{k}(\widehat{\mtx{B}})
        = \Tr\left((\mtx{\Omega}^{\top} \mtx{B}^k \mtx{\Omega})^{\dagger} (\mtx{\Omega}^{\top} \mtx{B}^{k+1} \mtx{\Omega}) \right),
    \label{equ:3-nystrom-trace-generalization}
\end{equation}
for $k \in \mathbb{N}$,
the underlying low-rank approximation for $k=1$ is the Nystr\"om approximation
which we have discussed in \refsec{sec:3-nystrom-nystrom}.
For $k=2$, it corresponds to the approximation \refequ{equ:3-nystrom-RSVD}.
To see this, we rewrite it as
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) ((\mtx{B} \mtx{\Omega})^{\top} (\mtx{B} \mtx{\Omega}))^{\dagger} (\mtx{B} \mtx{\Omega})^{\top} \mtx{B},
    \label{equ:3-nystrom-RSVD-rewritten}
\end{equation}
using the definition of the pseudoinverse.
For $k=3$ we would obtain a scheme which theoretically coincides with the Nystr\"om
approximation with subspace iteration \cite{tropp2023randomized}
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B}^2 \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B}^3 \mtx{\Omega})^{\dagger} (\mtx{B}^2 \mtx{\Omega})^{\top},
    \label{equ:3-nystrom-nystrom-SI}
\end{equation}
and so on.\\

Generalizing \refalg{alg:3-nystrom-nystrom-chebyshev} is straight-forward and
due to the efficient exponentiation of Chebyshev polynomials using
\refalg{alg:3-nystrom-chebyshev-exponentiation}, remains of the same
computational and storage complexity, provided we consider $k$ as a small constant.\\

In practice, however, this extension may not immediately be useful: \cite[lemma~5.2]{tropp2023randomized}
shows that for \gls{PSD} matrices, approximations corresponding to the case $k=2$
can be expected not to be as accurate as approximations in the case $k=1$, and
while for $k=3$ the additional subspace iteration for the Nystr\"om approximation
should theoretically produce a better sketch, it suffers from other shortcomings,
such as a worse conditioning of the generalized eigenvalue problem (\refalg{alg:3-nystrom-eigenvalue-problem}).
A comparison of the accuracy of the extended methods for $k=1, 2, 3$ can be
found in \reffig{fig:3-nystrom-other-approximations}.

\begin{figure}[ht]
    \centering
    \input{plots/other_approximations.pgf}
    \caption{Difference in the accuracy when running the \gls{NC} method developed in this
    chapter for different types of low-rank factorizations.
    A 2D model problem from \refsec{sec:5-experiments-density-function}
    is used for a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$
    and we fix \gls{sketch-size} $=80$.}
    \label{fig:3-nystrom-other-approximations}
\end{figure}
