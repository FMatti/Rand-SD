\chapter{Randomized low-rank approximation}
\label{chp:3-nystrom}

We again start off with \refequ{equ:1-introduction-spectral-density-as-trace}, but now
directly analyze the structure of the matrix function
\begin{equation}
    g_{\sigma}(t\mtx{I} - \mtx{A}).
    \label{equ:3-nystrom-matrix-function}
\end{equation}
Suppose $\mtx{A}$ is symmetric and has the spectral decomposition
\begin{equation}
    \mtx{A}
        = \mtx{U} \mtx{\Lambda} \mtx{U}^{\top} 
        = \sum_{i=1}^n \lambda_i \vct{u}_i \vct{u}_i^{\top}.
    \label{equ:3-nystrom-matrix-spectral-decomposition}
\end{equation}
Appling the matrix function to this expression, we see that
\begin{equation}
    g_{\sigma}(t\mtx{I} - \mtx{A})
        = \sum_{i=1}^n g_{\sigma}(t - \lambda_i) \vct{u}_i \vct{u}_i^{\top}.
    \label{equ:3-nystrom-matrix-function-spectral-decomposition}
\end{equation}
The \glsfirst{smoothing-kernel} typically decays
rapidly to zero when its argument deviates from zero, particularly for
small \gls{smoothing-parameter}. This can for example be observed when using Gaussian
smoothing \refequ{equ:1-introduction-def-gaussian-kernel}.
Therefore, all terms in the sum \refequ{equ:3-nystrom-matrix-function-spectral-decomposition}
are suppressed except the ones for which \gls{eigenvalue} is close to \gls{spectral-parameter}.
Thus,
\begin{equation}
    g_{\sigma}(t\mtx{I} - \mtx{A})
        \approx \sum_{i: |t - \lambda_i|/\sigma~\text{small}} g_{\sigma}(t - \lambda_i) \vct{u}_i \vct{u}_i^{\top}
    \label{equ:3-nystrom-matrix-function-low-rank-approximation}
\end{equation}
exhibits an approximate low-rank structure, i.e. the matrix can approximately 
be represented by a product of two or more matrices which are significantly smaller.\\

The aim of this chapter is to find a good factorization of \refequ{equ:3-nystrom-matrix-function}
in terms of smaller matrices, which we can quickly compute and for which we can
easily determine the trace.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The numerical rank of a matrix}
\label{sec:3-nystrom-numerical-rank}

To quantify by how much a matrix could potentially be compressed by being accurately
represented as the product of smaller matrices, we introduce the rank of a
matrix \cite[section~III.3]{hefferon2012linear}.
\begin{definition}{Rank of a matrix}{3-nystrom-rank}
    The rank of a matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ is defined as
    \begin{equation}
        \rank(\mtx{B}) = \dimension(\range(\mtx{B})),
        \label{equ:3-nystrom-rank}
    \end{equation}
    where $\dimension$ is the dimension and $\range$ denotes the set of all elements
    which can be attained by right-multiplying $\mtx{B}$ with a vector from $\mathbb{R}^n$.
\end{definition}

To account for the finite precision of floating point operations, we use the
notion of the \gls{numerical-rank} \cite[definition~1.1]{noga2013rank}.
\begin{definition}{Numerical rank of a matrix}{3-nystrom-numerical-rank}
    The numerical rank of a matrix $\mtx{B} \in \mathbb{R}^{n \times n}$
    in the norm $\lVert \cdot \rVert _{\cdot}$ is the number
    \begin{equation}
        r_{\varepsilon, \cdot}(\mtx{B}) = \min \{\rank(\mtx{C}): \mtx{C} \in \mathbb{R}^{n \times n}: \lVert \mtx{B} - \mtx{C} \rVert _{\cdot} \leq \varepsilon \}.
        \label{equ:3-nystrom-def-numerical-rank}
    \end{equation}
\end{definition}%
A matrix is said to be numerically low-rank, if $r_{\varepsilon, \cdot}(\mtx{B}) \ll n$.\\ 

For unitarily invariant norms $\lVert \cdot \rVert _{\cdot}$ and 
symmetric matrices $\mtx{B}$, we may use
\cite[theorem~5]{mirsky1960truncation} to express \gls{numerical-rank} in terms
of its eigenvalues $\lambda_1(\mtx{B}), \dots, \lambda_n(\mtx{B})$.
In particular, for the spectral norm $\lVert \cdot \rVert _2$ we have
\begin{equation}
    r_{\varepsilon, 2}(\mtx{B}) = \min \{1 \leq r \leq n: \lambda_{r+1}(\mtx{B}) \leq \varepsilon \},
    \label{equ:3-nystrom-numerical-rank-spectral-norm}
\end{equation}
while for the Frobenius norm $\lVert \cdot \rVert _F$ we obtain
\begin{equation}
    r_{\varepsilon, F}(\mtx{B}) = \min \{1 \leq r \leq n: \sum_{j=r+1}^n \lambda_{j}(\mtx{B})^2 \leq \varepsilon^2 \}.
    \label{equ:3-nystrom-numerical-rank-frobenius-norm}
\end{equation}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Low-rank factorization}
\label{sec:3-nystrom-nystrom}

We will now give an introduction to low-rank factorizations of
constant matrices.

\subsection{Constant matrices}
\label{subsec:3-nystrom-factorization-constant-matrices}

From \refdef{def:3-nystrom-rank}, it is clear that the rank of a symmetric
matrix is also equal to the number of its eigenvalues which are non-zero.
Hence, for a symmetric \gls{PSD} matrix $\mtx{B} \in \mathbb{R}^{n \times n}$
of rank $r \ll n$, we can immediately obtain a factorization of the matrix in
terms of smaller matrices by looking at its spectral decomposition
\begin{equation}
    \mtx{B}
        = \mtx{V} \mtx{\Sigma} \mtx{V}^{\top} 
        = \begin{bmatrix} \mtx{V}_1 & \mtx{V}_2 \end{bmatrix} 
          \begin{bmatrix} \mtx{\Sigma}_1 & \mtx{0} \\ \mtx{0} & \mtx{0} \end{bmatrix} 
          \begin{bmatrix} \mtx{V}_1^{\top} \\ \mtx{V}_2^{\top} \end{bmatrix}
        = \mtx{V}_1 \mtx{\Sigma}_1 \mtx{V}_1^{\top}
    \label{equ:3-nystrom-eigenvalue-decoposition}
\end{equation}
with $\mtx{\Sigma}_1 \in \mathbb{R}^{r \times r}$ containing all the non-zero
eigenvalues and $\mtx{V}_1 \in \mathbb{R}^{n \times r}$ the corresponding
eigenvectors. What's more, we see that
\begin{equation}
    \mtx{V}_1 \mtx{V}_1^{\top} \mtx{B} = \mtx{B},
    \label{equ:3-nystrom-range-captured}
\end{equation}
i.e. the columns of the matrix $\mtx{V}_1$ fully capture the range of $\mtx{B}$.\\

However, computing the spectral
decomposition is often prohibitively expensive and usually not necessary for
obtaining approximate factorizations of a matrix. There exist multiple approximate
factorization methods of low-rank matrices, many of which are based on sketching.
Sketching is a widely used technique for approximating the column space of a matrix
\cite{halko2011finding,woodruff2014sketching,lin2017randomized,tropp2017sketching,tropp2023randomized}.
Multiplying a low-rank matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ with a standard Gaussian
\gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$ with $n_{\Omega} \ll n$, will
result in a significantly smaller matrix $\mtx{B}\mtx{\Omega}$ -- the sketch -- whose
columns roughly span the same range as the columns of $\mtx{B}$.
Closely following \cite[section~2.1]{tropp2023randomized}, we illustrate the
reason why this approach works hereafter.\\

Suppose $\mtx{B}$ is symmetric and admits a spectral decomposition
\begin{equation}
    \mtx{B}
        = \mtx{V} \mtx{\Sigma} \mtx{V}^{\top} 
        = \sum_{i=1}^n \lambda_i \vct{v}_i \vct{v}_i^{\top}.
    \label{equ:3-nystrom-sketching-spectral-decomposition}
\end{equation}
If we left-multiply $\mtx{B}$ with a standard Gaussian random vector $\vct{\omega} \in \mathbb{R}^n$,
we obtain
\begin{equation}
    \mtx{B}\vct{\omega}
        = \sum_{i=1}^n \lambda_i \vct{v}_i (\vct{v}_i^{\top} \vct{\omega})
        = \sum_{i=1}^n (\lambda_i  z_i) \vct{v}_i,
        \label{equ:3-nystrom-sketching-random-multiplication}
\end{equation}
where $z_i = \vct{v}_i^{\top} \vct{\omega} \in \mathbb{R}$ again follows a
standard Gaussian distribution, since the $\vct{v}_i$
have unit norm. The component of $\mtx{B}\vct{\omega}$ in the direction of the
$j$-th eigenvector $\vct{v}_j$ is
\begin{equation}
    \vct{v}_j^{\top}\mtx{B}\vct{\omega}
        = \sum_{i=1}^n (\lambda_i  z_i) (\vct{v}_j^{\top} \vct{v}_i)
        = \lambda_j  z_j.
        \label{equ:3-nystrom-sketching-dominant-component}
\end{equation}
That is, $\mtx{B}\vct{\omega}$ is roughly aligned with the eigenvectors corresponding
to the largest eigenvalues: the dominant eigenvectors.
When repeating this procedure for multiple $\vct{\omega}$ by right-multiplying $\mtx{B}$
with a standard Gaussian \gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$,
it is shown in \cite{halko2011finding} that
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) (\mtx{B} \mtx{\Omega})^{\dagger} \mtx{B}
    \label{equ:3-nystrom-RSVD}
\end{equation}
is a good approximation of $\mtx{B}$.\\

Based on the sketch $\mtx{B}\mtx{\Omega}$, we can also compute low-rank approximations
of the form
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) \mtx{K} (\mtx{B} \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom-general}
\end{equation}
We want $\widehat{\mtx{B}} \approx \mtx{B}$. Therefore, by multiplying \refequ{equ:3-nystrom-nystrom-general}
from the left with $\mtx{\Omega}^{\top}$ and from the right with $\mtx{\Omega}$,
and by imposing $\widehat{\mtx{B}} = \mtx{B}$ we get
\begin{equation}
    \mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega} = (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega}) \mtx{K} (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom-unknown}
\end{equation}
By the properties of the pseudo-inverse of a matrix we see that
$\mtx{K} = (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}$ satisfies this
relation \cite[section~3.1]{lin2017randomized}.
This approximation is referred to as the Nystr\"om approximation \cite{gittens2013nystrom,lin2017randomized}
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger} (\mtx{B} \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom}
\end{equation}
It is shown in \cite[lemma~5.2]{tropp2023randomized} that the Nystr\"om approximation
is at least as accurate as the approximation in \refequ{equ:3-nystrom-RSVD}
for \gls{PSD} matrices when measured in the spectral
and Frobenius norms. The following theorem quantifies the accuracy of the
Nystr\"om approximation in the Frobenius norm \cite[lemma~3.2]{persson2022hutch}:

\begin{theorem}{Accuracy of Nystr\"om approximation}{3-nystrom-nystrom-constant}
    Let $\mtx{B}$ be a symmetric positive definite matrix. Its Nystr\"om approximation
    $\widehat{\mtx{B}}$ \refequ{equ:3-nystrom-nystrom} of \gls{sketch-size} $\geq 10$
    satisfies with probability at least $1-6e^{-n_{\Omega}/2}$
    \begin{equation}
        \lVert \mtx{B} - \widehat{\mtx{B}} \rVert _F \leq 542\sqrt{\frac{2}{n_{\Omega}}} \Tr(\mtx{B}).
    \end{equation}
\end{theorem}

\subsection{Parameter-dependent matrices}
\label{subsec:3-nystrom-factorization-parameter-matrices}

These two low-rank factorizations can be extended in a straightforward way
to the case where the matrix $\mtx{B} \in \mathbb{R}^{n \times n}$ depends
continuously on a parameter $t \in [a, b]$. In the case of the Nystr\"om approximation
we give an expression for the trace estimation error in the $L^1$-norm in the
following theorem from \cite{he2023parameter}:
\begin{theorem}{$L^1$ error of the trace of a Nystr\"om approximation}{3-nystrom-frobenius-norm}
    Let $\mtx{B}(t) \in \mathbb{R}^{n \times n}$ be symmetric \gls{PSD}
    and continuously depend on $t \in [a,b] \subset \mathbb{R}$. Let the
    standard Gaussian \glsfirst{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$
    used in \refequ{equ:3-nystrom-nystrom} have
    $n_{\Omega} = r + p$ columns for some $r \geq 2, p \geq 4$. Then
    for all $\gamma \geq 1$, the inequality
    \begin{equation}
        \int_{a}^{b} | \Tr(\mtx{B}(t)) - \Tr(\widehat{\mtx{B}}(t))| \mathrm{d}t
            < \gamma \sqrt{1 + r} \int_{a}^{b} \sum_{i = r+1}^n \lambda_i(\mtx{B}(t)) \mathrm{d}t
    \end{equation}
    holds with probability $1 - \gamma^{-p}$.
\end{theorem}

%\begin{proof}
%    For every $t \in [a, b]$ we have
%    \begin{align}
%        |\Tr(\mtx{B}(t)) - \Tr(\widehat{\mtx{B}}(t))|
%            &= |\Tr(\mtx{B}(t) - \widehat{\mtx{B}}(t))| && \text{(linearity of trace)} \notag \\ 
%            &= \Tr(\mtx{B}(t) - \widehat{\mtx{B}}(t)) &&  \text{($\mtx{B}(t) - \widehat{\mtx{B}}(t) \succcurlyeq 0$ \cite[lemma~2.1]{frangella2023preconditioning})} \notag \\ 
%            &= \sum_{i=1}^n \lambda_i(\mtx{B}(t) - \widehat{\mtx{B}}(t)) &&  \text{(trace is sum of eigenvalues)} \notag \\
%            &= \lVert \mtx{B}(t) - \widehat{\mtx{B}}(t) \rVert _{(1)} && \text{(definition of Schatten norm)} \notag \\
%            &= \lVert (\mtx{I} - \Pi_{\mtx{B}(t)^{1/2} \mtx{\Omega}})\mtx{B}(t)^{1/2} \rVert _F^2 && \text{(\cite[proof of corollary 8.8]{tropp2023randomized})}
%    \end{align}
%    Thus, we can apply \cite[theorem~9]{kressner2023randomized} to get
%    \begin{equation}
%        \int_{a}^{b} |\Tr(\mtx{B}(t)) - \Tr(\widehat{\mtx{B}}(t))| \mathrm{d}t
%            < \gamma \sqrt{1 + r} \int_{a}^{b} \sum_{i = r+1}^n \lambda_i(\mtx{B}(t)) \mathrm{d}t.
%    \end{equation}
%\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Nystr\"om-Chebyshev method}
\label{sec:3-nystrom-nystrom-chebyshev}

Motivated by the observations from the beginning of this chapter, we now present
an algorithm for estimating the \glsfirst{smooth-spectral-density}.
Since all \gls{smoothing-kernel} we consider are symmetric
and non-negative, the matrix function \refequ{equ:3-nystrom-matrix-function} is
symmetric \gls{PSD}. Therefore, we opt for the Nystr\"om approximation \refequ{equ:3-nystrom-nystrom}
as our randomized low-rank factorization engine. Before we go into the details
of the method, we need to quantify the \glsfirst{numerical-rank} of this matrix
function, in order to get an a priori guarantee for the convergence of the 
method.\\

The eigenvalues of $g_{\sigma}(t\mtx{I} - \mtx{A})$ are given by
\begin{equation}
    \lambda_i(g_{\sigma}(t\mtx{I} - \mtx{A})) = g_{\sigma}(t - \lambda_{(i)}(\mtx{A})) = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{-\frac{(t - \lambda_{(i)}(\mtx{A}))^2}{2 \sigma^2}}
    \label{equ:3-nystrom-kernel-function-eigenvalues}
\end{equation}
where $\lambda_{(1)}(\mtx{A}), \dots, \lambda_{(n)}(\mtx{A})$ denote the
eigenvalues of $\mtx{A}$ sorted by increasing
distance from \gls{spectral-parameter}, i.e. such that $\lambda_1(g_{\sigma}(t\mtx{I} - \mtx{A})) \geq \dots \geq \lambda_n(g_{\sigma}(t\mtx{I} - \mtx{A}))$. Consequently,
we may upper bound the numerical rank of \refequ{equ:3-nystrom-matrix-function} as
\begin{equation}
    r_{\varepsilon, \cdot}(g_{\sigma}(t\mtx{I} - \mtx{A})) \leq \#\{1\leq i\leq n: |t - \lambda_i(g_{\sigma}(t\mtx{I} - \mtx{A}))| \leq C_{\varepsilon, \cdot}(\sigma)\}
    \label{equ:3-nystrom-kernel-numerical-rank}
\end{equation}
where we define the distances
\begin{align}
    C_{\varepsilon, 2}(\sigma) = \sigma \sqrt{-2 \log(n \sqrt{2 \pi} \sigma \varepsilon)}, \label{equ:3-nystrom-kernel-numerical-rank-spectral-constant} \\
    C_{\varepsilon, F}(\sigma) = \sigma \sqrt{-2 \log(\sqrt{2 \pi} \sigma \varepsilon)}. \label{equ:3-nystrom-kernel-numerical-rank-frobenius-constant} 
\end{align}
For the spectral norm, \refequ{equ:3-nystrom-kernel-numerical-rank} even holds
as an equality.
The expression \refequ{equ:3-nystrom-kernel-numerical-rank} has a very
visual interpretation: The \glsfirst{numerical-rank} of a matrix is at most
equal to the number of eigenvalues which are closer to \gls{spectral-parameter}
than $C_{\varepsilon, \cdot}(\sigma)$.
This is also illustrated in \reffig{fig:3-nystrom-numerical-rank-constant}.\\
\begin{figure}[ht]
    \centering
    \input{figures/numerical_rank.tex}
    \caption{Computing the numerical rank by counting the number of eigenvalues
        $\lambda_1, \dots, \lambda_n$ of a matrix $\mtx{A}$ which lie at most
        a constant $C_{\varepsilon, \cdot}(\sigma)$ away from \gls{spectral-parameter}.}
    \label{fig:3-nystrom-numerical-rank-constant}
\end{figure}

If we additionally assume the eigenvalues of the matrix $\mtx{A}$
to be evenly distributed within $[a, b]$, that is, in any subinterval of fixed length in
$[a, b]$ we can expect to find roughly the same number of eigenvalues (see \reffig{fig:3-nystrom-evenly-distributed-spectrum}), then
we can estimate the numerical rank of $g_{\sigma}(t\mtx{I} - \mtx{A})$ to be
\begin{equation}
    r_{\varepsilon, \cdot}(g_{\sigma}(t\mtx{I} - \mtx{A})) \lessapprox \frac{2 n}{b - a} C_{\varepsilon, \cdot}(\sigma).
    \label{equ:3-nystrom-kernel-numerical-rank-even-eigenvalues}
\end{equation}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.45\columnwidth}
        \input{figures/uneven_spectrum.tex}
        \caption{Unevenly distributed spectrum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\columnwidth}
        \input{figures/even_spectrum.tex}
        \caption{Evenly distributed spectrum}
    \end{subfigure}      
    \caption{Examples of an unevenly and an evenly distributed spectrum
    with some comments about the numerical rank of $g_{\sigma}(t\mtx{I} - \mtx{A})$.}
    \label{fig:3-nystrom-evenly-distributed-spectrum}
\end{figure}

In \reffig{fig:3-nystrom-singular-value-decay}, we numerically check the decay
of the eigenvalues for one of our example matrices which we use in the numerical
experiments (\refsec{sec:5-experiments-density-function}). Clearly, the estimated
numerical rank of the matrix not accurate for some values of \gls{spectral-parameter}
in this case, since the spectrum is
nowhere near evenly distributed. Nevertheless, it provides us with a good first
guess which can be used to decide on what \gls{sketch-size} to use.\\
\begin{figure}[ht]
    \centering
    \input{figures/singular_value_decay.tex}
    \caption{Singular value decay of a Gaussian \glsfirst{smoothing-kernel}
       with \gls{smoothing-parameter} $=0.05$ applied to the matrix introduced
       \refsec{sec:5-experiments-density-function} for $c=1$. For reference,
       the effective spectral density is plotted above the color bar which
       assigns a color to the values of \gls{spectral-parameter}.}
    \label{fig:3-nystrom-singular-value-decay}
\end{figure}

The backbone of what is also known as the spectrum sweeping method \cite{lin2017randomized}
is the Nystr\"om approximation of the Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion},
hence why we refer to it as the \glsfirst{NC} method. 
For some standard Gaussian \gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$,
the approximation reads
\begin{equation}
    \widehat{g}_{\sigma}^m(t\mtx{I} - \mtx{A})
    = (g_{\sigma}^m(t\mtx{I} - \mtx{A}) \mtx{\Omega}) (\mtx{\Omega}^{\top} g_{\sigma}^m(t\mtx{I} - \mtx{A}) \mtx{\Omega})^{\dagger} (g_{\sigma}^m(t\mtx{I} - \mtx{A}) \mtx{\Omega})^{\top}.
    \label{equ:3-nystrom-nystrom-smoothing-kernel}
\end{equation}
Since we are only interested in the trace of this approximation,
we may use the cyclic property of the trace to obtain
\begin{equation}
    \widehat{\phi}_{\sigma}^m(t)
        = \Tr\big((\underbrace{\mtx{\Omega}^{\top} g_{\sigma}^m(t\mtx{I} - \mtx{A}) \mtx{\Omega}}_{=\mtx{K}_1(t)})^{\dagger} (\underbrace{\mtx{\Omega}^{\top} (g_{\sigma}^m(t\mtx{I} - \mtx{A}))^2 \mtx{\Omega}}_{=\mtx{K}_2(t)})\big).
    \label{equ:3-nystrom-spectral-density}
\end{equation}\\

The interpolation framework introducted in \refsec{sec:2-chebyshev-interpolation},
allows us to interpolate the appearing matrices $\mtx{K}_1(t) \in \mathbb{R}^{n_{\Omega} \times n_{\Omega}}$
and $\mtx{K}_2(t) \in \mathbb{R}^{n_{\Omega} \times n_{\Omega}}$,
However, when interpolating the
squared \gls{smoothing-kernel} in a separate expansion
\begin{equation}
    (g_{\sigma}(t\mtx{I} - \mtx{A})^2)^m = \sum_{l=0}^{2m} \nu_l(t) T_l(A),
    \label{equ:3-nystrom-ESS-chebyshev-expansion}
\end{equation}
as is suggested in \cite{lin2017randomized}, it is no longer guaranteed that
the $(g_{\sigma}(t\mtx{I} - \mtx{A})^2)^m$ is the square of $g_{\sigma}^m(t\mtx{I} - \mtx{A})$
We have observed that this loss of the square relationship
decreases the numerical accuracy noticeably, which is
shown in \reffig{fig:3-nystrom-interpolation-issue}.\\
\begin{figure}[ht]
    \centering
    \input{plots/interpolation_issue.pgf}
    \caption{Difference in the accuracy when computing \refequ{equ:3-nystrom-spectral-density}
        using a separate expansion (interpolation) versus explicitly squaring
        the matrix function (squaring). The model problem from \refsec{sec:5-experiments-density-function}
        is used for a Gaussian \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$
        and we fix \gls{sketch-size} $=80$.}
    \label{fig:3-nystrom-interpolation-issue}
\end{figure}

In order to circumvent the above mentioned issue, we propose a way of computing
a consistent expansion for \gls{smoothing-kernel}$^2$ which is more accurate
(see \reffig{fig:3-nystrom-interpolation-issue}) and significantly faster
(see \reftab{tab:3-nystrom-timing-squared-interpolation}).
The relation of the Chebyshev expansion to the \gls{DCT} shown in \refequ{equ:2-chebyshev-chebyshev-DCT}
can be exploited to design fast and exact multiplication algorithms between polynomials
of the form \refequ{equ:2-chebyshev-chebyshev-expansion} \cite[proposition~3.1]{baszenski1997cosine}.
In particular, raising a Chebyshev expansion to an integer power can be achieved
very efficiently by chaining a \gls{DCT} with an inverse \gls{DCT}:
Suppose we have computed the coefficients $\vct{\mu} \in \mathbb{R}^{m+1}$
of a Chebyshev expansion \refequ{equ:2-chebyshev-chebyshev-expansion}.
Then we may quickly compute the coefficients $\vct{\nu} \in \mathbb{R}^{km+1}$
of the same expansion raised to the power $k \geq 2$ by first zero-padding
$\widehat{\vct{\mu}} = [\vct{\mu}^{\top}, \vct{0}_{(k-1)m}^{\top}]^{\top} \in \mathbb{R}^{km+1}$
and subsequently computing
\begin{equation}
    \vct{\nu} = \DCT^{-1}\left\{ \DCT\left\{\widehat{\vct{\mu}}\right\}^{k} \right\}
    \label{equ:3-nystrom-chebyshev-potentiate-coefficients}
\end{equation}
where the exponentiation of a vector is understood elementwise.
The corresponding algorithm is presented hereafter.
\begin{algo}{Exponentiation of Chebyshev polynomials}{3-nystrom-chebyshev-exponentiation}
    \input{algorithms/chebyshev_exponentiation.tex}
\end{algo}
The complexity of this procedure is $\mathcal{O}(km \log(km))$ \cite{makhoul1980fct}.\\
\begin{table}[ht]
    \caption{Runtime comparison of the two approaches with which the coefficients
    of the Chebyshev expansion of a function. We average over 7 runs of the
    algorithms and repeat these runs 100 times to form the mean and standard
    deviation which are given in the below table. We refer to the interpolation
    of \gls{smoothing-kernel}$^{2}$ with \cite[algorithm~1]{lin2017randomized} as \enquote{quadrature},
    to the interpolation of \gls{smoothing-kernel}$^{2}$ with \refalg{alg:2-chebyshev-chebyshev-expansion} as \enquote{DCT},
    and finally to the fast squaring algorithm \refalg{alg:3-nystrom-chebyshev-exponentiation} as \enquote{squaring}.
    For each algorithm, we interpolate \gls{smoothing-kernel} with \gls{smoothing-parameter} $=0.05$,
    at \gls{num-evaluation-points} $=1000$ points, for various values of \gls{chebyshev-degree}.}
    \label{tab:3-nystrom-timing-squared-interpolation}
    \input{tables/timing_squared_interpolation.tex}
\end{table}

The algorithm gives us the consistent expansion
\begin{equation}
    (g_{\sigma}^m(t\mtx{I} - \mtx{A}))^2 = \sum_{l=0}^{2m} \nu_l(t) T_l(A).
    \label{equ:3-nystrom-consistent-chebyshev-expansion}
\end{equation}
This way of expanding the squared matrix function
is exactly equivalent to \cite[algorithm~5]{lin2017randomized}, but
usually orders of magnitude faster because the generalized eigenvalue problem
on line 13 of this algorithm does not have to be assembled from the
product of two large matrices.\\

%\todo{Maybe justify this with table/plot}

Putting all things together, we get the \gls{NC} method, whose pseudocode
can be found in \refalg{alg:3-nystrom-nystrom-chebyshev}.
\begin{algo}{Nystr\"om-Chebyshev method}{3-nystrom-nystrom-chebyshev}
    \input{algorithms/nystrom_chebyshev.tex}
\end{algo}

Again denoting the cost of a matrix-vector product of $\mtx{A} \in \mathbb{R}^{n \times n}$
with $c(n)$, e.g. $\mathcal{O}(c(n)) = n^2$ for dense and $\mathcal{O}(c(n)) = n$
for sparse matrices, we find the computational complexity of the \gls{NC}
method to be $\mathcal{O}(m \log(m) n_t + m n_{\Omega}^2 n + m n_t n_{\Omega}^2 +  m c(n) n_{\Omega} + n_t n_{\Omega}^3)$, with
$\mathcal{O}(n n_{\Omega} + n_{\Omega}^2 n_t + m n_t)$ required additional storage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation details}
\label{subsec:3-nystrom-implementation-details}

In \refalg{alg:3-nystrom-nystrom-chebyshev}, we have that
\begin{equation}
    \mtx{K}_1(t_i) = \mtx{\Omega}^{\top} g_{\sigma}^m(t_i\mtx{I} - \mtx{A}) \mtx{\Omega}.
    \label{equ:3-nystrom-interpolated-matrix}
\end{equation}
Hence, if $g_{\sigma}^m(t_i\mtx{I} - \mtx{A})$ is close to the zero matrix,
which we have seen in \refsec{sec:3-nystrom-nystrom-chebyshev} to happen if
$t_i$ is far away from any of the eigenvalues of $\mtx{A}$, $\mtx{K}_1(t_i)$ will
also be close to the zero matrix. In this case, it may not be a good idea to
compute the pseudo-inverse of $\mtx{K}_1(t_i)$ in \reflin{lin:3-nystrom-pseudo-inverse}
of \refalg{alg:3-nystrom-nystrom-chebyshev}. Even less, since in that case we already know that
\begin{equation}
    \phi_{\sigma}^m(t_i) = \Tr(g_{\sigma}^m(t_i\mtx{I} - \mtx{A})) \approx \Tr(\mtx{0}) = 0.
    \label{equ:3-nystrom-short-circuit}
\end{equation}
Motivated by this observation we use a \enquote{short-circuit} mechanism which 
computes the \gls{sketch-size}-query Hutchinson's estimate \refequ{equ:2-chebyshev-DGC-hutchionson-estimator}
\begin{equation}
    \frac{1}{n_{\Omega}} \Tr(\mtx{K}_1(t_i))
    \label{equ:3-nystrom-short-circuit-hutchinson}
\end{equation}
of $\Tr(g_{\sigma}^m(t_i\mtx{I} - \mtx{A}))$
before executing \reflin{lin:3-nystrom-pseudo-inverse} in \refalg{alg:3-nystrom-nystrom-chebyshev}.
If the result is smaller than a \gls{short-circuit-threshold} $> 0$, it immediately sets
$\widetilde{\phi}_{\sigma}^m(t_i)=0$ and skips \reflin{lin:3-nystrom-pseudo-inverse}.
for this $t_i$. We usually use \gls{short-circuit-threshold} $= 10^{-5}$. The effect this short-circuit
mechanism has on the approximation quality can be seen in \reffig{fig:3-nystrom-short-circuit-mechanism}.\\

\begin{figure}[ht]
    \centering
    \input{plots/short_circuit_mechanism.pgf}
    \caption{The difference the short-circuit mechanism can make when approximating
        a spectral density using \refalg{alg:3-nystrom-nystrom-chebyshev}.}
    \label{fig:3-nystrom-short-circuit-mechanism}
\end{figure}

There exists an alternative way of computing the result on \reflin{lin:3-nystrom-pseudo-inverse}
in \refalg{alg:3-nystrom-nystrom-chebyshev}, namely traces of the form
\begin{equation}
    \Tr((\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}(\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega})).
    \label{equ:3-nystrom-trace-pseudo-inverse}
\end{equation}

Rather than explicitly forming the pseudo-inverse,
it converts the problem of computing such a trace into solving a generalized
eigenvalue problem by making use of the following theorem \cite[theorem~3]{lin2017randomized}.

\begin{theorem}{Generalized eigenvalue problem to compute pseudo-inverse}{3-nystrom-eigenvalue-problem}
    Let $\mtx{B} \in \mathbb{R}^{n \times n}$ be symmetric with rank $r \ll n$ and
    truncated spectral decomposition \refequ{equ:3-nystrom-eigenvalue-decoposition}
    \begin{equation}
        \mtx{B} = \mtx{V}_1 \mtx{\Sigma}_1 \mtx{V}_1^{\top},
        \label{equ:3-nystrom-eigenvalue-problem-spectral-decomposition}
    \end{equation}
    where $\mtx{\Sigma}_1 \in \mathbb{R}^{r \times r}$ and
    $\mtx{V}_1 \in \mathbb{R}^{n \times r}$ with $\mtx{V}_1^{\top} \mtx{V}_1 = \mtx{I}$.
    For $\mtx{\Omega} \in \mathbb{R}^{n \times n_{\Omega}}, n_{\Omega} > r$,
    such that $\mtx{\Omega}^{\top} \mtx{V}_1$ has linearly independent columns.
    Then the solution of the generalized eigenvalue problem
    \begin{equation}
        (\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega}) \mtx{C} = (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega}) \mtx{C}  \mtx{\Xi}
        \label{equ:3-nystrom-low-rank-eigenvalue-problem}
    \end{equation}
    with $\mtx{C} \in \mathbb{R}^{n_{\Omega} \times r}$ and $\mtx{\Xi} \in \mathbb{R}^{r \times r}$ a 
    non-zero diagonal matrix is $\mtx{C} = (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger} \mtx{\Theta}$
    and $\mtx{\Xi} = \mtx{\Theta}^{\top} \mtx{\Sigma}_1 \mtx{\Theta}$
    for a permutation matrix $\mtx{\Theta} \in \mathbb{R}^{r \times r}$.

    Furthermore,
    \begin{equation}
        \Tr((\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}(\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega})) = \Tr(\mtx{\Xi})
        \label{equ:3-nystrom-low-rank-trace-equalities}
    \end{equation}
\end{theorem}
A proof of this theorem can be found in \cite[theorem~3]{lin2017randomized}. We
choose to still include our own version which goes slightly more into detail.
\begin{proof}
    Since $\mtx{\Omega}^{\top} \mtx{V}_1$ has independent columns, its pseudo-inverse
    is its left inverse, meaning 
    $(\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1) = ( \mtx{V}_1^{\top}\mtx{\Omega}) ( \mtx{V}_1^{\top}\mtx{\Omega})^{\dagger} = \mtx{I}$.
    We use $\mtx{B} = \mtx{V}_1 \mtx{\Sigma}_1 \mtx{V}_1^{\top}$ and insert
    $\mtx{C} = (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger} \mtx{\Theta}^{\top}$
    and $\mtx{\Xi} = \mtx{\Theta} \mtx{\Sigma} \mtx{\Theta}^{\top}$ in the
    left-hand side of \refequ{equ:3-nystrom-low-rank-eigenvalue-problem} to get
    \begin{equation}
        (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 \underbrace{(\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}}_{=\mtx{I}} \mtx{\Theta} = (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 \mtx{\Theta}
    \end{equation}
    and on the right-hand side of \refequ{equ:3-nystrom-low-rank-eigenvalue-problem} for
    \begin{equation}
        (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1 \underbrace{(\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}}_{=\mtx{I}} \underbrace{\mtx{\Theta} \mtx{\Theta}^{\top}}_{=\mtx{I}} \mtx{\Sigma}_1 \mtx{\Theta} = (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 \mtx{\Theta}
    \end{equation}
    Now that $\mtx{\Omega}^{\top} \mtx{V}_1$ has linearly independent columns, $\mtx{\Sigma}_1$ is a non-zero
    diagonal matrix, and $\mtx{\Theta}$ is a permutation, we conclude that
    the given $\mtx{C}$ and $\mtx{\Xi}$ indeed solve the generalized eigenvalue problem.
    By the uniqueness of generalized eigenvalues and eigenvectors (up to
    permutation and scaling), these are indeed the only solutions.

    \refequ{equ:3-nystrom-low-rank-trace-equalities} can be shown by inserting the truncated
    spectral decomposition \refequ{equ:3-nystrom-eigenvalue-problem-spectral-decomposition}
    and using the properties of the pseudo-inverse
    \begin{align*}
        &\Tr((\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger}(\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega})) \notag \\
            &= \Tr((\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger} \mtx{\Sigma}_1^{-1} (\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1) \mtx{\Sigma}_1^2 (\mtx{V}_1^{\top} \mtx{\Omega})) &&\text{(spectral decomposition of $\mtx{B}$)} \notag \\
            &= \Tr(\mtx{\Sigma}_1^{-1} (\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1)\mtx{\Sigma}_1^2 (\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}) &&\text{(cyclic property of trace)} \notag \\
            &= \Tr(\mtx{\Sigma}_1^{-1} \underbrace{(\mtx{\Omega}^{\top} \mtx{V}_1)^{\dagger} (\mtx{\Omega}^{\top} \mtx{V}_1)}_{=\mtx{I}} \mtx{\Sigma}_1^2 \underbrace{(\mtx{V}_1^{\top} \mtx{\Omega}) (\mtx{V}_1^{\top} \mtx{\Omega})^{\dagger}}_{=\mtx{I}}) &&\text{($\mtx{\Omega}^{\top} \mtx{V}_1$ independent columns)} \notag \\
            &= \Tr(\mtx{\Sigma}_1) &&\text{($\mtx{\Sigma}_1^{-1} \mtx{\Sigma}_1^{2} = \mtx{\Sigma}_1$)}  \notag \\
            &= \Tr(\mtx{\Xi}) &&\text{($\mtx{\Sigma}_1 = \mtx{\Xi}$ up to permutation)}
    \end{align*}
\end{proof}

A standard way of computing \refequ{equ:3-nystrom-low-rank-eigenvalue-problem}
is based on the spectral decomposition
\begin{equation}
    \mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega} = \begin{bmatrix} \mtx{W}_1 & \mtx{W}_2 \end{bmatrix} 
    \begin{bmatrix} \mtx{\Gamma}_1 & \mtx{0} \\ \mtx{0} & \mtx{\Gamma}_2 \ \end{bmatrix} 
    \begin{bmatrix} \mtx{W}_1^{\top} \\ \mtx{W}_2^{\top} \end{bmatrix}.
\end{equation}
It allows us to convert the generalized eigenvalue problem \refequ{equ:3-nystrom-low-rank-eigenvalue-problem}
into the standard eigenvalue problem
\begin{equation}
    \mtx{\Gamma}_1^{-1/2} \mtx{W}_1^{\top} (\mtx{\Omega}^{\top} \mtx{B}^2 \mtx{\Omega}) \mtx{W}_1 \mtx{\Gamma}_1^{-1/2} \mtx{X} = \mtx{X} \mtx{\Xi},
    \label{equ:3-nystrom-converted-generalized-eigenvalue-problem}
\end{equation}
which projects out the kernel of $\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega}$.\\
%By identifying 
%We can retrieve the generalized eigenvector as
%\begin{equation}
%    \mtx{C} = \mtx{V}_1 \mtx{M}^{-1/2} \mtx{X}.
%    \label{equ:3-nystrom-generalized-eigenvector}
%\end{equation}\\

Since by \refthm{thm:3-nystrom-eigenvalue-problem} the diagonal of $\mtx{\Xi}(t)$
should merely be a permutation of the eigenvalues of $g_{\sigma}(tI - A)$,
we expect its elements to be within the range of $g_{\sigma}$. Hence, we may remove
all elements in $\mtx{\Xi}(t)$ which are outside of $[0, 1 / (n \sqrt{2 \pi \sigma^2})]$ \cite{lin2017randomized}.
In this way, we can filter out computational artefacts which for example might result
from an inaccurate Chebyshev expansion and furthermore 
can enforce non-negativity of the resulting approximation of
\gls{smooth-spectral-density}. However, one needs to be careful not to accidentally
remove a valid element from $\mtx{\Xi}(t)$. In the case where \gls{spectral-parameter}
coincides with an eigenvalue of $\mtx{A}$, one of the elements in $\mtx{\Xi}(t)$
should correctly assume the value $1 / (n \sqrt{2 \pi \sigma^2})$. Hence, if due to
certain numerical inaccuracies this value slightly exceeds the above threshold,
it will get falsely removed. Theis problematic case can be avoided by introducing a
\gls{filter-tolerance}$>0$, such that only elements in $\mtx{\Xi}(t)$ which are
outside of $[-\eta, 1 / (n \sqrt{2 \pi \sigma^2}) + \eta]$ will be filtered out. \\

%\todo{maybe add plot justifying the filtering tolerance}

We can summarize this alternative way of treating
\reflin{lin:3-nystrom-pseudo-inverse} of \refalg{alg:3-nystrom-nystrom-chebyshev}
in the following algorithm.
\begin{algo}{Trace through generalized eigenvalue problem}{3-nystrom-eigenvalue-problem}
    \input{algorithms/eigenvalue_problem.tex}
\end{algo}

In the end, this procedure only slightly improves the accuracy
and only for small \gls{chebyshev-degree}, i.e. when the Chebyshev expansion has
not converged yet. This is shown with an example in \reffig{fig:3-nystrom-eigenvalue-problem}.

\begin{figure}[ht]
    \centering
    \input{plots/eigenvalue_problem.pgf}
    \caption{The difference between directly computing the pseudo-inverse
        in \reflin{lin:3-nystrom-pseudo-inverse} of \refalg{alg:2-chebyshev-DGC}
        versus solving the eigenvalue problem \refequ{equ:3-nystrom-converted-generalized-eigenvalue-problem}.}
    \label{fig:3-nystrom-eigenvalue-problem}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\subsection{Theoretical analysis}
%\label{subsec:3-nystrom-theoretical-analysis}
%
%\todo{Combine matrix rank result with approximation result}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Extension to other low-rank approximations}
\label{subsec:3-nystrom-other-low-rank}

Notice that many other methods \cite{halko2011finding,tropp2023randomized} fit
into the scheme of \refequ{equ:3-nystrom-spectral-density}. In fact, if we
generalize
\begin{equation}
    \Tr^{k}(\widehat{\mtx{B}})
        = \Tr\left((\mtx{\Omega}^{\top} \mtx{B}^k \mtx{\Omega})^{\dagger} (\mtx{\Omega}^{\top} \mtx{B}^{k+1} \mtx{\Omega}) \right),
    \label{equ:3-nystrom-trace-generalization}
\end{equation}
the underlying low-rank approximation for $k=1$ is the Nystr\"om approximation
which we have discussed in \refsec{sec:3-nystrom-nystrom}.
For $k=2$, it corresponds to the approximation \refequ{equ:3-nystrom-RSVD}
if we rewrite it as
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) ((\mtx{B} \mtx{\Omega})^{\top} (\mtx{B} \mtx{\Omega}))^{\dagger} (\mtx{B} \mtx{\Omega})^{\top} \mtx{B},
    \label{equ:3-nystrom-RSVD-rewritten}
\end{equation}
using the definition of the pseudo-inverse.
For $k=3$ we would obtain a scheme which theoretically coincides with the Nystr\"om
approximation with one subspace iteration \cite{tropp2023randomized}
\begin{equation}
    \widehat{\mtx{B}} = (\mtx{B}^2 \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B}^3 \mtx{\Omega})^{\dagger} (\mtx{B}^2 \mtx{\Omega})^{\top},
    \label{equ:3-nystrom-nystrom-SI}
\end{equation}
and so on.\\

Generalizing \refalg{alg:3-nystrom-nystrom-chebyshev} is straight forward and
due to the efficient exponentiation of Chebyshev polynomials using
\refalg{alg:3-nystrom-chebyshev-exponentiation}, remains of the same
computational and storage complexity.
