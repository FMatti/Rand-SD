\documentclass[final, leqno, 12pt]{beamer}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[orientation=portrait,size=a0, scale=1.2]{beamerposter}
\usetheme{minimal}
\usecolortheme{minimal}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}

%%% TEMPORARY PREAMBLE (to be replaced when beamer-compatible bibliography was found)

% Fix undefined contorl sequence issue with pyplot pgf exports
\def\mathdefault#1{#1}

% Custom commands
\newcommand{\pp}{\text{\raisebox{0.3ex}{\relscale{0.9}++}}}
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}
\newcommand{\mtx}[1]{\boldsymbol{#1}}
\newcommand{\vct}[1]{\boldsymbol{#1}}

% Custom opertators
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Real}{Re}
\DeclareMathOperator{\Imag}{Im}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dimension}{dim}
\DeclareMathOperator{\DCT}{DCT}
\DeclareMathOperator{\Hutch}{H}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\range}{range}

\usepackage{glossaries}
\makeglossaries
\input{../thesis/glossary.tex}

%%% COLOR THEME

% Color package
\usepackage{xcolor}

% Color definitions
\definecolor{darkblue}{HTML}{2F455C}
\definecolor{mainblue}{HTML}{0e437c}
\definecolor{darkorange}{HTML}{F98125}
\colorlet{lightishblue}{darkblue!20!white}
\colorlet{lightblue}{darkblue!10!white}
\colorlet{linkcolor}{mainblue}

% Reference commands
\newcommand{\reffig}[1]{\hyperref[#1]{\textcolor{linkcolor}{figure \ref*{#1}}}}
\newcommand{\Reffig}[1]{\hyperref[#1]{\textcolor{linkcolor}{Figure \ref*{#1}}}}
\newcommand{\reftab}[1]{\hyperref[#1]{\textcolor{linkcolor}{table \ref*{#1}}}}
\newcommand{\Reftab}[1]{\hyperref[#1]{\textcolor{linkcolor}{Table \ref*{#1}}}}
\newcommand{\refdef}[1]{\hyperref[#1]{\textcolor{linkcolor}{definition \ref*{#1}}}}
\newcommand{\Refdef}[1]{\hyperref[#1]{\textcolor{linkcolor}{Definition \ref*{#1}}}}
\newcommand{\reflem}[1]{\hyperref[#1]{\textcolor{linkcolor}{lemma \ref*{#1}}}}
\newcommand{\Reflem}[1]{\hyperref[#1]{\textcolor{linkcolor}{Lemma \ref*{#1}}}}
\newcommand{\refthm}[1]{\hyperref[#1]{\textcolor{linkcolor}{theorem \ref*{#1}}}}
\newcommand{\Refthm}[1]{\hyperref[#1]{\textcolor{linkcolor}{Theorem \ref*{#1}}}}
\newcommand{\refalg}[1]{\hyperref[#1]{\textcolor{linkcolor}{algorithm \ref*{#1}}}}
\newcommand{\Refalg}[1]{\hyperref[#1]{\textcolor{linkcolor}{Algorithm \ref*{#1}}}}
\newcommand{\refsec}[1]{\hyperref[#1]{\textcolor{linkcolor}{section \ref*{#1}}}}
\newcommand{\Refsec}[1]{\hyperref[#1]{\textcolor{linkcolor}{Section \ref*{#1}}}}
\newcommand{\refchp}[1]{\hyperref[#1]{\textcolor{linkcolor}{chapter \ref*{#1}}}}
\newcommand{\Refchp}[1]{\hyperref[#1]{\textcolor{linkcolor}{Chapter \ref*{#1}}}}
\newcommand{\reflin}[1]{\hyperref[#1]{\textcolor{linkcolor}{line \ref*{#1}}}}
\newcommand{\Reflin}[1]{\hyperref[#1]{\textcolor{linkcolor}{Line \ref*{#1}}}}
\newcommand{\refequ}[1]{\hyperref[#1]{\textcolor{linkcolor}{(\ref*{#1})}}}
\let\oldcite\cite
\renewcommand{\cite}[2][]{\textcolor{linkcolor}{\oldcite{#2}}}

% Colorboxes
\usepackage[many]{tcolorbox}
\tcbset{breakable}
%\newtcolorbox{block}[2][]
%{
%  colframe = darkblue,
%  colback  = lightblue,
%  coltitle = white,
%  title    = {#2},
%  sharp corners,
%  before skip=10pt,
%  after skip=10pt,
%  left=3pt,
%  right=3pt,
%  boxrule=0pt,
%  #1,
%}
%
\tcbuselibrary{theorems}

\newtcbtheorem[]{thm}{Theorem}%
{
  colframe = darkblue,
  colback  = lightishblue,
  coltitle = white,
  sharp corners, 
  before skip=10pt,
  after skip=10pt,
  left=3pt,
  right=3pt,
  boxrule=0pt,
}{thm}

% ADDITIONAL

\setlength\fboxsep{0.5cm}

\title{Randomized Estimation of Spectral Densities}

\author{Fabio Matti \and Haoze He \and Daniel Kressner}

\institute[shortinst]{ANCHP, École polytechnique fédérale de Lausanne (EPFL), Lausanne, Switzerland}

\footercontent{
    \begin{tikzpicture}
        \node[anchor=west] at (0, 0) {\input{re-pro-badge.tex}};
        \node[anchor=east] at (78, 0.5) {Master's thesis};
        \node[anchor=east] at (78, -0.5) {Lausanne, 02/02/2024};
    \end{tikzpicture} %\begin{tabular}{@{}c@{}} Master's thesis \\ Lausanne, 02/02/2024 \end{tabular}}
}
\begin{document}


%%% ABSTRACT

\begin{frame}[t]
\vspace{-20pt}
\begin{tikzpicture}[remember picture, overlay]
\draw[white,line width=0pt] (current page.south east) rectangle (current page.north west);
\fill[shadecolor] (-10, -5.5) rectangle (100, 0.75);
\fill[shadecolor] (-10, -61.25) rectangle (100, -32);
\fill[shadecolor] (-10, -110) rectangle (100, -92);
\end{tikzpicture}
\begin{columns}[t]
\begin{column}{0.85\paperwidth}
\vspace{-40pt}
    \justify
    In many problems in physics, engineering, and computer science, 
    the eigenvalues of certain matrices help understand the nature of a system.
    However, computing the eigenvalues of a matrix can be prohibitively expensive.
    Furthermore, it is often not crucial to know the exact individual eigenvalues,
    but more so their approximate locations with respect to each other.
    The goal of spectral density theory is to find the approximate distribution of
    the eigenvalues of large matrices.
\vspace{20pt}
\end{column}
\end{columns}

%%% INTRODUCTION

\begin{columns}[t]

    \begin{column}{0.45\paperwidth}

        \begin{block}{Spectral density}
            The \gls{spectral-density} of symmetric matrix $\mtx{A} \in \mathbb{R}^{n \times n}$
            with eigenvalues $\lambda_1, \dots, \lambda_n \in \mathbb{R}$, which
            we assume to be contained in $[-1, 1]$,
            is defined as
            \begin{equation}
                \phi(t) = \frac{1}{n} \sum_{i=1}^n \delta(t - \lambda_i).
            \end{equation}
            Since the \gls{dirac-delta} cannot be approximated using smooth functions,
            and because we do not care about the exact locations of the eigenvalues anyway,
            we work with the \gls{smooth-spectral-density} defined as
            \begin{equation}
                \phi_{\sigma}(t) = \sum_{i=1}^n g_{\sigma}(t - \lambda_i)
            \end{equation}
            with a \gls{smoothing-kernel}, which is parametrized by a \gls{smoothing-parameter} $>0$.
            Usually, \gls{smoothing-kernel} is a Gaussian of width \gls{smoothing-parameter}
            \begin{equation}
                g_{\sigma}(s) = \frac{1}{n \sqrt{2 \pi \sigma^2}} e^{-\frac{s^2}{2\sigma^2}}.
            \end{equation}
            We may then convert the problem of computing \gls{smooth-spectral-density}
            to the trace estimation problem
            \begin{equation}
                \phi_{\sigma}(t) = \Tr(g_{\sigma}(t \mtx{I}_n - \mtx{A})).
            \end{equation}
            %\begin{figure}
            %    \scalebox{2.0}{\input{../thesis/plots/spectral_density_example_0.01.pgf}}
            %    \scalebox{2.0}{\input{../thesis/plots/spectral_density_example_0.02.pgf}}
            %    \scalebox{2.0}{\input{../thesis/plots/spectral_density_example_0.05.pgf}}
            %\end{figure}
        \end{block}

    \end{column}

    \begin{column}{0.45\paperwidth}

        \begin{block}{Chebyshev expansion}
            It is often prohibitively expensive to directly
            evaluate the matrix function $g_{\sigma}(t\mtx{I}_n - \mtx{A})$.
            For this and other reasons, a Chebyshev expansion with \gls{chebyshev-degree}
            \begin{equation}
                g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A}) = \sum_{l=0}^{m} \mu_l(t) T_l(\mtx{A})
                \label{equ:chebyshev-expansion}
            \end{equation}
            is first computed instead. For a fixed $t$, the coefficients $\{\mu_l\}_{l=0}^m$ can
            efficiently be computed through a \gls{DCT}.
            When defining the vector of coefficients $\vct{\mu} \in \mathbb{R}^{m+1}$
            and the vector $\vct{g} \in \mathbb{R}^{m+1}$ with components
            $g_l = g_{\sigma}(t - \cos(\pi l / m)), l=0, \dots, m$, we can
            switch between the two in $\mathcal{O}(m \log(m))$ complexity using
            \begin{equation}
                \vct{\mu} = \DCT^{-1}(\vct{g}) \iff \vct{g} = \DCT(\vct{\mu}).
                \label{equ:DCT}
            \end{equation}
            This way of computing the Chebyshev expansion of a function
            can also be used to efficiently square Chebyshev polynomials,
            which will be useful for our implementation
            \begin{equation}
                \big(\sum_{l=0}^{m} \mu_l T_l(\mtx{A})\big)^2 = \sum_{l=0}^{m} \nu_l T_l(\mtx{A}) \implies \vct{\nu} = \DCT^{-1}\big(\DCT(\vct{\mu})^2\big).
                \label{equ:DCT-squaring}
            \end{equation}
            Finally, the starting point for all our methods will be the expanded spectral density
            \begin{equation}
                \phi_{\sigma}^{(m)}(t) = \Tr(g_{\sigma}^{(m)}(t \mtx{I}_n - \mtx{A})).
                \label{equ:expanded-spectral-density}
            \end{equation}
            %\begin{figure}
            %    \scalebox{2.0}{\input{../thesis/figures/chebyshev_convergence.tex}}
            %\end{figure}
        \end{block}

    \end{column}
\end{columns}

%%% METHODS

\vspace{15pt}
\begin{columns}[t]
    \setbeamercolor{block title}{fg=maincolor,bg=shadecolor}
    \setbeamercolor{block separator}{bg=maincolor}
    \setbeamercolor{block body}{fg=black,bg=shadecolor}

    \begin{column}{0.3\paperwidth}

        \begin{block}{Delta-Gauss-Chebyshev}
            Stochastic trace estimation is usually based on the fact that
            for a standard Gaussian random vector $\vct{\psi} \in \mathbb{R}^n$
            \begin{equation}
                \Tr(\mtx{B}) = \mathbb{E}\left[\vct{\psi}^{\top} \mtx{B} \vct{\psi}\right].
            \end{equation}
            The Girard-Hutchinson's trace estimator averages over $n_{\Psi} \in \mathbb{N}$
            independent realizations of this estimate to construct
            \begin{equation}
                \Hutch_{n_{\Psi}}(\mtx{B}) = \frac{1}{n_{\Psi}} \sum_{i=1}^{n_{\Psi}} \vct{\psi}_i^{\top} \mtx{B} \vct{\psi}_i.
            \end{equation}
            The \gls{DGC} method \cite{lin2017randomized} applies this
            estimator to \refequ{equ:expanded-spectral-density} to obtain
            \begin{equation}
                \boxed{\widetilde{\phi}_{\sigma}^{(m)}(t) = \Hutch_{n_{\Psi}}(g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})).}
            \end{equation}
            We can derive the following result for the \gls{DGC} method:
            \begin{thm}{Delta-Gauss-Chebyshev method}{delta-gauss-chebyshev}
                $\mtx{A} \in \mathbb{R}^{n \times n}$ symmetric, then with high probability
                \begin{equation*}
                    \lVert \phi_{\sigma} - \widetilde{\phi}_{\sigma}^{(m)}\rVert _1
                    \leq \frac{2\sqrt{2}}{\sigma^2} (1 + \sigma)^{-m} \left( 1 +  \frac{c_{\Psi}}{\sqrt{n n_{\Psi}}} \right) + \frac{c_{\Psi}}{\sqrt{n_{\Psi}}}.
                \end{equation*}
            \end{thm}
        \end{block}

    \end{column}

    \begin{column}{0.3\paperwidth}

        \begin{block}{Nystr\"om-Chebyshev}
            The Nystr\"om approximation offers a way to compress a symmetric
            \gls{PSD} matrix of low rank by multiplying it with
            a small \gls{sketching-matrix} $\in \mathbb{R}^{n \times n_{\Omega}}$, where
            $n_{\Omega} \ll n$, and forming
            \begin{equation}
                \widehat{\mtx{B}} = (\mtx{B} \mtx{\Omega}) (\mtx{\Omega}^{\top} \mtx{B} \mtx{\Omega})^{\dagger} (\mtx{B} \mtx{\Omega})^{\top}.
            \end{equation}
            Since $g_{\sigma}(t\mtx{I}_n - \mtx{A})$ exhibits a low rank structrue,
            particularly for small \gls{smoothing-parameter}, the \gls{NC}
            method uses the Nystr\"om approximation $\widehat{g}_{\sigma}^{(m)}$ of \refequ{equ:chebyshev-expansion}
            %Error goes down like $\mathcal{O}(\varepsilon^{-2})$ or much faster if $\mtx{B}$ low-rank.
            \begin{equation}
                \boxed{\widehat{\phi}_{\sigma}^{(m)}(t) = \Tr(\widehat{g}_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})).}
            \end{equation}
            An efficient implementation can be achieved due to the cyclic property
            of the trace, since
            \begin{equation}
                \widehat{\phi}_{\sigma}^{(m)}(t) = \Tr(\mtx{K}_1(t)^{\dagger} \mtx{K}_2(t))
            \end{equation}
            for the two matrices 
            \begin{align}
                \mtx{K}_1(t) = \mtx{\Omega}^{\top} g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}) \mtx{\Omega} \in \mathbb{R}^{n_{\Omega} \times n_{\Omega}} \\
                \mtx{K}_2(t) = \mtx{\Omega}^{\top} (g_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A}))^2 \mtx{\Omega} \in \mathbb{R}^{n_{\Omega} \times n_{\Omega}}
            \end{align}
            which can be computed efficiently with \refequ{equ:DCT} and \refequ{equ:DCT-squaring}.
        \end{block}

    \end{column}

    \begin{column}{0.3\paperwidth}

        \begin{block}{Nystr\"om-Chebyshev++}
            The Nystr\"om++ estimator is a variance-reduced version
            of the Girard-Hutchinson's estimator. It is defined as
            \begin{equation}
                \Tr^{++}(\mtx{B}) = \Tr(\widehat{\mtx{B}}) + \Hutch_{n_{\Psi}}(\mtx{\Delta})
            \end{equation}
            with the residual $\mtx{\Delta} = \mtx{B} - \widehat{\mtx{B}}$.
            The generalization of this technique to the
            parameter-dependent case yields the \gls{NCPP} method, which 
            may be computed by combining the \gls{NC} with the \gls{DGC} method
            \begin{equation}
                \boxed{\breve{\phi}_{\sigma}^{(m)}(t) = \widehat{\phi}_{\sigma}^{(m)}(t) + \widetilde{\phi}_{\sigma}^{(m)}(t) - \Hutch_{n_{\Psi}}(\widehat{g}_{\sigma}^{(m)}(t\mtx{I}_n - \mtx{A})).}
            \end{equation}
        In the case where the Chebyshev expansion is exact,
        we are able to show the following guarantee \cite{he2023parameter}:
        \begin{thm}{Parameter-dependent Nystr\"om++}{parameter-dependent-nystrom-pp}
            %$\mtx{B}(t) \in \mathbb{R}^{n \times n}$ is symmetric \gls{PSD}
            %and continuous in $t \in [a, b]$. If $n_{\Omega} = n_{\Psi} = \mathcal{O}\left( \varepsilon^{-1} \right)$,
            %then with high probability
            %\begin{equation}
            %    \int_{a}^{b} |\Tr(\mtx{B}(t)) - \Tr^{++}(\mtx{B}(t))| \mathrm{d}t \leq \varepsilon \int_{a}^{b}\Tr(\mtx{B}(t)) \mathrm{d}t
            %\end{equation}
            $\mtx{A} \in \mathbb{R}^{n \times n}$ symmetric. If $n_{\Omega} = n_{\Psi} = \mathcal{O}\left( \varepsilon^{-1} \right)$
            and we suppose the Chebyshev expansion is exact, then with high probability
            \begin{equation*}
                \lVert \phi_{\sigma} - \phi_{\sigma}^{(m)} \rVert _1 \leq \varepsilon.
            \end{equation*}
        \end{thm}
    \end{block}

    \end{column}

\end{columns}

%%% NUMERICAL RESULTS

\vspace{5pt}
\begin{columns}[t]

    \begin{column}{0.45\paperwidth}

        \begin{block}{Model problem}
            We consider the three-dimensional finite difference discretization matrix
            $\mtx{A} \in \mathbb{R}^{1000 \times 1000}$ of
            the Laplace operator in a potential of periodic Gaussian wells \cite{lin2017randomized}.
            %\begin{equation}
            %    \mathcal{A} u(\vct{x}) = - \Delta u(\vct{x}) + V(\vct{x}) u(\vct{x}).
            %\end{equation}
            
            \begin{figure}
                \scalebox{1.6}{\input{../thesis/plots/periodic_gaussian_well_1.pgf}}
                \scalebox{1.6}{\input{../thesis/plots/periodic_gaussian_well_2.pgf}}
                \scalebox{1.6}{\input{../thesis/plots/periodic_gaussian_well_5.pgf}}
            \end{figure}
        %\end{block}

        %\begin{block}{Allocation of matrix-vector products}
            \gls{NCPP} is a hybrid of \gls{DGC} and \gls{NC}. For small \gls{smoothing-parameter}, the
            matrix in \refequ{equ:expanded-spectral-density} has low rank,
            so the \gls{NC} method performs well. For large \gls{smoothing-parameter},
            the Nystr\"om approximation alone is not sufficient.%, and hence, the \gls{DGC}
            %part is more relevant.
            \begin{figure}
                \scalebox{1.6}{\input{../thesis/plots/electronic_structure_matvec_mixture.pgf}}
            \end{figure}
            \vspace{-30pt}
        \end{block}

    \end{column}

    \begin{column}{0.45\paperwidth}

        \begin{block}{Convergence study}
            For fixed $m$ but varying $n_{\Omega} + n_{\Psi}$, and vice versa,
            we plot the $L^1$ approximation error of all three methods on the
            model problem.
            \begin{figure}
                \scalebox{1.6}{\input{../thesis/plots/electronic_structure_convergence_m_nv160.pgf}}
                \scalebox{1.6}{\input{../thesis/plots/electronic_structure_convergence_nv_m2400.pgf}}
            \end{figure}
            \vspace{-30pt}
            If instead of a Gaussian \gls{smoothing-kernel} we use a Lorentzian
            $g_{\sigma}(s) \propto \sigma / (s^2 + \sigma^2)$,
            %\begin{equation}
            %    g_{\sigma}(s) = \frac{1}{\pi} \frac{\sigma}{s^2 + \sigma^2},
            %\end{equation}
            there exists a specialized method called the Haydock method. We repeat 
            the same plots as before.%, and also give the run time (s) for approximating $n_t=100$ values of $t$.
            \begin{figure}
                \scalebox{1.6}{\input{../thesis/plots/haydock_convergence_m_nv160.pgf}}
                \scalebox{1.6}{\input{../thesis/plots/haydock_convergence_nv_m2400.pgf}}
            \end{figure}
            \vspace{-30pt}
            %\centering
            %\scalebox{0.6}{\input{../thesis/tables/timing_haydock.tex}}
        \end{block}

    \end{column}

\end{columns}

\begin{columns}[t]
    \setbeamercolor{block title}{fg=maincolor,bg=shadecolor}
    \setbeamercolor{block separator}{bg=maincolor}
    \setbeamercolor{block body}{fg=black,bg=shadecolor}
    \begin{column}{0.45\paperwidth}
        \begin{block}{Conclusion}
            We were able to significantly improve many algorithmic aspects of these
            methods. The development of an alternative expansion framework based on
            the \gls{DCT} allowed us to vastly simplify the Chebyshev expansion,
            while obtaining provable accuracy, all
            this in addition to making this stage orders of magnitude faster in many cases.
            Furthermore, we derived theoretical error guarantees for all the encountered methods.
        \end{block}
    \end{column}
    \begin{column}{0.45\paperwidth}
        \begin{block}{References}
            \vspace{-10pt}
            \bibliographystyle{ieeetr}\bibliography{../thesis/bibliography.bib}
        
        \end{block}
    \end{column}
\end{columns}

\end{frame}

\end{document}